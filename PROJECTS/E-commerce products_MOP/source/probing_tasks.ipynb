{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "from config import SIMILARITY_PATH, EMBEDDING_PATH, PRETRAIN_OUTPUT_PATH\n",
    "from emb_extr_res.emb_extr_res import get_embeddings_df, get_pairs_similarity_df, get_pretrain_agg_similarity\n",
    "from load_data.wdc.load_wdc_dataset import EnglishDatasetLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to results\n",
    "test_embeddings_path = path.join(EMBEDDING_PATH, r'test_embeddings.csv')\n",
    "train_embeddings_path = path.join(EMBEDDING_PATH, r'train_embeddings.csv')\n",
    "\n",
    "test_similarity_path = path.join(SIMILARITY_PATH, 'test_similarity.csv')\n",
    "train_similarity_path = path.join(SIMILARITY_PATH, 'train_similarity.csv')\n",
    "\n",
    "pretraining_output_path = path.join(PRETRAIN_OUTPUT_PATH, 'similarity_evaluation_test_evaluation_results.csv')\n",
    "\n",
    "dataset_type = \"cameras\"\n",
    "dataset_size = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train_df = get_embeddings_df(train_embeddings_path)\n",
    "embedding_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = EnglishDatasetLoader.load_train(dataset_type, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_brands_list(train_df, brands_to_drop):\n",
    "    brands_ = train_df[\"brand_left\"].unique().tolist()\n",
    "    brands_.extend(train_df[\"brand_right\"].unique().tolist())\n",
    "\n",
    "    brands_ = [i for i in brands_ if i is not None]\n",
    "    brands = []\n",
    "    for brand in brands_:\n",
    "        brs = brand.split()\n",
    "        brs = [x.replace('\"', '').replace(\"'\", \"\") for x in brs]\n",
    "        brands.extend(brs)\n",
    "\n",
    "    brands = list(set(brands))\n",
    "    brands = [el for el in brands if el not in brands_to_drop]\n",
    "    return brands\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_to_drop = [',', 'd','memory',  'photo', 'co', 'usa',  'power',  'digital', 'camera', 'cam',  'hd',  'a',  'inc',  'le',  'film',  'case',  'pro', 'cameras']\n",
    "brands = prepare_brands_list(train_df, brands_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_left = train_df[[\"id_left\", \"title_left\"]]\n",
    "train_df_right = train_df[[\"id_right\", \"title_right\"]]\n",
    "train_df_left = train_df_left.drop_duplicates().rename({\"id_left\" : \"id\", \"title_left\" : \"title\"}, axis = 'columns')\n",
    "train_df_right = train_df_right.drop_duplicates().rename({\"id_right\" : \"id\", \"title_right\" : \"title\"}, axis = 'columns')\n",
    "df_train_all = pd.concat([train_df_right, train_df_left])\n",
    "df_train_titles = df_train_all.groupby(\"id\").first().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_probing_len(df, train_embeddings_path):\n",
    "\n",
    "    df[\"nr_of_chars\"] = df[\"title\"].apply(lambda x : len(x))\n",
    "    df[\"nr_of_words\"] = df[\"title\"].apply(lambda x : len(x.split()))\n",
    "\n",
    "    nr_of_words_bins = [0, 10, 15, 20, 100]\n",
    "    nr_of_chars_bins = [0, 50, 75, 100, 500]\n",
    "    df['nr_of_chars_bins'] = pd.cut(x=df['nr_of_chars'], bins=nr_of_chars_bins, labels=[0, 1, 2, 3])\n",
    "\n",
    "    df['nr_of_words_bins'] = pd.cut(x=df['nr_of_words'], bins=nr_of_words_bins, labels=[0, 1, 2, 3])\n",
    "\n",
    "    \n",
    "\n",
    "    embedding_train_df_all = get_embeddings_df(train_embeddings_path)\n",
    "\n",
    "    probing_df_chars = pd.merge(df[[\"id\", \"nr_of_chars_bins\" ]], embedding_train_df_all, left_on = \"id\", right_on = 'id')\n",
    "    probing_df_chars = probing_df_chars.rename({\"nr_of_chars_bins\" : \"label\"}, axis=1)\n",
    "\n",
    "    probing_df_words = pd.merge(df[[\"id\", \"nr_of_words_bins\" ]], embedding_train_df_all, left_on = \"id\", right_on = 'id')\n",
    "    probing_df_words = probing_df_words.rename({\"nr_of_words_bins\" : \"label\"}, axis=1)\n",
    "\n",
    "    return probing_df_chars, probing_df_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_chars, probing_df_words = prepare_probing_len(df_train_titles, train_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probing_df_words.to_csv(r'probing_tasks\\dataset\\probing_df_words.csv')\n",
    "# probing_df_chars.to_csv(r'probing_tasks\\dataset\\probing_df_chars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_brands(title, brands):\n",
    "    for brand in brands:\n",
    "        title  = title.replace(brand, '')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_dataset(train_df, brands):\n",
    "    ids = []\n",
    "    ids.extend(train_df[\"id_left\"].unique().tolist())\n",
    "    ids.extend(train_df[\"id_right\"].unique().tolist())\n",
    "    ids = np.array(list(set(ids)))\n",
    "\n",
    "    remove_brand_mask = np.random.choice([True, False], size =len(ids))\n",
    "    \n",
    "    ids_removed_brands = ids[remove_brand_mask]   \n",
    "\n",
    "    train_df1 = train_df.copy()\n",
    "\n",
    "\n",
    "    id_remove_left =  train_df1[\"id_left\"].isin(ids_removed_brands).values\n",
    "\n",
    "    train_df1.loc[id_remove_left, \"title_left\"] = train_df1.loc[id_remove_left, :].apply(lambda x: drop_brands(x.title_left, brands), axis=1)\n",
    "\n",
    "    id_remove_right =  train_df1[\"id_right\"].isin(ids_removed_brands).values\n",
    "\n",
    "    train_df1.loc[id_remove_right, \"title_right\"] = train_df1.loc[id_remove_right, :].apply(lambda x: drop_brands(x.title_right, brands), axis=1)\n",
    "\n",
    "    train_df1[\"changed\"] = False\n",
    "    train_df1[\"changed\"] = train_df1[\"id_left\"].isin(ids_removed_brands)\n",
    "    train_df1[\"changed\"] = train_df1.apply(lambda x: True if x[\"id_right\"] in (ids_removed_brands) else x[\"changed\"], axis=1)    \n",
    "\n",
    "    \n",
    "    return train_df1, ids_removed_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset, ids_removed_brands = prepare_new_dataset(train_df, brands)\n",
    "\n",
    "probing_task_df = new_dataset[new_dataset[\"changed\"] == True].drop(\"changed\", axis=1)\n",
    "# probing_task_df.to_csv(\"datasets/df_removed_brands1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brands_in_title_check(dataset, brands):\n",
    "    train_df_left = dataset[[\"id_left\", \"title_left\"]]\n",
    "    train_df_right = dataset[[\"id_right\", \"title_right\"]]\n",
    "    train_df_left = train_df_left.drop_duplicates().rename({\"id_left\" : \"id\", \"title_left\" : \"title\"}, axis = 'columns')\n",
    "    train_df_right = train_df_right.drop_duplicates().rename({\"id_right\" : \"id\", \"title_right\" : \"title\"}, axis = 'columns')\n",
    "    df_train_all = pd.concat([train_df_right, train_df_left])\n",
    "    df_train_titles = df_train_all.groupby(\"id\").first().reset_index()\n",
    "\n",
    "    df_train_titles[\"brand_in_title\"] = df_train_titles[\"title\"].apply(lambda x : any(ele in x for ele in brands))\n",
    "\n",
    "    return df_train_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_title_check(dataset, key_words):\n",
    "    train_df_left = dataset[[\"id_left\", \"title_left\"]]\n",
    "    train_df_right = dataset[[\"id_right\", \"title_right\"]]\n",
    "    train_df_left = train_df_left.drop_duplicates().rename({\"id_left\" : \"id\", \"title_left\" : \"title\"}, axis = 'columns')\n",
    "    train_df_right = train_df_right.drop_duplicates().rename({\"id_right\" : \"id\", \"title_right\" : \"title\"}, axis = 'columns')\n",
    "    df_train_all = pd.concat([train_df_right, train_df_left])\n",
    "    df_train_titles = df_train_all.groupby(\"id\").first().reset_index()\n",
    "\n",
    "    df_train_titles[\"brand_in_title\"] = df_train_titles[\"title\"].apply(lambda x : any(ele in x.lower() for ele in key_words))\n",
    "\n",
    "    return df_train_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = ['camera', 'digital', 'len']\n",
    "df_words = words_in_title_check(train_df, key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train_df_all = get_embeddings_df(train_embeddings_path)\n",
    "\n",
    "probing_df_key_words = pd.merge(df_words[[\"id\", \"brand_in_title\" ]], embedding_train_df_all, left_on = \"id\", right_on = 'id')\n",
    "probing_df_key_words = probing_df_key_words.rename({\"brand_in_title\" : \"label\"}, axis=1)\n",
    "probing_df_key_words = probing_df_key_words.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probing_df_key_words.to_csv(r'probing_tasks\\dataset\\probing_df_key_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_in_title_df = brands_in_title_check(new_dataset, brands)\n",
    "deleted_ids = brands_in_title_df[brands_in_title_df[\"brand_in_title\"]==True][\"id\"].values\n",
    "\n",
    "embedding_train_df = get_embeddings_df(path.join(EMBEDDING_PATH, r'train_embeddings_removed_brands1.csv'))\n",
    "embedding_train_df_all = get_embeddings_df(train_embeddings_path)\n",
    "\n",
    "new_emb = embedding_train_df_all[~embedding_train_df_all[\"id\"].isin(ids_removed_brands)] \n",
    "new_emb1 = embedding_train_df[embedding_train_df[\"id\"].isin(ids_removed_brands)]\n",
    "new = pd.concat([new_emb1, new_emb])\n",
    "new[\"label\"] = new[\"id\"].isin(deleted_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new.to_csv(r'probing_tasks\\dataset\\train_brand_names.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senetnce length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_words = pd.read_csv(r'probing_tasks\\dataset\\probing_df_words.csv')\n",
    "probing_df_words = probing_df_words.drop([\"Unnamed: 0\", \"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_words_X, probing_df_words_y = probing_df_words.drop([\"label\"], axis=1), probing_df_words[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(probing_df_words_X, probing_df_words_y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(multi_class=\"multinomial\", random_state=42, penalty = 'l1', solver=\"saga\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brand = [67, 64.6,  65.3]\n",
    "brand_classsif = [\"LogisticRegression\", \"RandomForest\" ,\"XGB\"]\n",
    "\n",
    "plt.bar(brand_classsif, brand, color = [(30/255, 57/255, 240/255, 0.70) , (30/255, 57/255, 240/255, 0.60), (30/255, 57/255, 240/255, 0.55)])\n",
    "plt.ylim([0,100])\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_brands = pd.read_csv(r'probing_tasks\\dataset\\train_brand_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_brands = probing_df_brands.drop([\"id\"], axis=1)\n",
    "\n",
    "X, y = probing_df_brands.drop([\"label\"], axis=1), probing_df_brands[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = [76.07, 72.00, 75.83]\n",
    "brand_classsif = [\"LogisticRegression\", \"RandomForest\" ,\"XGB\"]\n",
    "\n",
    "\n",
    "plt.bar(brand_classsif, brand, color = [(30/255, 57/255, 240/255) , (84/255, 117/255, 232/255), (68/255, 103/255, 227/255)])\n",
    "plt.ylim([0,100])\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_df_key_words = probing_df_words = pd.read_csv(r'probing_tasks\\dataset\\probing_df_key_words.csv')\n",
    "X, y = probing_df_key_words.drop([\"label\"], axis=1), probing_df_key_words[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = [82.29, 77.03, 79.66]\n",
    "brand_classsif = [\"LogisticRegression\", \"RandomForest\" ,\"XGB\"]\n",
    "\n",
    "plt.bar(brand_classsif, brand, color = [(30/255, 57/255, 240/255) , (84/255, 117/255, 232/255), (68/255, 103/255, 227/255)])\n",
    "plt.ylim([0,100])\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = EnglishDatasetLoader.load_train(dataset_type, dataset_size)\n",
    "res = train_df[[\"id_left\", \"id_right\", \"label\"]]\n",
    "res = res.rename({\"id_left\" : \"left_id\", \"id_right\":\"right_id\"}, axis = 1)\n",
    "\n",
    "sim_train_df = get_pairs_similarity_df(train_similarity_path)\n",
    "sim_train_df[\"right_id\"] = pd.to_numeric(sim_train_df[\"right_id\"])\n",
    "sim_train_df[\"left_id\"] = pd.to_numeric(sim_train_df[\"left_id\"])\n",
    "sim_df = pd.merge(res, sim_train_df, on =  [\"left_id\", \"right_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sim_df[\"cosine_score\"][sim_df['label']==0], color = \"blue\", label = \"label = 0\")\n",
    "sns.histplot(sim_df[\"cosine_score\"][sim_df['label']==1], color = \"orange\", alpha = 0.5, label = \"label = 1\")\n",
    "plt.legend()\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b90b5480b90dfd82255d68efb607ef96370ef33575f247c89a0b81cbaa1e7b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
