{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to be run on Google Colab.\n",
    "\n",
    "In this notebook, we performed:\n",
    "\n",
    "* Fine-tuning a BERT-like model (from the HuggingFace library) on WDC dataset - in the case of Project \\#2 the `xlm-roberta-base` model and `Computers medium` dataset.\n",
    "* Computing of the sentences embeddings for both pretrained and finetuned models\n",
    "* Computing their similarity for both pretrained and finetuned models\n",
    "* Saving embeddings and their similarity in files\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHdOMsFlarLu",
    "outputId": "1aa3892e-5ee5-4251-baec-586f59b36f48"
   },
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Sa9_Q9tddNe",
    "outputId": "1459002d-7739-476c-de2b-67bf474b7d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6kWzrlNd_E2"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x8XVVJYeAqa"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fine-tuning\n",
    "Fine-tuning a selected bi-encoders model on WDC to achieve better quality embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of self-defined modules\n",
    "# setting path\n",
    "import os, sys\n",
    "current_dir = os.path.abspath('')\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "from source.load_data.wdc.load_wdc_dataset import get_wdc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhJAvI3pa01R"
   },
   "outputs": [],
   "source": [
    "dataset_type = 'computers'\n",
    "dataset_size = 'medium'\n",
    "train_batch_size = 16\n",
    "num_epochs = 80 # since xml-roberta has more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXNyy86phZrs"
   },
   "outputs": [],
   "source": [
    "model_name = 'xlm-roberta-base' \n",
    "model_save_path = '/content/drive/MyDrive/NLP/output/training_wdc_'+dataset_type+'_'+dataset_size+'_'+model_name.replace(\"/\", \"-\")+'-'+datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Nc-2VakmbGK"
   },
   "outputs": [],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK1lxg7RAWUs"
   },
   "outputs": [],
   "source": [
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x2txR1zHpXl"
   },
   "outputs": [],
   "source": [
    "train_samples = get_wdc_dataset(dataset_type, dataset_size, is_train=True, features_to_concat=['title'])\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "test_samples = get_wdc_dataset(dataset_type, dataset_size, is_train=False, features_to_concat=['title'])\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='test_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOHOe-hWl99T"
   },
   "outputs": [],
   "source": [
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the model and save on a google drive\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8O63szpI5PO"
   },
   "source": [
    "# Preparing and saving embedding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of self-defined modules\n",
    "from source.emb_prep_res.compute_save_emb import get_embedding_records, get_embeddings_pairs, create_csv_file\n",
    "from source.emb_prep_res.compute_save_similiarity import compute_and_save_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqgT3f4cA-te"
   },
   "outputs": [],
   "source": [
    "dir = f'/content/drive/MyDrive/NLP/embeddings/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "embeddings_file_path_train = f'{dir}/train_embeddings.csv'\n",
    "embeddings_file_path_test = f'{dir}/test_embeddings.csv'\n",
    "\n",
    "field_names = ['id', 'embedding']\n",
    "model_save_path = '/content/drive/MyDrive/NLP/output/training_wdc_'+dataset_type+'_'+dataset_size+'_'+model_name.replace(\"/\", \"-\")+'-'+datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading saved model after fine-tuning from local path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06HvNxCJfDoo"
   },
   "outputs": [],
   "source": [
    "train_embeddings_1, train_embeddings_2 = get_embeddings_pairs(train_samples, model, batch_size=16)\n",
    "test_embeddings_1, test_embeddings_2 = get_embeddings_pairs(test_samples, model, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving embeddings for each offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpFXpnHWIKVz"
   },
   "outputs": [],
   "source": [
    "train_records = get_embedding_records(train_samples, train_embeddings_1, train_embeddings_2)\n",
    "test_records = get_embedding_records(test_samples, test_embeddings_1, test_embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3vyQIk-FSEr"
   },
   "outputs": [],
   "source": [
    "create_csv_file(embeddings_file_path_train, field_names, train_records)\n",
    "create_csv_file(embeddings_file_path_test, field_names, test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC8kLnD5I8X8"
   },
   "source": [
    "Computing and saving csv files with similarity measures for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlUtyNmwiBnM"
   },
   "outputs": [],
   "source": [
    "dir_sim = f'/content/drive/MyDrive/NLP/similarity/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "test_similarity_path = f'{dir_sim}/test_similarity.csv'\n",
    "train_similarity_path = f'{dir_sim}/train_similarity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKoIzo3_iCqT"
   },
   "outputs": [],
   "source": [
    "compute_and_save_similarity_scores(train_similarity_path, train_samples, train_embeddings_1, train_embeddings_2)\n",
    "compute_and_save_similarity_scores(test_similarity_path, test_samples, test_embeddings_1, test_embeddings_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e356dd047687aae92824e4bc0188a2818a6df554d25fe2bbe32ed0c4d26f6b28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
