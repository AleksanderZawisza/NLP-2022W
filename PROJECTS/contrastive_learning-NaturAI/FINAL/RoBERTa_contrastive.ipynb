{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d1e1e7be9c34ff7bcd7a17081f0c49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_271c9a8b5bfd49d08d95ce0572c58ea4",
              "IPY_MODEL_b005b28d5f164451b11ffde22c763de6",
              "IPY_MODEL_b51f91e20f1847bb85aec84ad397ba8f"
            ],
            "layout": "IPY_MODEL_d3219e0c942548dcb53439d00aaada77"
          }
        },
        "271c9a8b5bfd49d08d95ce0572c58ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6a98fe185f4bde9342c28b526f669b",
            "placeholder": "​",
            "style": "IPY_MODEL_e89035010c6b406bbcfb93616d108b6b",
            "value": "Downloading: 100%"
          }
        },
        "b005b28d5f164451b11ffde22c763de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5794438424b400db6ff27c09d180f0a",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20bf93499352484bbf7a9b59240d831c",
            "value": 481
          }
        },
        "b51f91e20f1847bb85aec84ad397ba8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_226d8c6968e14bc68509d849b6e66088",
            "placeholder": "​",
            "style": "IPY_MODEL_c9e04d3a3a4c47cb89c18cfba6a450e1",
            "value": " 481/481 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "d3219e0c942548dcb53439d00aaada77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6a98fe185f4bde9342c28b526f669b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e89035010c6b406bbcfb93616d108b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5794438424b400db6ff27c09d180f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20bf93499352484bbf7a9b59240d831c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "226d8c6968e14bc68509d849b6e66088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e04d3a3a4c47cb89c18cfba6a450e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5cbdfbe2454098ad58d2bb144e6787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c431b3a329e5482d83a249efb4bc124b",
              "IPY_MODEL_1cc8a0062c694fcdb6abb4b57de5b1fc",
              "IPY_MODEL_0579e99daeb7471c8a3bbf74ba187038"
            ],
            "layout": "IPY_MODEL_0e64e5cf909a4a818311549a70d7816e"
          }
        },
        "c431b3a329e5482d83a249efb4bc124b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29dbf1b2d53049f99fae7ab01dde69e1",
            "placeholder": "​",
            "style": "IPY_MODEL_507c86846e3248abb762afbbbd74af00",
            "value": "Downloading: 100%"
          }
        },
        "1cc8a0062c694fcdb6abb4b57de5b1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f21940b27549bb8ee10b0db5841355",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4f43282b75f4a2fb82679da0a052ea3",
            "value": 898823
          }
        },
        "0579e99daeb7471c8a3bbf74ba187038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_156eaa08ebef4cd59fc4a9017fd20769",
            "placeholder": "​",
            "style": "IPY_MODEL_678d1900792042a9b7423c409a6203c4",
            "value": " 899k/899k [00:01&lt;00:00, 934kB/s]"
          }
        },
        "0e64e5cf909a4a818311549a70d7816e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29dbf1b2d53049f99fae7ab01dde69e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "507c86846e3248abb762afbbbd74af00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1f21940b27549bb8ee10b0db5841355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f43282b75f4a2fb82679da0a052ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "156eaa08ebef4cd59fc4a9017fd20769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "678d1900792042a9b7423c409a6203c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e508a629bbb402e88bc06420e1846c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b392996f9704375884d78ff544ffafc",
              "IPY_MODEL_39728d4a681548449c39eb917c134baa",
              "IPY_MODEL_7887b5c224fc469e8e021cca3517fac8"
            ],
            "layout": "IPY_MODEL_a9da7cc5e332495b8514faab1b935526"
          }
        },
        "0b392996f9704375884d78ff544ffafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d40b5ee8dc64e1fa6561313cbccb97a",
            "placeholder": "​",
            "style": "IPY_MODEL_3dec7b9839d24aff8bd2abbb6d21a57e",
            "value": "Downloading: 100%"
          }
        },
        "39728d4a681548449c39eb917c134baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1ddc51401dc480882d76310c3d3718e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63e4fc9edcd04e45afc851730504264b",
            "value": 456318
          }
        },
        "7887b5c224fc469e8e021cca3517fac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9acab6e6f24b87862a21132af1d5ca",
            "placeholder": "​",
            "style": "IPY_MODEL_833bd1e4a71f44b0a7c95a7bcc3ed6a2",
            "value": " 456k/456k [00:01&lt;00:00, 476kB/s]"
          }
        },
        "a9da7cc5e332495b8514faab1b935526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d40b5ee8dc64e1fa6561313cbccb97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dec7b9839d24aff8bd2abbb6d21a57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1ddc51401dc480882d76310c3d3718e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e4fc9edcd04e45afc851730504264b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff9acab6e6f24b87862a21132af1d5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833bd1e4a71f44b0a7c95a7bcc3ed6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d220f2071284672b0665d6f9246486f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34962118422a46e0b0635c3c83cb839c",
              "IPY_MODEL_6c44b857ba994962bd38c6e09be1b02a",
              "IPY_MODEL_c72cbcd7fac140e5b64e4a986f8e5117"
            ],
            "layout": "IPY_MODEL_c6bddd227d564319b77674fa31b92950"
          }
        },
        "34962118422a46e0b0635c3c83cb839c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4560da840b97478fbdc40c336a043d60",
            "placeholder": "​",
            "style": "IPY_MODEL_2035228ef2f64bcb916bdf6a7607e1f2",
            "value": "Downloading: 100%"
          }
        },
        "6c44b857ba994962bd38c6e09be1b02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754d5467066f4da4a580957022d89a82",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a4930dc978549efa9c067bbbefee458",
            "value": 1355863
          }
        },
        "c72cbcd7fac140e5b64e4a986f8e5117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00da581750d4e36a02e689def39f3a3",
            "placeholder": "​",
            "style": "IPY_MODEL_77e1de70adb74a3793b986dad89372e3",
            "value": " 1.36M/1.36M [00:01&lt;00:00, 1.04MB/s]"
          }
        },
        "c6bddd227d564319b77674fa31b92950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4560da840b97478fbdc40c336a043d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2035228ef2f64bcb916bdf6a7607e1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754d5467066f4da4a580957022d89a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4930dc978549efa9c067bbbefee458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b00da581750d4e36a02e689def39f3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e1de70adb74a3793b986dad89372e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0df9b3995b446f0b463ccbb31e98d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82437f58eb074d3db59e7514cc761d12",
              "IPY_MODEL_58ac61fd40c745c297553381008931e1",
              "IPY_MODEL_61efd3c00555403397d6046bca4d8f50"
            ],
            "layout": "IPY_MODEL_f232f914fbc6404babd39acc9a66d256"
          }
        },
        "82437f58eb074d3db59e7514cc761d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eec98615cea4686b1123291c0a6063a",
            "placeholder": "​",
            "style": "IPY_MODEL_0b02ee3e507b450fb6517c67ff0e0a53",
            "value": "Downloading: 100%"
          }
        },
        "58ac61fd40c745c297553381008931e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bb3434dfe674c1f84d01ff072665b89",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_526e5302e5fb4a8185e1d62e9d5d1caf",
            "value": 501200538
          }
        },
        "61efd3c00555403397d6046bca4d8f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6619745f15748b793d38e968958ed01",
            "placeholder": "​",
            "style": "IPY_MODEL_3788b36686974b7ba98c1a9ddc156b5e",
            "value": " 501M/501M [00:10&lt;00:00, 51.0MB/s]"
          }
        },
        "f232f914fbc6404babd39acc9a66d256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eec98615cea4686b1123291c0a6063a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b02ee3e507b450fb6517c67ff0e0a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bb3434dfe674c1f84d01ff072665b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526e5302e5fb4a8185e1d62e9d5d1caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6619745f15748b793d38e968958ed01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3788b36686974b7ba98c1a9ddc156b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03qdeOg8ga6r"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install pytorch_metric_learning -q\n",
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ZLupGY2PtzET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/NLP_Projekt/\n",
        "from RobertaContrastiveModel import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp9ZwcqvglvF",
        "outputId": "270c2147-42d7-4327-997c-da2b433fc361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/MojePliki/Studia aktualne/NLP_Projekt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model hyperparameters"
      ],
      "metadata": {
        "id": "RjBT5iqLPCiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_args(n, use_rlarge):\n",
        "    if use_rlarge:\n",
        "        modeln = \"roberta-large\"\n",
        "    else:\n",
        "        modeln = \"roberta-base\"\n",
        "    args = Namespace(\n",
        "        max_length = 64,\n",
        "        learning_rate=1e-5,\n",
        "        num_warmup_steps=10,\n",
        "        eps=1e-08,\n",
        "        model_name=modeln,\n",
        "        model_type=\"softriple\",\n",
        "        weight_decay=0.01,\n",
        "        la=8,\n",
        "        supcon_temp=0.1,\n",
        "        gamma=0.1,\n",
        "        margin=0.1,\n",
        "        centers=5,\n",
        "        beta=0.4,\n",
        "        seed=2048,\n",
        "        output_dir=\"./result\",\n",
        "        save_steps=220,\n",
        "        epochs=100,\n",
        "        num_training_steps=100,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        sample_size=n,\n",
        "        n_split=2,\n",
        "        dataset_name=\"imdb\",\n",
        "        softmax_scale=1,\n",
        "        alpha=0.1,\n",
        "        extended_inference=0.1)\n",
        "    return args"
      ],
      "metadata": {
        "id": "UQF51CPxiNTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = create_args(20, False)"
      ],
      "metadata": {
        "id": "biqh4KiYtWQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "P-BOpmGXcME7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "results, trainer = cross_validate(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5d1e1e7be9c34ff7bcd7a17081f0c49d",
            "271c9a8b5bfd49d08d95ce0572c58ea4",
            "b005b28d5f164451b11ffde22c763de6",
            "b51f91e20f1847bb85aec84ad397ba8f",
            "d3219e0c942548dcb53439d00aaada77",
            "bf6a98fe185f4bde9342c28b526f669b",
            "e89035010c6b406bbcfb93616d108b6b",
            "b5794438424b400db6ff27c09d180f0a",
            "20bf93499352484bbf7a9b59240d831c",
            "226d8c6968e14bc68509d849b6e66088",
            "c9e04d3a3a4c47cb89c18cfba6a450e1",
            "ea5cbdfbe2454098ad58d2bb144e6787",
            "c431b3a329e5482d83a249efb4bc124b",
            "1cc8a0062c694fcdb6abb4b57de5b1fc",
            "0579e99daeb7471c8a3bbf74ba187038",
            "0e64e5cf909a4a818311549a70d7816e",
            "29dbf1b2d53049f99fae7ab01dde69e1",
            "507c86846e3248abb762afbbbd74af00",
            "b1f21940b27549bb8ee10b0db5841355",
            "c4f43282b75f4a2fb82679da0a052ea3",
            "156eaa08ebef4cd59fc4a9017fd20769",
            "678d1900792042a9b7423c409a6203c4",
            "7e508a629bbb402e88bc06420e1846c1",
            "0b392996f9704375884d78ff544ffafc",
            "39728d4a681548449c39eb917c134baa",
            "7887b5c224fc469e8e021cca3517fac8",
            "a9da7cc5e332495b8514faab1b935526",
            "3d40b5ee8dc64e1fa6561313cbccb97a",
            "3dec7b9839d24aff8bd2abbb6d21a57e",
            "f1ddc51401dc480882d76310c3d3718e",
            "63e4fc9edcd04e45afc851730504264b",
            "ff9acab6e6f24b87862a21132af1d5ca",
            "833bd1e4a71f44b0a7c95a7bcc3ed6a2",
            "1d220f2071284672b0665d6f9246486f",
            "34962118422a46e0b0635c3c83cb839c",
            "6c44b857ba994962bd38c6e09be1b02a",
            "c72cbcd7fac140e5b64e4a986f8e5117",
            "c6bddd227d564319b77674fa31b92950",
            "4560da840b97478fbdc40c336a043d60",
            "2035228ef2f64bcb916bdf6a7607e1f2",
            "754d5467066f4da4a580957022d89a82",
            "4a4930dc978549efa9c067bbbefee458",
            "b00da581750d4e36a02e689def39f3a3",
            "77e1de70adb74a3793b986dad89372e3",
            "c0df9b3995b446f0b463ccbb31e98d60",
            "82437f58eb074d3db59e7514cc761d12",
            "58ac61fd40c745c297553381008931e1",
            "61efd3c00555403397d6046bca4d8f50",
            "f232f914fbc6404babd39acc9a66d256",
            "1eec98615cea4686b1123291c0a6063a",
            "0b02ee3e507b450fb6517c67ff0e0a53",
            "3bb3434dfe674c1f84d01ff072665b89",
            "526e5302e5fb4a8185e1d62e9d5d1caf",
            "b6619745f15748b793d38e968958ed01",
            "3788b36686974b7ba98c1a9ddc156b5e"
          ]
        },
        "id": "f3enEo6MirSs",
        "outputId": "94bd1a78-b8df-4818-e881-c795f50ce9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d1e1e7be9c34ff7bcd7a17081f0c49d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea5cbdfbe2454098ad58d2bb144e6787"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e508a629bbb402e88bc06420e1846c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d220f2071284672b0665d6f9246486f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0df9b3995b446f0b463ccbb31e98d60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaContrastiveLearning: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'clf_loss.fc', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 124654850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:23, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 667\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaContrastiveLearning: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'clf_loss.fc', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 124654850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:24, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 667\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaContrastiveLearning: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'clf_loss.fc', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 124654850\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:25, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 666\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYh06C-htk1e",
        "outputId": "6f36c147-0725-4407-b1c5-288947770768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.540727971716013, 0.12077990318359527],\n",
              " [0.8029228570432849, 0.16340788895894603],\n",
              " [0.5860343101722413, 0.07526330328860029],\n",
              " [0.5825695664804575, 0.07803253845640629]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = prepare_dataset('imdb')"
      ],
      "metadata": {
        "id": "PEZ1i41hvdcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0waI_As8vjgH",
        "outputId": "5fa79c18-4a6d-4958-e864-33e327888cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence label\n",
              "1860  Yes, it feels, and for the most part plays lik...     1\n",
              "353   This film held my interest enough to watch it ...     1\n",
              "1333  i am surprised so few have good words for this...     1\n",
              "905   Well where do I begin my story?? I went to thi...     0\n",
              "1289  I waited for this movie to play in great antic...     0\n",
              "...                                                 ...   ...\n",
              "1130  I haven't had a chance to view the previous fi...     0\n",
              "1294  This movie was a masterpiece of human emotions...     1\n",
              "860   This production was quite a surprise for me. I...     1\n",
              "1459  Absolutely wonderful drama and Ros is top notc...     1\n",
              "1126  Amicus made close to a good half dozen of thes...     1\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85a97fb9-80f1-4102-b4b0-2cfa018f33ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1860</th>\n",
              "      <td>Yes, it feels, and for the most part plays lik...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>This film held my interest enough to watch it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>i am surprised so few have good words for this...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>Well where do I begin my story?? I went to thi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1289</th>\n",
              "      <td>I waited for this movie to play in great antic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>I haven't had a chance to view the previous fi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>This movie was a masterpiece of human emotions...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>This production was quite a surprise for me. I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>Absolutely wonderful drama and Ros is top notc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>Amicus made close to a good half dozen of thes...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85a97fb9-80f1-4102-b4b0-2cfa018f33ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85a97fb9-80f1-4102-b4b0-2cfa018f33ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85a97fb9-80f1-4102-b4b0-2cfa018f33ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "test_dataset = CLDatasetClassification(np.arange(len(df)), df, tokenizer, 64)"
      ],
      "metadata": {
        "id": "peTGVeEyyybx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec63447-9a7c-4206-d859-ba2215797190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "Oib2sMHwvSKz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "44548974-0c43-4a62-d40a-5d5eb99cdc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[0.21922123, 0.78077877],\n",
              "       [0.21736833, 0.78263164],\n",
              "       [0.21969764, 0.7803024 ],\n",
              "       ...,\n",
              "       [0.57942134, 0.4205787 ],\n",
              "       [0.24741371, 0.7525863 ],\n",
              "       [0.2288537 , 0.7711463 ]], dtype=float32), label_ids=array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]]), metrics={'test_loss': 0.2831921875476837, 'test_accuracy_score': 0.6735, 'test_f1_score': 0.6505724673032549, 'test_recall_score': 0.9273631840796019, 'test_precision_score': 0.6164021164021164, 'test_runtime': 7.5535, 'test_samples_per_second': 264.779, 'test_steps_per_second': 4.236})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot model performance based on sample size"
      ],
      "metadata": {
        "id": "VSMFY1OBcSdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "accs = []\n",
        "f1s = []\n",
        "for n in np.arange(10,101,10):\n",
        "    args = create_args(n, False)\n",
        "    results, trainer = cross_validate(args)\n",
        "    accs.append(results[2][0])\n",
        "    f1s.append(results[0][0])"
      ],
      "metadata": {
        "id": "G7aqlN7Lcb6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ns = np.arange(10,101,10)"
      ],
      "metadata": {
        "id": "TzcpTmPxfeHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "YQBty7Nvl5Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ns,accs)\n",
        "plt.title('Accuracy for RoBERTa Base model')\n",
        "plt.xlabel('n train')\n",
        "plt.ylabel('acc')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "fObt9Ih4faMH",
        "outputId": "0b10cd28-3792-4385-f27e-027a9c2bbf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ns,f1s)\n",
        "plt.title('F1-Score for RoBERTa Base model')\n",
        "plt.xlabel('n train')\n",
        "plt.ylabel('F1')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "haprkxQNn1ZH",
        "outputId": "51df6f65-68cb-4d8a-8a16-815577a030bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV9b3/8deHrCyBsARIwiqbIAhoRHDDqq2gFWjtorVXbat0o9tttdZ6rdretnbX1tv7Q622VxSXKqLFrS4F3Fllk0UChLAkAcKe/fP7Yyb2EAMEyMkkOe/n43EeOWe28zmTOfOe5cx3zN0REZHE1SbqAkREJFoKAhGRBKcgEBFJcAoCEZEEpyAQEUlwCgIRkQSnIJBmy8yGmNkSM9trZt+Ouh6Jlpm5mQ1swHDnm9nmpqiptVAQtGBmtsHMDprZvphHTthvupmtNrMaM7u2AdP6ipm9H650t5vZHDPLiPuHOLIbgVfdPcPd7z7RiZnZbWZWGc6nUjN7w8zGHce4+8xslZldHtP//HBe76vzGBf2f83MysJuJWb2pJllm9nNMcOWmVl1zOsVx/DZHjSzinC8vWa20MzGH/tckkSkIGj5LnP3DjGPLWH3pcA3gEVHm0C4wvg5cKW7ZwBDgUcbs0gzSz6O0foCDV4ZNvD9HnX3DkA34FXg8WOY7KO18xn4LvCQmfWI6b+lzv+ig7u/GdN/WjjuQKAD8Bt3/3nMNL8GvBkz7inHUBvAr8LpdAT+DDxpZknHOA1JQAqCVsrd73H3l4GyBgx+BsEKaHE47k53/6u77wUws7Zm9lsz22hmu81svpm1DftNMrMV4Rb2a2Y2tHai4R7LD83sPWC/mSWb2dhwS7zUzJaa2fn1FWRmrwAfA/4UbuUONrNOZvY3MysOa7nFzNqEw19rZq+b2e/NbAdw21HmTxUwA8g1s6xwGjlmNtvMdprZOjO7/gjjvwDsBQY0YP7WHbcUmAWMOtJwZnaXmRWY2Z5wC//cBk7fgYeBLkCPcFoDzOwVM9sR7pHMMLPMmPf6oZkVhnsTq83swrB7GzO7ycw+CMd9zMy6HKbe881ss5ndaGZFZrbVzKaY2SVmtiacrzfHDJ9mZn8wsy3h4w9mlhbT/4ZwGlvM7Mt13ivNzH5jZpss2IP939plUo6dgkAA3gYuNrPbzezs2C9j6DfA6cBZBCuXG4EaMxsMPEKwdZwFzAGeMbPUmHGvBC4FMglWSv8AfhZO5wfA32tXxLHc/QJgHuFWtLuvAf4IdAJOAsYDVwNfihntTGB9+D7/faQPHNZ4NbAD2BV2nglsBnKAzwA/N7ML6hnXzOxSIBVYeaT3Ocx7dwU+Daw7yqDvEoRFF4IV++Nmlt6A6ScRfLZ8YHttZ+AXBJ9tKNCbMCzNbAgwDTgj3CO8GNgQjvctYArB/M4hmFf3HOHtewLpQC5wK3Av8EWC5edc4L/MrH847I+BseFnHAmMAW4Ja5pAsHx8HBgEXFTnfX4JDA7HHRjzfnI83F2PFvog+LLuA0rDx6x6hpkPXNuAaU0Engmnsw/4HZBEsLFwEBhZzzj/BTwW87oNUAicH1Pfl2P6/xD4vzrTeAG45jA1vQZcFz5PAiqAYTH9vwq8Fj6/Fth0lM94WziNUqCaIARqa+0ddsuIGf4XwIP1jLs/HPbGmGHPB2pi/he1j/Yxn+UAsBtwYAnQp0591wLzj1D/rvr+D2G/Bwn2/krD/1cZcNURpjUFWBw+HwgUEaxsU+oMtwq4MOZ1NlAJJNczzfPD904KX2eEn/XMmGEWAlPC5x8Al8T0uxjYED7/C/DLmH6Dw2kNJAi1/cCAmP7jgPyYOjZH/f1sSQ/tEbR8U9w9M3xMacgIdujJzD4A7v6cu19GsPU5mWCldB3BsfR0gi9tXTnAxtoX7l4DFBBsndUqiHneF/hseFio1MxKgXMIVi5H0w1IiX2/8Pnh3utwHnP32r2T5QRbqrWfZaeHh8MOM/3HwvncnuCQ0NVm9tWY/lti/he1j/0x/b/t7p2AU4HOQK8jFWpmP7DgpPTucF51IpgPh/Ob8LO1A/KAX5vZxHBaPcxsZnj4Zw/wUO203H0dwV7dbUBROFxOOM2+wFMx/69VBCEYe24k1g53rw6fHwz/bo/pf5Dg/AjUWX7C5zkx/Qrq9KuVFX7GhTF1PR92l+OgIEhAfujJzE11+tV4cG7hFWA4UEKwdVnfsfAtBCsKIDhkQrBlXRg7yZjnBQR7BLEryvbu/ssGlF1CsCXaN6ZbnyO81xG5ewkwFbjNzLLDz9LFDv2lVN3px46/AXgOuKyh7xkz7jKCw2P3hPPsI8LzATcCnwM6hyv43QRbw0ebvrv7cuB1gsNyEPwYwIER7t6R4HCNxYzzsLufQzB/Hbgz7FUATKzzP0t393rnyzE6ZPkhmN+1P3bYSrAsxfarVUIQKKfE1NTJgxPlchwUBK2UmaWGx5MNSDGz9NoTq/UMO9nMrjCzzuHx7zEEx4TfCrfy/wL8LjyZmmRm48LzCI8Bl5rZhWaWAnwfKAfeOExZDwGXmdnF4XTSwxOMR9wyBgi3Mh8D/tvMMsysL/Cf4TSPi7uvJjg0daO7F4R1/yKs61TgK4ebfljzBI7zV03AXwm2qicdpn8GUAUUA8lmdivBr4EaxMxOJtjbqq0vg+CQ324zywVuiBl2iJldEP5PywhWsjVh7/8lmOd9w2GzzGxyQ+s4ikeAW8JpdiM4xl87vx8DrjWzYWbWDvhJ7UjhMnkv8Hsz6x7WlWtmFzdSXQlHQdB6vUjwhT4LmB4+P+8ww+4CrgfWArWHDX7t7jPC/j8AlhGcvNxJsLXYJlyRfpHgJG4JwdbxZe5eUd+bhCvbycDNBCu4AoIVUkOXw28RHBteT3Du42GCkDoRvwamhiuUK4F+BFulTwE/cfd/xgz7+dpDagTz4nXg9pj+OfbR6wgupx7hPLqL4DxLfV4gONyxhuCwSBlHP/R1Y/ie+wn+/w8A/y/sdztwGsFexT+AJ2PGSyM4+VoCbAO6Az8K+90FzAZeNLO9wFsEJ+Ubw8+ABcB7BMvXorAb7v4c8AeCPdN14d9YPwy7vxUe6vonMKSR6ko45q4b04iIJDLtEYiIJDgFgYhIgotrEJjZBAuuUlxnZjfV07+Pmb1qZovN7D0zuySe9YiIyEfF7RxBeHXjGoIrAzcTnFy70t1XxgwzneCilj+b2TBgjrv3i0tBIiJSr+NpCKyhxgDr3H09gJnNJPjFSOwl+c6/fxLXiX//hviwunXr5v369WvcSkVEWrmFCxeWuHu9F93FMwhyOfTnbpv56M/ObiP4Wdq3gPZ8tD0RAMxsKsHFP/Tp04cFCxY0erEiIq2ZmW08XL+oTxZfSdCWSy/gEuD/6rvoyd2nu3ueu+dlZekqchGRxhTPICjk0EvEe/HRy/W/QnAFIR60257OkdtSERGRRhbPIHgXGGRm/cMmf68guEIx1iagtt3zoQRBUBzHmkREpI64BYEHN/6YRnCp/CqClhtXmNkdZlbbvsr3gevNbClBuyPXui51FhFpUvE8WYy7zyG4WUlst1tjnq8Ezo5nDSIicmRRnywWEZGIKQhERBJcXA8NiUjLs2rrHv61ppgeHdPIzWxHTmY6PTumk5yk7cbWSkEgIkBw//KH39nE7bNXUlFdc0i/NgY9O6aTk9mW3M5tyckMHr3Cv7md29IhrWWuTtydveVVFO0pp3hvOUV7y8K/5RTtKaNkXwUDstozeXQuo3tncpibyrVoLfM/JyKN6kBFFbc8tZwnFxdy3uAs7rx8BAcqqtlSepDCXQeDv6VlFJYeYPGmUuYs20pl9aE/8OuYnkxu53bkZoaBERMSuZltyeqQRps2TbcSralxduyvoGhvGUV7g5V8cbhyLwpX9LUr/rLKmo+Mn5rchu4ZaXRtn8rMdwv465sb6du1HZNH5jB5dC4DslrPnTFb3I1p8vLyXE1MiDSeD4r38fWHFrK2aB/fu2gw0z428Kgr7Ooap2RfOZvDkAiCIvhb221PWdUh46QkGdmd2pKTmU5uZkxghHsYuZltSU9JOmq95VXVMVvs5RTHbsGHK/aiPeXs2F9Bdc1H128Z6cl0z0ije0Y63Tum0T0jjaza1xlpdO+YRlZGOh3Tkz/c+t9bVsnzy7fx9JItvPFBCTUOp/bqxORRuVw2MpvuGenHMMejYWYL3T2v3n4KApHE9czSLdz09/dIS0niritGce6gxmvCZW9ZJVvCvYjC0rI6excH2b6njLrr6a7tUw/Zm0hOsg+32ov2BCv63QcrP/JeZtC1fdqHK/L6Vu7dM9LJykhrUNgcyfY9ZTyzdAuzlhSyvHAPbQzOHtiNKaNyuXh4z2Z7iExBICKHqKiq4edzVvHgGxs4vW9n/vSF0WR3atukNVRW17BtdxAQW3YHIREcfjr4YWhU13iwQj/Cyr17Rhpd2qdGcjJ7XdFenl4ShELBzoOkp7ThoqE9mDIql/MGZ5Ga3HxOsCsIRORDhaUH+eaMRSwpKOW6c/rzw4knk9IMfxFUu25qCSdn3Z1Fm3Yxa/EWnn1vC7sOVNK5XQqXnprNlFG5nN63c+SfQ0EgIgC8trqI7z66hKpq59efOZWJI7KjLqnVqayuYd7aYp5avIWXVm6jrLKGXp3bMnlUDlNG5TKoR0YkdSkIRBJcdY1z18tr+eMraxnSI4M/f/F0+ndrH3VZrd6+8ipeXLGNWUu2MH9tMTUOp+R0ZMqoXCaNyqFHx6Y7yawgEElgO/aV852ZS5i/roTPnN6Ln04eTtvUEzthKseuaG8Zzy7dytNLClm6eTdmMO6krkwZncuE4T3pmJ4S1/dXEIgkqAUbdjLt4cXsOlDBTycP53Nn9D76SBJ364v3fXiSeeOOA6Qmt+Giod2ZPCqX84dkkZbc+EGtIBBJMO7O/fPz+eVz75PbuS3/c9VpnJLTKeqypA53Z0lBKU8v2cIzS7ewY38FndqmcMmIbKaMyuGMfl0a7SI8BYFIAtlTVsmNj7/H8yu2cfEpPfj1Z0fG/bCDnLjK6hrmryvh6cWFvLBiOwcrq8nNbMtlI3P41OhchvQ8sZPMCgKRBLFyyx6+MWMhBbsOctOEk7nu3P6R/2xRjt2BiipeWrmdpxYXMm9tCdU1zsk9M7hxwhAuOLnHcU3zSEHQPC+BE5Fj9viCAm6ZtZxObVOYOXUsZ/TrEnVJcpzapSYzeVQuk0flUrKvnH+8t5VZSwox4hPqCgKRFq6sspqfPL2CRxcUcNaArtx1xWiyMtKiLksaSbcOaVxzVj+uOatf3N5DQSDSgm0o2c/XZyxi1dY9fOuCgXz3osEkNWELn9I6KAhEWqjnl2/jhseXkpRkPPClM/jYkO5RlyQtlIJApIWprK7hV8+/z73z8hnZqxP3XHUavTq3i7osacEUBCItyLbdZUx7eBELNu7imnF9ufnSoXG5+EgSi4JApIV4fV0J35m5mAMV1dx95WgmjcyJuiRpJRQEIs1cTY3zP6+t43cvrWFAVgdmTj2Ngd2jacFSWicFgUgztmt/Bd97bAmvrS5myqgcfv7pEbRL1ddWGpeWKJFmaklBKd+csYjiveX8bMpwrjqzj64SlrhQEIg0M+7O/721kZ8+u5IeHdN54uvjOLVXZtRlSSumIBBpRvaXV3HTk8t4ZukWLjy5O7/93Egy26VGXZa0cgoCkWaiYOcBrn3gHfJL9nPjhCF87bwBjdYEsciRKAhEmoGyymq+PmMhxXvLmXHdWMYN6Bp1SZJAFAQizcBPn13J8sI93Hd1nkJAmlybqAsQSXRPLylkxtub+Or4k7ho2PG1NS9yIhQEIhFaV7SXHz25jDH9unDDJ4ZEXY4kKAWBSET2l1fxtYcW0S41iT9+YTTJSfo6SjR0jkAkAu7Oj59axvrifTz0lTPp0TE96pIkgWkTRCQCD7+ziVlLtvC9iwZz1sBuUZcjCU5BINLElhfu5vbZKxk/OItvfmxg1OWIKAhEmtLug5V8fcZCunZI5fefH6ULxqRZ0DkCkSbi7tzw+FK2lpbx6FfH0aW9mo6Q5iGuewRmNsHMVpvZOjO7qZ7+vzezJeFjjZmVxrMekSjdNy+fF1du50eXDOX0vp2jLkfkQ3HbIzCzJOAe4OPAZuBdM5vt7itrh3H378UM/y1gdLzqEYnSgg07+eXz7zNxeE++fHa/qMsROUQ89wjGAOvcfb27VwAzgclHGP5K4JE41iMSiR37ypn28GJ6d27LnZ85VfcUkGYnnkGQCxTEvN4cdvsIM+sL9AdeOUz/qWa2wMwWFBcXN3qhIvFSXeN899El7DxQwT1XnUbH9JSoSxL5iObyq6ErgCfcvbq+nu4+3d3z3D0vKyuriUsTOX53v7yWeWtLuGPSKZyS0ynqckTqFc8gKAR6x7zuFXarzxXosJC0MnPXFHP3K2v59Gm5fP6M3kcfQSQi8QyCd4FBZtbfzFIJVvaz6w5kZicDnYE341iLSJPauvsg3310CYO6d+BnU4brvIA0a3ELAnevAqYBLwCrgMfcfYWZ3WFmk2IGvQKY6e4er1pEmlJldQ3THl5MeWU1/3PV6bRL1eU60rzFdQl19znAnDrdbq3z+rZ41iDS1H71/Pss3LiLu68czcDuHaIuR+SomsvJYpFW4YUV27h3Xj5Xj+vLpJE5UZcj0iAKApFGsmnHAX7w+FJG9urEjy8dGnU5Ig2mIBBpBLU3n29jxp++cBppyUlRlyTSYDqLJdII7nh2JSu27OH+a/Lo3aVd1OWIHBPtEYicoKcWb+bhtzfxtfEDuHCobj4vLY+CQOQErNm+l5ufXM6Y/l34wScGR12OyHFREIgcp/3lVXxjxiLapyXxpyt183lpuXSOQOQ4uDs3x9x8vrtuPi8tmDZhRI7DjLc38bRuPi+thIJA5Bgt27ybO57Rzeel9VAQiByD3Qcq+cbDuvm8tC46RyDSQO7OD57Qzeel9dEegUgD3TtvPS/p5vPSCikIJFLz15awvHA3zb0V8nfyd3Ln86t183lplXRoSCLzTv5Ovnj/2wD06dKOiSN6cumIbEbkdmpWN3Ip2VfOtx5ZpJvPS6ulIJBIVFTVcPNTy8jNbMu0Cwby/PJt3D8vn//3r/X06tyWS0Zkc8mIbEb2ijYUqmuc78xcTOmBSh74xhjdfF5aJQWBROLeeetZV7SP+6/J48KhPbhyTB9KD1Tw4srtPLdsKw+8ns/0uevJzWzLxOE9mTgim9G9M5v8Vzp3vbyW19ft4M7LRzAsp2OTvrdIU1EQSJPbuGM/d7+8lonDex7SSFtmu1Q+l9ebz+X1ZveBSl5aFYTC397cyH3z88nulM7E4dlcMqInp/XpHPdQmLummD++spbLT+vF5/J083lpvay5n6SrKy8vzxcsWBB1GXKc3J1rHniXRRt38c//HE/PTkdvmmFPWSUvr9rOP97bxty1xVRU1dCzYzoThvfkkhHZ5PVt/FDYuvsgl949n6wOacz65tm0TdX9BaRlM7OF7p5XXz/tEUiTeva9rcxdU8xPLhvWoBAA6JiewqdG9+JTo3uxt6ySV94v4h/vbeXhdzbx4Bsb6J6R9mEonNGvC0knGAqH3Hz+i6cpBKTVUxBIk9l9sJI7nl3JiNxOXD2u33FNIyM9hcmjcpk8Kpd95VW88n4Rzy3bymMLCvjbmxvp1iGNCcN7cMmIbMb063JcLYLe+Vxw8/k/XjmaAVm6+by0fgoCaTK/eWE1O/aV85drzjjhrXaADmnJTBqZw6SROewvr+K11cXMWbaVvy8s5KG3NtG1fSoXD+/JJcOzGXtSw0Lh+eVbuW9+cPP5y3TzeUkQCgJpEksKSnno7Y1cM64fI3p1avTpt09L5tJTs7n01GwOVlTz2uoi/rFsK7MWF/Lw25vo0j6Vi0/pwcTh2Ywb0JWUekJhQ8l+bnj8Pd18XhKOgkDirqq6hpufXEb3jDS+3wR38WqbmsTEEdlMHJFNWWX1h3sKs5ds4ZF3Cshsl8InhgWHj84e2I2UpDaUVVbzjRmLaNNGN5+XxKMgkLh78I0NrNy6hz9fdRoZTXxBVnpKEhOG92TC8J6UVVYzd00xzy3fxnPLtvHYgs10apvCx4f1YH95FSu36ubzkpgUBBJXhaUH+e2La7jg5O5MGN4z0lrSU5L4xCk9+cQpPSmvqmb+2hL+sWwrL6zYxt6yKt18XhKWgkDi6idPr8Bxbp90SrNqoyctOYkLh/bgwqE9qKiq4f1texie0/jnLkRaArU+KnHzwopt/HPVdr570eBmfbglNbkNp/Zq+uYrRJoLBYHExb7yKm6bvYKTe2bwlXP6R12OiByBDg1JXPz+pTVs3V3Gn74wut6faopI86FvqDS65YW7eeD1fK4c04fT+3aJuhwROQoFgTSq6hrnx08to0v7VG6acHLU5YhIAygIpFHNeHsjSzfv5pZLh9GpnW7iItISKAik0WzfU8avn1/NOQO7MXmU2ukRaSkUBNJo7nh2JeXVNfx0yvBmdc2AiByZgkAaxWurg3sETPvYQPp3ax91OSJyDBQEcsIOVlTzX08v56Ss9nx1/ElRlyMix0jXEcgJ++MraynYeZBHrh+rVjtFWiDtEcgJWbN9L9Pnrufy03oxbkDXqMsRkeMQ1yAwswlmttrM1pnZTYcZ5nNmttLMVpjZw/GsRxpXTXjNQIf0ZN3IRaQFi9uhITNLAu4BPg5sBt41s9nuvjJmmEHAj4Cz3X2XmXWPVz3S+B5fWMC7G3bxq8tPpUv71KjLEZHjFM89gjHAOndf7+4VwExgcp1hrgfucfddAO5eFMd6pBHt2FfOL557nzH9uvDZvF5RlyMiJyCeQZALFMS83hx2izUYGGxmr5vZW2Y2ob4JmdlUM1tgZguKi4vjVK4ci/+es4r95VX896d0zYBISxf1yeJkYBBwPnAlcK+ZZdYdyN2nu3ueu+dlZWU1cYlS1xsflPDkokKmnncSg3pkRF2OiJygeAZBIdA75nWvsFuszcBsd69093xgDUEwSDNVXlXNLU8tp0+XdnzrAv2rRFqDeAbBu8AgM+tvZqnAFcDsOsPMItgbwMy6ERwqWh/HmuQE/fm1D1hfsp+fThlOeoquGRBpDeIWBO5eBUwDXgBWAY+5+wozu8PMJoWDvQDsMLOVwKvADe6+I141yYlZX7yP/3n1Az55ajbjB+sQnUhrEdcri919DjCnTrdbY5478J/hQ5oxd+eWWctJS27DrZ8cFnU5ItKIoj5ZLC3ErCWFvPHBDm6cMITuHdOjLkdEGpGCQI6q9EAFP3t2FSN7Z/KFM/tGXY6INDI1OidHdefz71N6sJK/fWo4SW10zYBIa3PcewRmphvSJoAFG3byyDsFfPnsfpyS0ynqckQkDk7k0NCLjVaFNEuV1TX8+Knl5HRK57sXDY66HBGJkyMeGjKzuw/XC/jIFcDSutw3L5/V2/dy79V5tE/TUUSR1upo3+4vAd8Hyuvpd2XjlyPNRcHOA9z18ho+MawHHx/WI+pyRCSOjhYE7wLL3f2Nuj3M7La4VCSRc3dufXo5SWbcNumUqMsRkTg7WhB8Biirr4e792/8cqQ5eG75Nl5dXcwtlw4lJ7Nt1OWISJwd7WRxB3c/0CSVSLOwt6yS259ZwbDsjlx7Vr+oyxGRJnC0IJhV+8TM/h7nWqQZ+O2LayjaW87PPz2C5CRdbyiSCI72TY+9euikeBYi0Xtvcyl/fXMD/zG2L6N660dhIoniaEHgh3kurUxVdQ03P7WMrA5p/ODiIVGXIyJN6Ggni0ea2R6CPYO24XPC1+7uHeNanTSZv725keWFe/jTF0bTMT0l6nJEpAkdMQjcXXceSQBbdx/kty+uZvzgLC4dkR11OSLSxHQ2ULh99kqqapyfTtaN6EUSkYIgwf1z5XaeX7GNb184iD5d20VdjohEQEGQwA5UVPGT2SsY1L0D15+rH4WJJCq1JJbA/vDPtRSWHuTxr40jNVnbBCKJSt/+BLVyyx7un5/P5/N6c0a/LlGXIyIR0h5BAtlbVsnr63bwrzXFvLRyO5ltU/jRJbq/kEiiUxC0YjU1zsqte/jXmmL+taaYRRt3UVXjdEhL5uyBXZl63klktkuNukwRiZiCoJXZtb+CuWuDFf/cNSWU7AtuJTEsuyNTzzuJ8YOzOK1vZ1LUjpCIhBQELVx1jbN0cyn/Wh2s/JduLsUdMtulcO6gLMYPzuK8Qd3o3jE96lJFpJlSELRARXvKPjzcM29tCbsPVmIGo3pn8p0LBzF+cBan9sokqY0uDhORo1MQtACV1TUs3LiLf60p5rXVxazaGjT51K1DGhcN7cH5Q7I4Z2A3OrfX8X4ROXYKgmZq864DwVb/6mLe+GAH+8qrSG5jnN63MzdOGML4wVkM7dmRNtrqF5ETpCBoJsoqq3knf2e41V/EB8X7AcjNbMukUTmMH5zFWQO6kqGWQUWkkSkIIuLu5Jfs//BY/1vrd1BWWUNqchvO7N+FL5zZl/GDsxiQ1V4NwYlIXCkImtiGkv3cPz+f19YUUbDzIAAndWvPFWf0YfyQLMb270rbVLX+LSJNR0HQhNydaY8sYl3RPs4ZmMXU8wYwflCWWv0UkUgpCJrQm+t3sLxwD7/49AiuHNMn6nJERAA1Otek7p27nm4dUvnU6NyoSxER+ZCCoIms2b6XV1cXc/W4fqSn6ByAiDQfCoImct+89aSntOGLY/tGXYqIyCEUBE2gaE8ZsxZv4bOn96aLrv4VkWZGQdAE/vrmBipravjKOf2jLkVE5CMUBHG2v7yKh97axMXDetKvW/uoyxER+QgFQZw9vqCA3Qcruf483RxeRJqnuAaBmU0ws9Vmts7Mbqqn/7VmVmxmS8LHdfGsp6lVVddw/+v5nN63M6f37Rx1OSIi9YrbBWVmlgTcA3wc2Ay8a2az3X1lnUEfdfdp8aojSi+s2E7BzoPccumwqEsRETmseO4RjAHWuft6d68AZgKT4/h+zYq7M33uB/Tv1p6LhvaIuhwRkcOKZxDkAgUxrzeH3eq63MzeM7MnzKx3fRMys6lmtsDMFhQXF8ej1kb37oZdLN28mw88Bk0AAAvASURBVK+c0193ChORZi3qk8XPAP3c/VTgJeCv9Q3k7tPdPc/d87Kyspq0wOM1fe4HdGmfyuWn9Yq6FBGRI4pnEBQCsVv4vcJuH3L3He5eHr68Dzg9jvU0mXVF+/jnqiL+Y2xfNSktIs1ePIPgXWCQmfU3s1TgCmB27ABmlh3zchKwKo71NJn7568nLbkN/zFOzUmISPMXt18NuXuVmU0DXgCSgL+4+wozuwNY4O6zgW+b2SSgCtgJXBuveppK8d5y/r6okM+c3otuHdKiLkdE5Kjiej8Cd58DzKnT7daY5z8CfhTPGpra/725gcpqNSchIi1H1CeLW5WDFdX87a2NXDS0BwOyOkRdjohIgygIGtETCwsoPVDJVDUnISItiIKgkVTXOPfNz2dU70zy1JyEiLQgCoJG8tLKbWzccYCp552EmS4gE5GWQ0HQSKbPXU/vLm25+JSeUZciInJMFASNYOHGnSzaVMp155yk5iREpMVREDSC6XPX06ltCp/NU3MSItLyKAhOUH7Jfl5cuZ3/GNuXdqlxvSxDRCQuFAQn6P7560lp04arz1JzEiLSMikITsCOfeU8vmAznxqdS/eM9KjLERE5LgqCE/DQW5sor6rhunPVnISItFwKguNUVlnN397cwAUnd2dQj4yoyxEROW4KguP05KJCduyv4Ppz1ZyEiLRsCoLjUFPj3DdvPSNyOzH2pC5RlyMickIUBMfh5feLWF+yn+vVnISItAIKguNw79z15Ga25ZLhak5CRFo+BcExWrxpF+9s2MmXz+lPcpJmn4i0fFqTHaP75uWTkZ7M58/oHXUpIiKNQkFwDDbtOMBzy7dy1Zl96ZCm5iREpHVQEByDv7yeT1Ib40tn94u6FBGRRqMgaKBd+yt49N0CJo/KpUdHNSchIq2HgqCBZry9kYOV1bqATERaHQVBA5RVVvPgGxsZPziLIT3VnISItC4KggZ4ekkhJfvKmXqe9gZEpPVREBxFTY1z77x8hmV35KwBXaMuR0Sk0SkIjuK1NUWsK9rHVDUnISKtlILgKKbPXU92p3QuPTU76lJEROJCQXAE720u5a31O/ny2f1JUXMSItJKae12BPfOyycjLZkrxqg5CRFpvRQEh1Gw8wBzlm3lyjP7kJGeEnU5IiJxoyA4jAde34AB157VL+pSRETiSkFQj90HKpn57iYuG5lDTmbbqMsREYkrBUE9Hn5nEwcqqrnu3P5RlyIiEncKgjoqqmp44PV8zhnYjVNyOkVdjohI3CkI6pi9dAtFe8u5Xs1JiEiCUBDEcHfunbueIT0yOG9Qt6jLERFpEgqCGHPXlrB6+16uV3MSIpJAFAQx7p27nh4d05g0MifqUkREmoyCILRiy27mryvh2rP6k5qs2SIiiSOuazwzm2Bmq81snZnddIThLjczN7O8eNZzJPfNy6d9ahJfOLNPVCWIiEQibkFgZknAPcBEYBhwpZkNq2e4DOA7wNvxquVotpQe5JmlW/j8GX3o1FbNSYhIYonnHsEYYJ27r3f3CmAmMLme4X4K3AmUxbGWI3rwjQ048KWz+0VVgohIZOIZBLlAQczrzWG3D5nZaUBvd//HkSZkZlPNbIGZLSguLm7UIveUVfLw25u4ZEQ2vbu0a9Rpi4i0BJGdFTWzNsDvgO8fbVh3n+7uee6el5WV1ah1PPpOAfvKq7hezUmISIKKZxAUArEN+fcKu9XKAIYDr5nZBmAsMLspTxhXVtfwl9fzGXtSF07tldlUbysi0qzEMwjeBQaZWX8zSwWuAGbX9nT33e7ezd37uXs/4C1gkrsviGNNh3j2vS1s3V3GVDUnISIJLG5B4O5VwDTgBWAV8Ji7rzCzO8xsUrzet6Hcnelz8xnYvQPnD+4edTkiIpFJjufE3X0OMKdOt1sPM+z58aylrtfX7WDV1j386vJTadNGzUmISOJK2Etop89bT7cOaUwereYkRCSxJWQQrNq6h7lrivnS2f1IS06KuhwRkUglZBDcNy+ftilJXKXmJEREEi8Itu0uY/bSQj5/Rm8y26VGXY6ISOQSLggefGMD1TXOl8/WBWQiIpBgQbCvvIoZb29k4vBs+nRVcxIiIpBgQfDouwXsLaviOjUnISLyoYQJgqrqGv4yP58x/bowuk/nqMsREWk2EiYI5izfRmHpQa5XcxIiIodImCBon5rEx4f14MKT1ZyEiEisuDYx0ZxcOLQHFw7tEXUZIiLNTsLsEYiISP0UBCIiCU5BICKS4BQEIiIJTkEgIpLgFAQiIglOQSAikuAUBCIiCc7cPeoajomZFQMbo67jBHUDSqIuohnR/Pg3zYtDaX4c6kTmR193z6qvR4sLgtbAzBa4e17UdTQXmh//pnlxKM2PQ8VrfujQkIhIglMQiIgkOAVBNKZHXUAzo/nxb5oXh9L8OFRc5ofOEYiIJDjtEYiIJDgFgYhIglMQxJmZ9TazV81spZmtMLPvhN27mNlLZrY2/JswN1I2syQzW2xmz4av+5vZ22a2zsweNbPUqGtsKmaWaWZPmNn7ZrbKzMYl6rJhZt8LvyPLzewRM0tPpGXDzP5iZkVmtjymW73LggXuDufLe2Z22om8t4Ig/qqA77v7MGAs8E0zGwbcBLzs7oOAl8PXieI7wKqY13cCv3f3gcAu4CuRVBWNu4Dn3f1kYCTBfEm4ZcPMcoFvA3nuPhxIAq4gsZaNB4EJdbodblmYCAwKH1OBP5/IGysI4szdt7r7ovD5XoIvei4wGfhrONhfgSnRVNi0zKwXcClwX/jagAuAJ8JBEmledALOA+4HcPcKdy8lQZcNglvntjWzZKAdsJUEWjbcfS6ws07nwy0Lk4G/eeAtINPMso/3vRUETcjM+gGjgbeBHu6+Ney1DUiUGyr/AbgRqAlfdwVK3b0qfL2ZICgTQX+gGHggPFR2n5m1JwGXDXcvBH4DbCIIgN3AQhJ32ah1uGUhFyiIGe6E5o2CoImYWQfg78B33X1PbD8PfsPb6n/Ha2afBIrcfWHUtTQTycBpwJ/dfTSwnzqHgRJo2ehMsJXbH8gB2vPRwyQJLZ7LgoKgCZhZCkEIzHD3J8PO22t35cK/RVHV14TOBiaZ2QZgJsFu/10Eu7XJ4TC9gMJoymtym4HN7v52+PoJgmBIxGXjIiDf3YvdvRJ4kmB5SdRlo9bhloVCoHfMcCc0bxQEcRYeA78fWOXuv4vpNRu4Jnx+DfB0U9fW1Nz9R+7ey937EZwIfMXdrwJeBT4TDpYQ8wLA3bcBBWY2JOx0IbCSBFw2CA4JjTWzduF3pnZeJOSyEeNwy8Js4Orw10Njgd0xh5COma4sjjMzOweYByzj38fFbyY4T/AY0IegWe3PuXvdE0WtlpmdD/zA3T9pZicR7CF0ARYDX3T38ijraypmNorgxHkqsB74EsEGWsItG2Z2O/B5gl/aLQauIzjunRDLhpk9ApxP0NT0duAnwCzqWRbCsPwTweGzA8CX3H3Bcb+3gkBEJLHp0JCISIJTEIiIJDgFgYhIglMQiIgkOAWBiEiCUxCIHCMzmxI2HHis400ys1bfgJy0PPr5qMgxMrMHgWfd/Yl6+iXHtI0j0iIoCERCYaOAzwHzgbMILtmf7O4HY4Y5C3iWoFG03cDlBFeOLwHOAR4B1gC3EFwktgO4yt23m9m1BM0sTwvDZA+QB/QEbqwvWESagg4NiRxqEHCPu58ClBKs6D/k7m8QXN5/g7uPcvcPwl6p7p7n7r8lCJKxYUNyMwlaW61PNkF4fBL4ZeN/FJGGST76ICIJJd/dl4TPFwL9GjjeozHPewGPho2EpQL5hxlnlrvXACvNrNU3NS3Nl/YIRA4V245NNQ3fWNof8/yPwJ/cfQTwVSC9Ae9lDa5QpJEpCESO3V4g4wj9O/HvJoGvOcJwIs2CgkDk2M0EbgjvKjagnv63AY+b2UKgpEkrEzkO+tWQiEiC0x6BiEiCUxCIiCQ4BYGISIJTEIiIJDgFgYhIglMQiIgkOAWBiEiC+/8rQUlPngCfUAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "accs = []\n",
        "f1s = []\n",
        "for n in np.arange(10,101,10):\n",
        "    args = create_args(n,True)\n",
        "    results, trainer = cross_validate(args)\n",
        "    accs.append(results[2][0])\n",
        "    f1s.append(results[0][0])\n",
        "    del trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7YWxcFgn97H",
        "outputId": "26468140-c0ee-4628-8d13-2b5f8b1f7138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 30\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 30\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 40\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 40\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 50\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 50\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 60\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 60\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n",
            "  Number of trainable parameters = 355372034\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 70\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 70\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 80\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 80\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 90\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 90\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ns,accs)\n",
        "plt.title('Accuracy for RoBERTa Large model')\n",
        "plt.xlabel('n train')\n",
        "plt.ylabel('acc')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vY-vCR5nCY-E",
        "outputId": "3f77d437-e2e0-45de-fd74-5375db1adfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ns,f1s)\n",
        "plt.title('F1-Score for RoBERTa Large model')\n",
        "plt.xlabel('n train')\n",
        "plt.ylabel('F1')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8-ZlLeuBCgo7",
        "outputId": "47f3a924-ac4b-474c-ebcf-7eef8a2b44a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models comparison - roberta-base"
      ],
      "metadata": {
        "id": "sZ0M0elROTwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# roberta-base for n=100\n",
        "%%capture\n",
        "args = create_args(100, False)\n",
        "results, trainer = cross_validate(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4XCXKq8OX-o",
        "outputId": "f98be925-1260-4b1d-89c3-93f163476fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 124654850\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 124654850\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('acc: ', results[2][0], 'std: ', results[2][1])\n",
        "print('f1: ', results[0][0], 'std: ', results[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssa7S3aUPkNM",
        "outputId": "186b34d6-bf96-4e4e-f1d4-38d3cac200fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  0.759 std:  0.009000000000000008\n",
            "f1:  0.7540508691424319 std:  0.010622330576302519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP_Projekt/IMDB Dataset.csv\").iloc[2000:11000,:]\n",
        "df = df.rename(columns={'review': 'sentence', 'sentiment': 'label'})\n",
        "df['label'][df.label == 'positive'] = 1\n",
        "df['label'][df.label == 'negative'] = 0"
      ],
      "metadata": {
        "id": "5ILl-qm3WAdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "test_dataset = CLDatasetClassification(np.arange(9000), df, tokenizer, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqmgd-QFWPnq",
        "outputId": "42cf224b-4176-4619-ccf7-3c9b35fc1952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = trainer.predict(test_dataset=test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "tka5RIz6WS9n",
        "outputId": "a973a649-8596-4ec5-93cb-a4fb7909a5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 9000\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for roberta-base\n",
        "pred.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cq9ljraWYKm",
        "outputId": "09cc53ef-179e-4d9a-f342-ce3ca00f5562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.6475101709365845,\n",
              " 'test_accuracy_score': 0.726,\n",
              " 'test_f1_score': 0.7154953383504932,\n",
              " 'test_recall_score': 0.5339719313878369,\n",
              " 'test_precision_score': 0.8650306748466258,\n",
              " 'test_runtime': 34.2608,\n",
              " 'test_samples_per_second': 262.691,\n",
              " 'test_steps_per_second': 4.115}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models comparison - roberta-large"
      ],
      "metadata": {
        "id": "vdpIPwgHWBcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# roberta-large for n=100\n",
        "%%capture\n",
        "args = create_args(100, True)\n",
        "results, trainer = cross_validate(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjvvmWQHRXfZ",
        "outputId": "4ded471e-d8f0-4c96-b039-0e4fd5a26518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'clf_loss.fc', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "  Number of trainable parameters = 355372034\n",
            "Saving model checkpoint to ./result/checkpoint-220\n",
            "Configuration saved in ./result/checkpoint-220/config.json\n",
            "Model weights saved in ./result/checkpoint-220/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('acc: ', results[2][0], 'std: ', results[2][1])\n",
        "print('f1: ', results[0][0], 'std: ', results[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsr1WynERa2Q",
        "outputId": "ae7af463-ccee-4853-96e0-d41d10bbc582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  0.8394999999999999 std:  0.0005000000000000004\n",
            "f1:  0.839403140997314 std:  0.0005814980195831265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP_Projekt/IMDB Dataset.csv\").iloc[2000:11000,:]\n",
        "df = df.rename(columns={'review': 'sentence', 'sentiment': 'label'})\n",
        "df['label'][df.label == 'positive'] = 1\n",
        "df['label'][df.label == 'negative'] = 0"
      ],
      "metadata": {
        "id": "9vqtsmtES3-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "test_dataset = CLDatasetClassification(np.arange(9000), df, tokenizer, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qahusmJjTIis",
        "outputId": "18daf0bd-089d-4088-8a0e-d85ecb40253b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = trainer.predict(test_dataset=test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "hu4Lemt7VOLh",
        "outputId": "92b47f6d-dc3a-406d-bacf-068041102b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 9000\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for roberta-large\n",
        "pred.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wso8BnttV5ru",
        "outputId": "7e544139-d63d-4238-f814-b142c4a23eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.45538005232810974,\n",
              " 'test_accuracy_score': 0.8375555555555556,\n",
              " 'test_f1_score': 0.837507604119494,\n",
              " 'test_recall_score': 0.8556471374470929,\n",
              " 'test_precision_score': 0.8251342642320086,\n",
              " 'test_runtime': 102.8674,\n",
              " 'test_samples_per_second': 87.491,\n",
              " 'test_steps_per_second': 1.371}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}