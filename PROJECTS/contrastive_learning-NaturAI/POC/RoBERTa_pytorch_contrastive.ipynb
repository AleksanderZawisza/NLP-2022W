{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGS6CohK3x-t"
      },
      "outputs": [],
      "source": [
        "# datasets==2.5.1\n",
        "# numpy==1.23.3\n",
        "# pandas==1.5.0\n",
        "# pytorch_metric_learning==1.6.2\n",
        "# scikit_learn==1.1.2\n",
        "# sentence_transformers==2.2.2\n",
        "# tensorflow==2.10.0\n",
        "# torch==1.12.1\n",
        "# transformers==4.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install pytorch_metric_learning -q\n",
        "!pip install sentence_transformers -q"
      ],
      "metadata": {
        "id": "b5uHPq77NjAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaModel, RobertaClassificationHead\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from datasets import load_dataset\n",
        "from transformers.file_utils import is_tf_available, is_torch_available\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import transformers\n",
        "from numpy import unique\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import KFold\n",
        "from pytorch_metric_learning import losses as losses_ml\n",
        "from sentence_transformers.losses import BatchAllTripletLoss"
      ],
      "metadata": {
        "id": "fcSr01jP4E9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ClrnUvM_QIbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bde61c0-47cd-41b8-b6f6-7e537d86e398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils - prepare dataset"
      ],
      "metadata": {
        "id": "LNu7GIscSvZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing datasets to use with roberta model"
      ],
      "metadata": {
        "id": "R4IoRrIln9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset=\"sst2\", seed=42):\n",
        "    if dataset == \"sst2\":\n",
        "        df = pd.read_csv('/content/drive/MyDrive/NLP_Projekt/data/SST-2/train.tsv', sep='\\t')\n",
        "    elif dataset == \"trec\":\n",
        "        dataset = load_dataset('trec')\n",
        "        df = pd.DataFrame(\n",
        "            list(zip([(eval['label-coarse']) for eval in dataset['train']],\n",
        "                     [(eval['text']) for eval in dataset['train']])),\n",
        "            columns=['label', 'sentence'])\n",
        "    elif dataset == \"mr\":\n",
        "        d = []\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/MR/rt-polarity.neg', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 0\n",
        "                    }\n",
        "                )\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/MR/rt-polarity.pos', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 1\n",
        "                    }\n",
        "                )\n",
        "        df = pd.DataFrame(d)\n",
        "    elif dataset == \"cr\":\n",
        "        d = []\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/CR/custrev.neg', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 0\n",
        "                    }\n",
        "                )\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/CR/custrev.pos', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 1\n",
        "                    }\n",
        "                )\n",
        "        df = pd.DataFrame(d)\n",
        "    elif dataset == \"mpqa\":\n",
        "        d = []\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/MPQA/mpqa.neg', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 0\n",
        "                    }\n",
        "                )\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/MPQA/mpqa.pos', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 1\n",
        "                    }\n",
        "                )\n",
        "        df = pd.DataFrame(d)\n",
        "    elif dataset == \"subj\":\n",
        "        d = []\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/SUBJ/subj.objective', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 0\n",
        "                    }\n",
        "                )\n",
        "        with open('/content/drive/MyDrive/NLP_Projekt/data/SUBJ/subj.subjective', \"r\") as f:\n",
        "            for elem in f.readlines():\n",
        "                d.append(\n",
        "                    {\n",
        "                        'sentence': elem,\n",
        "                        'label': 1\n",
        "                    }\n",
        "                )\n",
        "        df = pd.DataFrame(d)\n",
        "    elif dataset == \"mrpc\":\n",
        "        df = pd.read_csv('/content/drive/MyDrive/NLP_Projekt/data/MRPC/train.tsv', sep='\\t', error_bad_lines=False)\n",
        "        df = df.rename(columns={'Quality': 'label', '#1 String': 'question', '#2 String': 'sentence'})\n",
        "        df[\"question\"] = df[\"question\"].astype(str)\n",
        "        df['sentence'] = df[\"sentence\"].astype(str)\n",
        "    elif dataset == 'imdb':\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/NLP_Projekt/IMDB Dataset.csv\").iloc[:2000,:]\n",
        "        df = df.rename(columns={'review': 'sentence', 'sentiment': 'label'})\n",
        "        df['label'][df.label == 'positive'] = 1\n",
        "        df['label'][df.label == 'negative'] = 0\n",
        "    else:\n",
        "        raise ValueError(f'Cannot load the dataset: {dataset}.')\n",
        "    df = shuffle(df, random_state=seed)\n",
        "    return df"
      ],
      "metadata": {
        "id": "MwVOZwleSuyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils - training"
      ],
      "metadata": {
        "id": "aeoctPqUTVlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds)\n",
        "    recall = recall_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy_score': acc,\n",
        "        'f1_score': f1,\n",
        "        'recall_score': recall,\n",
        "        'precision_score': precision\n",
        "    }"
      ],
      "metadata": {
        "id": "h1vBD2GiS_ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils - triple entropy dataset"
      ],
      "metadata": {
        "id": "Frn6Sd4QTZ2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset preparation for use with pytorch"
      ],
      "metadata": {
        "id": "OUlWC3MpoECG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLDatasetClassification(torch.utils.data.Dataset):\n",
        "    def __init__(self, index, df, tokenizer, max_length, sample_size=-1):\n",
        "        if sample_size != -1:\n",
        "            index = np.random.choice(index, sample_size, replace=False)\n",
        "\n",
        "        texts = df.iloc[index][\"sentence\"].tolist()\n",
        "        labels = df.iloc[index][\"label\"].tolist()\n",
        "        train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "        self.encodings = train_encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "class CLDatasetNLI(torch.utils.data.Dataset):\n",
        "    def __init__(self, index, df, tokenizer, max_length, sample_size=-1):\n",
        "        if sample_size != -1:\n",
        "            index = np.random.choice(index, sample_size, replace=False)\n",
        "\n",
        "        labels = df.iloc[index][\"label\"].tolist()\n",
        "        questions = df.iloc[index][\"question\"].tolist()\n",
        "        sentences = df.iloc[index][\"sentence\"].tolist()\n",
        "        texts = list(zip(questions, sentences))\n",
        "        texts = [txt for txt in texts if isinstance(txt[0], str) and isinstance(txt[1], str)]\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "TJVa59ODTNQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roberta Contrastive"
      ],
      "metadata": {
        "id": "S6eMHYiVTOb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# roberta model for contrastive learning task"
      ],
      "metadata": {
        "id": "fhdpApqQoKJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaContrastiveLearning(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config, **kwargs):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.class_weight = kwargs.get('class_weights', None)\n",
        "        self.clf_loss = kwargs.get('clf_loss', None)\n",
        "        self.beta = kwargs.get('beta', None)\n",
        "        self.only_cls = kwargs.get('only_cls', None)\n",
        "        self.extended_inference = kwargs.get('extended_inference', None)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            position_ids=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=None,\n",
        "            labels=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "        if self.beta is not None:\n",
        "            epsilon = self.beta\n",
        "        else:\n",
        "            epsilon = 1\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                if self.config is not None:\n",
        "                    loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "                flatten_labels = torch.ones(sequence_output.shape[0] * sequence_output.shape[1]).type(\n",
        "                    torch.LongTensor).to(loss.device)\n",
        "\n",
        "                for label in labels.view(-1):\n",
        "                    for idx, _ in enumerate(range(sequence_output.shape[1])):\n",
        "                        flatten_labels[idx] = label\n",
        "\n",
        "                sequence_selected = sequence_output.view(-1, sequence_output.shape[2])\n",
        "                if self.clf_loss is not None:\n",
        "                    if self.only_cls:\n",
        "                        cl_loss = self.clf_loss(sequence_output[:, 0, :], labels.view(-1))\n",
        "                    else:\n",
        "                        cl_loss = self.clf_loss(sequence_selected, flatten_labels)\n",
        "\n",
        "                    loss = epsilon * loss + (1 - epsilon) * cl_loss\n",
        "\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if self.extended_inference is not None and self.clf_loss is not None:\n",
        "            sim_to_classes = self.clf_loss.get_logits(sequence_output[:, 0, :])\n",
        "            softmax = torch.nn.Softmax(dim=1)\n",
        "            logits = epsilon * softmax(logits) + (1 - epsilon) * softmax(sim_to_classes)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "vPHGZh104Lvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validate"
      ],
      "metadata": {
        "id": "MfHuy-ypOydq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)"
      ],
      "metadata": {
        "id": "ia3HAWwbTKAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create function arguments namespace"
      ],
      "metadata": {
        "id": "b_nkZv8Pnqhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    max_length = 64,\n",
        "    learning_rate=1e-5,\n",
        "    num_warmup_steps=10,\n",
        "    eps=1e-08,\n",
        "    model_name=\"roberta-large\",\n",
        "    model_type=\"softriple\",\n",
        "    weight_decay=0.01,\n",
        "    la=8,\n",
        "    supcon_temp=0.1,\n",
        "    gamma=0.1,\n",
        "    margin=0.1,\n",
        "    centers=5,\n",
        "    beta=0.4,\n",
        "    seed=2048,\n",
        "    output_dir=\"/content/drive/MyDrive/NLP_Projekt/result\",\n",
        "    save_steps=250,\n",
        "    epochs=120,\n",
        "    num_training_steps=120,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    sample_size=128,\n",
        "    n_split=3,\n",
        "    dataset_name=\"imdb\",\n",
        "    softmax_scale=1,\n",
        "    alpha=0.1,\n",
        "    extended_inference=0.1\n",
        "    )"
      ],
      "metadata": {
        "id": "NDwVFGQPPppU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate(args):\n",
        "    seed = args.seed\n",
        "    max_length = args.max_length\n",
        "    learning_rate = args.learning_rate\n",
        "    num_warmup_steps = args.num_warmup_steps\n",
        "    num_training_steps = args.num_training_steps\n",
        "    eps = args.eps\n",
        "    model_name = args.model_name\n",
        "    model_type = args.model_type\n",
        "    weight_decay = args.weight_decay\n",
        "    la = args.la\n",
        "    gamma = args.gamma\n",
        "    margin = args.margin\n",
        "    centers = args.centers\n",
        "    beta = args.beta\n",
        "    save_steps = args.save_steps\n",
        "    sample_size = args.sample_size\n",
        "    n_split = args.n_split\n",
        "    supcon_temp = args.supcon_temp\n",
        "    np.random.seed(seed)\n",
        "    dataset_name = args.dataset_name\n",
        "    softmax_scale = args.softmax_scale\n",
        "    alpha = args.alpha\n",
        "    extended_inference = args.extended_inference\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "        save_steps=save_steps\n",
        "    )\n",
        "\n",
        "    kf = KFold(n_splits=n_split, random_state=seed, shuffle=True)\n",
        "    cross_val_res = {}\n",
        "    df = prepare_dataset(dataset_name)\n",
        "\n",
        "    if model_name == \"roberta-base\":\n",
        "        embedding_size = 768\n",
        "    else:\n",
        "        embedding_size = 1024\n",
        "\n",
        "    for fold_id, (train_index, valid_index) in enumerate(kf.split(df)):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if dataset_name == \"mrpc\":\n",
        "            train_dataset = CLDatasetNLI(train_index, df, tokenizer, max_length, sample_size)\n",
        "            valid_dataset = CLDatasetNLI(valid_index, df, tokenizer, max_length)\n",
        "        else:\n",
        "            train_dataset = CLDatasetClassification(train_index, df, tokenizer, max_length, sample_size)\n",
        "            valid_dataset = CLDatasetClassification(valid_index, df, tokenizer, max_length)\n",
        "        if model_type == \"softriple\":\n",
        "            clf_loss = losses_ml.SoftTripleLoss(num_classes=len(unique(df.label)), embedding_size=embedding_size,\n",
        "                                             centers_per_class=centers, la=la, gamma=gamma, margin=margin)\n",
        "        elif model_type == \"supcon\":\n",
        "            clf_loss = losses_ml.SupConLoss(temperature=supcon_temp)\n",
        "        elif model_type == \"proxynca\":\n",
        "            clf_loss = losses_ml.ProxyNCALoss(len(unique(df.label)), embedding_size=embedding_size,\n",
        "                                           softmax_scale=softmax_scale)\n",
        "        elif model_type == \"proxyanchor\":\n",
        "            clf_loss = losses_ml.ProxyAnchorLoss(len(unique(df.label)), embedding_size=embedding_size, margin=margin,\n",
        "                                              alpha=alpha)\n",
        "        elif model_type == \"npairs\":\n",
        "            clf_loss = losses_ml.NPairsLoss()\n",
        "        elif model_type == \"triplet\":\n",
        "            clf_loss = BatchAllTripletLoss()\n",
        "        elif model_type == \"baseline\":\n",
        "            clf_loss = None\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'The model_type: {model_type} is not supported. Choose one of following: triple_entropy, supcon, baseline.')\n",
        "        model = RobertaContrastiveLearning.from_pretrained(model_name,\n",
        "                                                           num_labels=len(\n",
        "                                                               unique(\n",
        "                                                                   df.label)),\n",
        "                                                           clf_loss=clf_loss,\n",
        "                                                           beta=beta,\n",
        "                                                           extended_inference=extended_inference)\n",
        "        param_groups = [{\"params\": model.parameters(),\n",
        "                         'lr': float(learning_rate)}]\n",
        "\n",
        "        optimizer = transformers.AdamW(param_groups, eps=eps, weight_decay=weight_decay, correct_bias=True)\n",
        "        scheduler = transformers.get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
        "                                                                 num_warmup_steps=num_warmup_steps,\n",
        "                                                                 num_training_steps=num_training_steps)\n",
        "        optimizers = optimizer, scheduler\n",
        "        set_seed(seed)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=valid_dataset,\n",
        "            optimizers=optimizers,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        cross_val_res[fold_id] = trainer.evaluate()\n",
        "        trainer.save_model(args.output_dir + f'{fold_id}_{model_name}_{model_type}_{dataset_name}/model')\n",
        "\n",
        "    print(f\"Model type: {model_type}, Dataset name: {dataset_name}\")\n",
        "    for measure in [\"eval_f1_score\", \"eval_recall_score\", \"eval_accuracy_score\", \"eval_precision_score\"]:\n",
        "        print(\n",
        "            f'measure: {measure.split(\"_\")[1]}, mean: {np.mean([el[measure] for el in cross_val_res.values()])}, std: {np.std([el[measure] for el in cross_val_res.values()])}')\n",
        "    return model"
      ],
      "metadata": {
        "id": "iBb35X_94MMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training cross validation"
      ],
      "metadata": {
        "id": "H1LY0UKWny9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cross_validate(args)"
      ],
      "metadata": {
        "id": "i4ZJ0n_M4MaM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a10d90e-885f-4f7c-cf49-954d08d8d5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'clf_loss.fc', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 128\n",
            "  Num Epochs = 120\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 240\n",
            "  Number of trainable parameters = 355372034\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 08:37, Epoch 120/120]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 667\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/NLP_Projekt/result0_roberta-large_softriple_imdb/model\n",
            "Configuration saved in /content/drive/MyDrive/NLP_Projekt/result0_roberta-large_softriple_imdb/model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP_Projekt/result0_roberta-large_softriple_imdb/model/pytorch_model.bin\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'clf_loss.fc', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 128\n",
            "  Num Epochs = 120\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 240\n",
            "  Number of trainable parameters = 355372034\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 08:38, Epoch 120/120]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 667\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/NLP_Projekt/result1_roberta-large_softriple_imdb/model\n",
            "Configuration saved in /content/drive/MyDrive/NLP_Projekt/result1_roberta-large_softriple_imdb/model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP_Projekt/result1_roberta-large_softriple_imdb/model/pytorch_model.bin\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaContrastiveLearning: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaContrastiveLearning from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaContrastiveLearning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'clf_loss.fc', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 128\n",
            "  Num Epochs = 120\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 240\n",
            "  Number of trainable parameters = 355372034\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 08:38, Epoch 120/120]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 666\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11/11 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/NLP_Projekt/result2_roberta-large_softriple_imdb/model\n",
            "Configuration saved in /content/drive/MyDrive/NLP_Projekt/result2_roberta-large_softriple_imdb/model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/NLP_Projekt/result2_roberta-large_softriple_imdb/model/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: softriple, Dataset name: imdb\n",
            "measure: f1, mean: 0.8226472145830771, std: 0.018078740190466666\n",
            "measure: recall, mean: 0.8832409088678412, std: 0.01912489706334964\n",
            "measure: accuracy, mean: 0.8234884059471765, std: 0.01750161849234549\n",
            "measure: precision, mean: 0.7909390234788839, std: 0.03393690669331701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model type: softriple, Dataset name: imdb\n",
        "# measure: f1, mean: 0.8226472145830771, std: 0.018078740190466666\n",
        "# measure: recall, mean: 0.8832409088678412, std: 0.01912489706334964\n",
        "# measure: accuracy, mean: 0.8234884059471765, std: 0.01750161849234549\n",
        "# measure: precision, mean: 0.7909390234788839, std: 0.03393690669331701"
      ],
      "metadata": {
        "id": "qpPsIaH4n3qa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}