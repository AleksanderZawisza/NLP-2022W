{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import math\n",
    "import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeding(word,emb):\n",
    "    \"\"\"Get word embedding \"\"\"\n",
    "    return emb.loc[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags_list_dict(row, with_sorting = False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : list\n",
    "        list of lists keyword, concept\n",
    "    with_sorting : bool\n",
    "        should sorting be enables\n",
    "        \n",
    "    Returns\n",
    "    ------\n",
    "    results : dict\n",
    "        initaly sorted (or random ordered) dictionary keyword : dict of concepts {concept : distance}\n",
    "    '''\n",
    "    result = {}\n",
    "    for tag in row:\n",
    "        key = tag[0].split(',')[0].upper()\n",
    "        value = tag[1].upper()\n",
    "        if key in result.keys():\n",
    "            if  value not in result[key]:\n",
    "                result[key][value]=1\n",
    "            else:\n",
    "                 result[key][value]+=1\n",
    "        else:\n",
    "            result[key]= {value:1}\n",
    "    if with_sorting:\n",
    "        for key in result.keys():\n",
    "            d = result[key]\n",
    "            d = {k:0 for k,v in dict(sorted(d.items(), key=lambda item: item[1], reverse = True)).items()}\n",
    "            result[key] = d\n",
    "    else:\n",
    "        for key in result.keys():\n",
    "            d = result[key]\n",
    "            d = {k:0 for k,v in d.items()}\n",
    "            result[key] = d\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguation(current_selection,embedings, weigths):\n",
    "    ''' \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_selection : dict\n",
    "         dictionary keyword: list of all unique concepts\n",
    "    weigths: dict\n",
    "         the importance of given keyword\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    new_current_selction : dict\n",
    "        dictionary with new best selction of concepts\n",
    "    \n",
    "    '''\n",
    "    # we iterate over the current_selection MAX_ITER times\n",
    "    new_current_selction  = copy.deepcopy(current_selection)\n",
    "    should_stop = False\n",
    "    for i in range(7):\n",
    "\n",
    "        for keyword, concepts_list in new_current_selction.items():\n",
    "            distances = {} # for each possible concept calaculate the mean distance from other kewords (concepts of them)\n",
    "            for concept in concepts_list.keys():\n",
    "                distances[concept] = []\n",
    "                for k, current_best_tags in new_current_selction.items():\n",
    "                    # foreach keyword that is not a current one \n",
    "                    if k!=keyword:\n",
    "                        current_best_tag = list(current_best_tags.keys())[0] # the first out of list of concepts\n",
    "                        try:\n",
    "                            distances[concept].append(weigths[k]*math.dist(get_embeding(concept,embedings),get_embeding(current_best_tag,embedings))) # append distance from this concept\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                distances[concept] = np.mean(distances[concept]) # mean distance \n",
    "            new_current_selction[keyword] = dict(sorted(distances.items(), key=lambda item: item[1]))  # upadate the current selection of this keyword\n",
    "    return new_current_selction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_importance(grouped_data, tagger_data):\n",
    "    \"\"\"get keywords importance\"\"\"\n",
    "    return grouped_data.reset_index().merge(tagger_data[['PMID','topic_keywords']] ,on = 'PMID').set_index('text_to_annotate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_best_tags(data, n = 1):\n",
    "    \"\"\"how many best concepts to take\"\"\"\n",
    "    return [{k:sorted(v, key=v.get)[:n] for k,v in dd.items()} for dd in data['after_disambiguation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_disambiguation(data, tagger, embedings,column_name = 'ncbo_annotations_pairs' ,  weighting = False, sorting = False,  take_best = 1):\n",
    "    \"\"\"prepare data for disambiguation - read csv, eval, set index etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datas : DataFrames\n",
    "        DataFrame with keywords\n",
    "    tagger : DataFrames\n",
    "        DataFrame with keywords importance\n",
    "    embedings : DataFrames\n",
    "        DataFrame with keywords embedings\n",
    "    column_name : str\n",
    "        Name of the column with annotations\n",
    "    weigthing: bool\n",
    "        Should the weigthed voting be performed\n",
    "    sorting: bool\n",
    "        Should the initial sorting be performed\n",
    "    take_best : int\n",
    "        How many best concepts should be returned\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    data  : DataFrame\n",
    "    \"\"\"\n",
    "    grouped = data.groupby('text_to_annotate').nth(0)\n",
    "    # get importance for each keyword -> will be used if weighting True\n",
    "    grouped = keywords_importance(grouped, tagger )\n",
    "    grouped['possible_tags'] = grouped[column_name].apply(lambda r: create_tags_list_dict(r, sorting))\n",
    "\n",
    "    # disambiguation\n",
    "    res = []\n",
    "    for idx, row  in tqdm.tqdm(grouped.iterrows(), total = len(grouped)):\n",
    "        current_selection = row['possible_tags']\n",
    "        if not weighting:\n",
    "            weigths = dict(zip(list(row['topic_keywords'].keys()),[1] * len(row['topic_keywords'])))\n",
    "        else:\n",
    "            weigths = row['topic_keywords']\n",
    "        r = disambiguation(current_selection, embedings,weigths,forced)\n",
    "        res.append(r)\n",
    "    grouped['after_disambiguation'] = res\n",
    "    data = data.merge(grouped['after_disambiguation'].reset_index(), on = 'text_to_annotate' )\n",
    "    data['disambiguation_best_concept'] = get_n_best_tags(data, take_best)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_name, tagger_name, embedings_name):\n",
    "    \"\"\"prepare data for disambiguation - read csv, eval, set index etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_name : str\n",
    "        Path to get data\n",
    "    tagger_name : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    embedings_name : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    data, tagger, embedings : tuple (DataFrame, DataFrame, DataFrame) \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_name)\n",
    "    data['ncbo_annotations_pairs'] = data['ncbo_annotations_pairs'].apply(eval)\n",
    "    data['ncbo_annotations_pairs']  = data['ncbo_annotations_pairs'].apply(lambda x : [[a[0].upper(),a[1]] for a in x])\n",
    "\n",
    "    tagger = pd.read_csv(tagger_name)\n",
    "    tagger['topic_keywords'] = tagger['topic_keywords'].apply(eval).apply(lambda x: {k.upper():v for k,v in dict(x).items()})\n",
    "\n",
    "\n",
    "    embedings = pd.read_csv(embedings_name)\n",
    "    embedings = embedings.set_index('words')\n",
    "    embedings.index = embedings.index.str.upper()\n",
    "    embedings = embedings[~embedings.index.duplicated(keep='first')]\n",
    "\n",
    "    return  data, tagger, embedings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_disambiguation(results_folder,data_path, tagger_path, embedings_path,timestamp,weigthing=False,sorting=False):\n",
    "    \"\"\"Performs disambiguation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_folder : str\n",
    "        Path to save results\n",
    "    \n",
    "    data_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    tagger_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    embedings_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        Timestamp that will be added to filenames\n",
    "\n",
    "    weigthing: bool\n",
    "        Should the weigthed voting be performed\n",
    "\n",
    "    sorting: bool\n",
    "        Should the initial sorting be performed\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    result_path : str\n",
    "        Path to results\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import pandas as pd\n",
    "    import os \n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    data, tagger, embedings = prepare_data( data_path,tagger_path,embedings_path)\n",
    "    data = prepare_for_disambiguation(data,tagger,embedings,'ncbo_annotations_pairs',weigthing,sorting)\n",
    "    data.to_csv(results_folder)\n",
    "\n",
    "    return results_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lda_results(data_path, num_topics = 10,num_keywords = 10):\n",
    "    from gensim import corpora, models\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \"\"\"Performs lda keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    timestamp : str\n",
    "        Timestamp of getting data\n",
    "    num_topic : int\n",
    "        Number of disired topics\n",
    "\n",
    "    num_keywords : int\n",
    "        Number of keywords per topic\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    result, topic_distribution,lda_model : (DataFrame,DataFrame,model)\n",
    "        DataFrame with reults, DataFrae with topic distribution and lda_model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def get_topic_distribution(lda_model, number_of_topics, number_of_keywords):\n",
    "        topics_distrib = {}\n",
    "        for t in lda_model.print_topics(number_of_topics,number_of_keywords):\n",
    "            topics_distrib[t[0]] =[(a.split('*')[1][1:-1],float(a.split(\"*\")[0])) for a in t[1].split(' + ')]\n",
    "        return topics_distrib\n",
    "\n",
    "\n",
    "    data = pd.read_csv(data_path)\n",
    "    columns = ['tokenized_sentences', 'tokenized_words_lemmatize']\n",
    "    for col in columns:\n",
    "        data[col] = data[col].apply(eval)\n",
    "\n",
    "    texts = data.groupby('PMID')['tokenized_words_lemmatize'].agg(lambda x: x.iloc[0]+x.iloc[1])\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    \n",
    "    lda_model = models.LdaMulticore(corpus=corpus,\n",
    "                                        id2word=dictionary,\n",
    "                                        num_topics=num_topics,\n",
    "                                        passes = 20)\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "    topic_distribution = get_topic_distribution(lda_model,num_topics,num_keywords)\n",
    "    topics_results = pd.DataFrame.from_records([topic_distribution]).T.reset_index().rename(columns = {'index':'topic_number',0:'topic_keywords'})\n",
    "    topics_results['keywords'] = topics_results['topic_keywords'].apply(lambda x: [a[0] for a in x])\n",
    "\n",
    "    \n",
    "\n",
    "    docs= []\n",
    "    for doc in doc_lda:\n",
    "        docs.append({\n",
    "            'topic_number':doc[0][0],\n",
    "            'topic_probs': float(doc[0][1]),\n",
    "            'topic_keywords': topics_results.iloc[doc[0][0]]['topic_keywords'],\n",
    "            'keywords': topics_results.iloc[doc[0][0]]['keywords']\n",
    "\n",
    "        })\n",
    "\n",
    "    docs = pd.DataFrame.from_records(docs)\n",
    "\n",
    "    results = data[['PMID']].drop_duplicates().reset_index(drop=True).join(docs)\n",
    "    topics_results = pd.DataFrame.from_records([topic_distribution]).T.reset_index().rename(columns = {'index':'topic_number',0:'topic_keywords'})\n",
    "\n",
    "\n",
    "\n",
    "    return results,topics_results, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_lda(data_path, models_path, results_path, timestamp, num_topics = 10,num_keywords = 10):\n",
    "    \"\"\"Performs lda keywords extraction for data after lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to preprocessed dataset. Dataset must contain a column with name 'tokenized_words_lemmatize'.\n",
    "    \n",
    "    models_path : str\n",
    "        Path to save the model to (folder must exist).\n",
    "\n",
    "    results_path : str\n",
    "        Path to save the results to (folder must exist).\n",
    "\n",
    "    timestamp : str\n",
    "        Timestamp that will be added to filenames\n",
    "\n",
    "    num_topic : int\n",
    "        Number of disired topics\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (result_path, model_save_name) : tuple\n",
    "        Frist element is the path to created file with extracted keywrods, second - path to created model.\n",
    "    \"\"\"\n",
    "\n",
    "    results,topics_results, lda_model =  get_lda_results(data_path, num_topics ,num_keywords)\n",
    "\n",
    "    results_path = os.path.join(os.path.join(results_path, f'lda_results_{timestamp}.csv'))\n",
    "    results.to_csv(results_path)\n",
    "\n",
    "    models_path = os.path.join(models_path,f\"lda_model_{timestamp}\")\n",
    "    lda_model.save(models_path)\n",
    "\n",
    "    return results_path,models_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 7\n",
    "data_name = '../0.RESULTS/lda_ncbo/lda_ncbo_2023-01-17_23-48-04.csv'\n",
    "tagger_name = '../0.RESULTS/lda/lda_results_2023-01-17_22-31-53.csv'\n",
    "embeding_name = '../0.RESULTS/embeddings/ncbo_embeddings.csv'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "weigthing = True\n",
    "sorting = True\n",
    "forced = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_extraction_method = 'lda'\n",
    "tagger = 'ncbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:06<00:00,  1.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../0.RESULTS/disambiguation/lda_ncbo_2023_01_18_22_54_12_no_sorting_no_weighting_no_forcing.csv'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/{keyword_extraction_method}_{tagger}_{timestamp}_no_sorting_no_weighting.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=False,forced = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:06<00:00,  1.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../0.RESULTS/disambiguation/lda_ncbo_2023_01_18_22_54_12_sorting_no_weighting_no_forcing.csv'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/{keyword_extraction_method}_{tagger}_{timestamp}_sorting_no_weighting.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=True,forced = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../0.RESULTS/disambiguation/lda_ncbo_2023_01_18_22_54_12_sorting_weighting_no_forcing.csv'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_folder = f'../0.RESULTS/disambiguation/{keyword_extraction_method}_{tagger}_{timestamp}_sorting_weighting.csv'\n",
    "prepare_disambiguation(results_folder,data_name, tagger_name, embeding_name,timestamp,weigthing=False,sorting=True,forced = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
