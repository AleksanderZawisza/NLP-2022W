{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d86d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from ast import literal_eval\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, CuDNNLSTM, LSTM\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fda2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/FakeNews_Preprocessed/data.csv\", index_col=0)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d555a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['house', 'dem', 'aide', 'comey', 'letter', 'j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['feeling', 'life', 'circle', 'roundabout', 'h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['truth', 'fire', 'october', '29', '2016', 'te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['video', '15', 'civilian', 'kill', 'single', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['print', 'iranian', 'woman', 'sentence', 'yea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>['rapper', 'unload', 'black', 'celebrity', 'me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>['green', 'bay', 'packer', 'lose', 'washington...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>['macy', 'today', 'grow', 'union', 'great', 'a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>['nato', 'russia', 'hold', 'parallel', 'exerci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>['david', 'swanson', 'author', 'activist', 'jo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18285 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      ['house', 'dem', 'aide', 'comey', 'letter', 'j...      1\n",
       "1      ['feeling', 'life', 'circle', 'roundabout', 'h...      0\n",
       "2      ['truth', 'fire', 'october', '29', '2016', 'te...      1\n",
       "3      ['video', '15', 'civilian', 'kill', 'single', ...      1\n",
       "4      ['print', 'iranian', 'woman', 'sentence', 'yea...      1\n",
       "...                                                  ...    ...\n",
       "18280  ['rapper', 'unload', 'black', 'celebrity', 'me...      0\n",
       "18281  ['green', 'bay', 'packer', 'lose', 'washington...      0\n",
       "18282  ['macy', 'today', 'grow', 'union', 'great', 'a...      0\n",
       "18283  ['nato', 'russia', 'hold', 'parallel', 'exerci...      1\n",
       "18284  ['david', 'swanson', 'author', 'activist', 'jo...      1\n",
       "\n",
       "[18285 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a761074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [house, dem, aide, comey, letter, jason, chaff...\n",
       "1        [feeling, life, circle, roundabout, head, stra...\n",
       "2        [truth, fire, october, 29, 2016, tension, inte...\n",
       "3        [video, 15, civilian, kill, single, airstrike,...\n",
       "4        [print, iranian, woman, sentence, year, prison...\n",
       "                               ...                        \n",
       "18280    [rapper, unload, black, celebrity, meet, donal...\n",
       "18281    [green, bay, packer, lose, washington, redskin...\n",
       "18282    [macy, today, grow, union, great, american, re...\n",
       "18283    [nato, russia, hold, parallel, exercise, balka...\n",
       "18284    [david, swanson, author, activist, journalist,...\n",
       "Name: text, Length: 18285, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = dataset['text'].apply(literal_eval)\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c27e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(x) for x in articles])\n",
    "dataset = dataset[lengths < 1000]\n",
    "articles = articles[lengths < 1000]\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "articles = articles.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a9aa39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_length = max(articles.apply(len))\n",
    "article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce58444",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 100\n",
    "\n",
    "word_model = gensim.models.Word2Vec(articles, vector_size = vec_size, window = 5, workers = 12)\n",
    "word_model.train(articles, epochs = 10, total_examples = len(articles))\n",
    "wv = word_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471dcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(articles)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "encoded_articles = tokenizer.texts_to_sequences(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021b92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_articles = pad_sequences(encoded_articles, maxlen = article_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1fe78c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17475, 999)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56192310",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros(shape=(vocabulary_size, vec_size))\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    ind = wv.has_index_for(w)\n",
    "    if ind:\n",
    "        emb_matrix[i] = wv.get_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce8e5fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13106, 999)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef97466",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_articles, dataset['label'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5b480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocabulary_size, \n",
    "                    output_dim = vec_size,\n",
    "                    input_length = article_length,\n",
    "                    embeddings_initializer = Constant(emb_matrix))\n",
    "         )\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff90ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 999, 100)          13487300  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 99900)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 99901     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,587,201\n",
      "Trainable params: 13,587,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebccd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = RMSprop(learning_rate = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5964325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61bc96b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "205/205 [==============================] - 2s 6ms/step - loss: 0.6315 - accuracy: 0.6477 - val_loss: 0.5492 - val_accuracy: 0.7226\n",
      "Epoch 2/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.4489 - accuracy: 0.7921 - val_loss: 0.4516 - val_accuracy: 0.7922\n",
      "Epoch 3/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.3624 - accuracy: 0.8514 - val_loss: 0.3970 - val_accuracy: 0.8270\n",
      "Epoch 4/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.3074 - accuracy: 0.8885 - val_loss: 0.3615 - val_accuracy: 0.8478\n",
      "Epoch 5/20\n",
      "205/205 [==============================] - 1s 7ms/step - loss: 0.2669 - accuracy: 0.9114 - val_loss: 0.3353 - val_accuracy: 0.8624\n",
      "Epoch 6/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.2351 - accuracy: 0.9282 - val_loss: 0.3140 - val_accuracy: 0.8734\n",
      "Epoch 7/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.2092 - accuracy: 0.9391 - val_loss: 0.2974 - val_accuracy: 0.8833\n",
      "Epoch 8/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1878 - accuracy: 0.9478 - val_loss: 0.2846 - val_accuracy: 0.8901\n",
      "Epoch 9/20\n",
      "205/205 [==============================] - 1s 7ms/step - loss: 0.1697 - accuracy: 0.9554 - val_loss: 0.2736 - val_accuracy: 0.8949\n",
      "Epoch 10/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1543 - accuracy: 0.9622 - val_loss: 0.2633 - val_accuracy: 0.8995\n",
      "Epoch 11/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1410 - accuracy: 0.9680 - val_loss: 0.2556 - val_accuracy: 0.9011\n",
      "Epoch 12/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1293 - accuracy: 0.9718 - val_loss: 0.2499 - val_accuracy: 0.9025\n",
      "Epoch 13/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1192 - accuracy: 0.9747 - val_loss: 0.2437 - val_accuracy: 0.9055\n",
      "Epoch 14/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1101 - accuracy: 0.9782 - val_loss: 0.2383 - val_accuracy: 0.9062\n",
      "Epoch 15/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.1020 - accuracy: 0.9796 - val_loss: 0.2342 - val_accuracy: 0.9071\n",
      "Epoch 16/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.0947 - accuracy: 0.9815 - val_loss: 0.2306 - val_accuracy: 0.9094\n",
      "Epoch 17/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.0881 - accuracy: 0.9838 - val_loss: 0.2269 - val_accuracy: 0.9112\n",
      "Epoch 18/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.0822 - accuracy: 0.9854 - val_loss: 0.2246 - val_accuracy: 0.9139\n",
      "Epoch 19/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.0767 - accuracy: 0.9863 - val_loss: 0.2236 - val_accuracy: 0.9142\n",
      "Epoch 20/20\n",
      "205/205 [==============================] - 1s 6ms/step - loss: 0.0719 - accuracy: 0.9878 - val_loss: 0.2197 - val_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1538f6ec100>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a57097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
