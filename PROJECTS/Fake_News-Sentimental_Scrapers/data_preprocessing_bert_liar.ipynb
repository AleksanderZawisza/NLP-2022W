{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"LIAR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f'./data/{CUR_DATASET}/train.tsv', sep='\\t', header = None)\n",
    "valid_dataset = pd.read_csv(f'./data/{CUR_DATASET}/valid.tsv', sep='\\t', header = None)\n",
    "test_dataset = pd.read_csv(f'./data/{CUR_DATASET}/test.tsv', sep='\\t', header = None)\n",
    "liar_dataset = pd.concat([train_dataset, valid_dataset, test_dataset], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb85e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1                                                  2   \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                                   3               4                     5   \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "         6           7     8     9      10     11    12                   13  \n",
       "0     Texas  republican   0.0   1.0    0.0    0.0   0.0             a mailer  \n",
       "1  Virginia    democrat   0.0   0.0    1.0    1.0   0.0      a floor speech.  \n",
       "2  Illinois    democrat  70.0  71.0  160.0  163.0   9.0               Denver  \n",
       "3       NaN        none   7.0  19.0    3.0    5.0  44.0       a news release  \n",
       "4   Florida    democrat  15.0   9.0   20.0   19.0   2.0  an interview on CNN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                         statements\n",
       "0        false  Says the Annies List political group supports ...\n",
       "1    half-true  When did the decline of coal start? It started...\n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        false  Health care reform legislation is likely to ma...\n",
       "4    half-true  The economic turnaround started at the end of ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset = liar_dataset.iloc[:, [1, 2]]\n",
    "liar_dataset = liar_dataset.rename(columns = {1: 'label', 2: 'statements'})\n",
    "liar_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9953dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset['label'] = liar_dataset['label'].replace({\n",
    "    'false' : 0,\n",
    "    'barely-true' : 0,\n",
    "    'pants-fire' : 0,\n",
    "    'half-true' : 1,\n",
    "    'mostly-true' : 1,\n",
    "    'true' : 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = liar_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label         0\n",
       "statements    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee421",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe069c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4346120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddeb4e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', '', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When did the decline of coal start It started ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton agrees with John McCain by vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Jim Dunnam has not lived in the district he re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Im the only person on this stage who has worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>However it took million in Oregon Lottery fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         statements\n",
       "0      0  Says the Annies List political group supports ...\n",
       "1      1  When did the decline of coal start It started ...\n",
       "2      1  Hillary Clinton agrees with John McCain by vot...\n",
       "3      0  Health care reform legislation is likely to ma...\n",
       "4      1  The economic turnaround started at the end of ...\n",
       "5      1  The Chicago Bears have had more starting quart...\n",
       "6      0  Jim Dunnam has not lived in the district he re...\n",
       "7      1  Im the only person on this stage who has worke...\n",
       "8      1  However it took million in Oregon Lottery fund...\n",
       "9      1  Says GOP primary opponents Glenn Grothman and ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_dataset[\"statements\"] = liar_dataset[\"statements\"].apply(preprocess_text)\n",
    "liar_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25112b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    liar_dataset[\"statements\"] = liar_dataset[\"statements\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    liar_dataset[\"statements\"] = liar_dataset[\"statements\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6292eec",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e159b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    liar_dataset[\"statements\"] = liar_dataset[\"statements\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    liar_dataset[\"statements\"] = liar_dataset[\"statements\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = liar_dataset[\"statements\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp-transformers/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7676622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  say annies list political group support thirdtrimester abortion demand\n",
      "Tokenized:  ['say', 'annie', '##s', 'list', 'political', 'group', 'support', 'third', '##tri', '##mes', '##ter', 'abortion', 'demand']\n",
      "Token IDs:  [2360, 8194, 2015, 2862, 2576, 2177, 2490, 2353, 18886, 7834, 3334, 11324, 5157]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_text[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_text[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dce90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12791/12791 [00:01<00:00, 7124.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94276bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12791,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae808334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12791,), (12791,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, train_text_filtered.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12791/12791 [00:02<00:00, 4740.82it/s]\n",
      "/tmp/ipykernel_8413/2816128611.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels_filtered = tokenize_map(train_text_filtered, labels_filtered)\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "745e5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34763ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f67d86270d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d21142e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca257209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12791]), torch.Size([12791, 512]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c866b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12791])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30770dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_size, val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 5 -----\n",
      "  Batch    50  of  2,558.    Elapsed: 18.022056579589844.\n",
      "  Batch   100  of  2,558.    Elapsed: 35.16300654411316.\n",
      "  Batch   150  of  2,558.    Elapsed: 52.399505615234375.\n",
      "  Batch   200  of  2,558.    Elapsed: 69.4800112247467.\n",
      "  Batch   250  of  2,558.    Elapsed: 86.67119908332825.\n",
      "  Batch   300  of  2,558.    Elapsed: 103.80137991905212.\n",
      "  Batch   350  of  2,558.    Elapsed: 120.94527506828308.\n",
      "  Batch   400  of  2,558.    Elapsed: 138.27227902412415.\n",
      "  Batch   450  of  2,558.    Elapsed: 155.8284728527069.\n",
      "  Batch   500  of  2,558.    Elapsed: 173.03303337097168.\n",
      "  Batch   550  of  2,558.    Elapsed: 190.4011104106903.\n",
      "  Batch   600  of  2,558.    Elapsed: 207.76721024513245.\n",
      "  Batch   650  of  2,558.    Elapsed: 224.97359538078308.\n",
      "  Batch   700  of  2,558.    Elapsed: 242.29281377792358.\n",
      "  Batch   750  of  2,558.    Elapsed: 259.6927402019501.\n",
      "  Batch   800  of  2,558.    Elapsed: 277.0478844642639.\n",
      "  Batch   850  of  2,558.    Elapsed: 294.5931251049042.\n",
      "  Batch   900  of  2,558.    Elapsed: 312.1658720970154.\n",
      "  Batch   950  of  2,558.    Elapsed: 329.5180368423462.\n",
      "  Batch 1,000  of  2,558.    Elapsed: 346.9695339202881.\n",
      "  Batch 1,050  of  2,558.    Elapsed: 364.4266006946564.\n",
      "  Batch 1,100  of  2,558.    Elapsed: 381.930677652359.\n",
      "  Batch 1,150  of  2,558.    Elapsed: 399.42344856262207.\n",
      "  Batch 1,200  of  2,558.    Elapsed: 416.68590569496155.\n",
      "  Batch 1,250  of  2,558.    Elapsed: 433.95419096946716.\n",
      "  Batch 1,300  of  2,558.    Elapsed: 451.2997679710388.\n",
      "  Batch 1,350  of  2,558.    Elapsed: 468.60467433929443.\n",
      "  Batch 1,400  of  2,558.    Elapsed: 486.33165431022644.\n",
      "  Batch 1,450  of  2,558.    Elapsed: 504.4865872859955.\n",
      "  Batch 1,500  of  2,558.    Elapsed: 522.2267918586731.\n",
      "  Batch 1,550  of  2,558.    Elapsed: 539.5429794788361.\n",
      "  Batch 1,600  of  2,558.    Elapsed: 557.402325630188.\n",
      "  Batch 1,650  of  2,558.    Elapsed: 574.6106872558594.\n",
      "  Batch 1,700  of  2,558.    Elapsed: 591.956503868103.\n",
      "  Batch 1,750  of  2,558.    Elapsed: 609.1592872142792.\n",
      "  Batch 1,800  of  2,558.    Elapsed: 626.6141855716705.\n",
      "  Batch 1,850  of  2,558.    Elapsed: 644.0993254184723.\n",
      "  Batch 1,900  of  2,558.    Elapsed: 661.594979763031.\n",
      "  Batch 1,950  of  2,558.    Elapsed: 679.0935099124908.\n",
      "  Batch 2,000  of  2,558.    Elapsed: 696.5298697948456.\n",
      "  Batch 2,050  of  2,558.    Elapsed: 713.9489867687225.\n",
      "  Batch 2,100  of  2,558.    Elapsed: 731.4133183956146.\n",
      "  Batch 2,150  of  2,558.    Elapsed: 748.7921895980835.\n",
      "  Batch 2,200  of  2,558.    Elapsed: 766.1967766284943.\n",
      "  Batch 2,250  of  2,558.    Elapsed: 783.5323085784912.\n",
      "  Batch 2,300  of  2,558.    Elapsed: 800.7988195419312.\n",
      "  Batch 2,350  of  2,558.    Elapsed: 818.417058467865.\n",
      "  Batch 2,400  of  2,558.    Elapsed: 835.591361284256.\n",
      "  Batch 2,450  of  2,558.    Elapsed: 852.709424495697.\n",
      "  Batch 2,500  of  2,558.    Elapsed: 869.8305308818817.\n",
      "  Batch 2,550  of  2,558.    Elapsed: 886.9372079372406.\n",
      "\n",
      "  Average training loss: 0.68\n",
      "  Training epoc h took: 889.6854140758514\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61510\n",
      "  F1: 0.62789\n",
      "  Validation Loss: 0.65884\n",
      "  Validation took: 93.92753386497498\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 5 -----\n",
      "  Batch    50  of  2,558.    Elapsed: 17.11999225616455.\n",
      "  Batch   100  of  2,558.    Elapsed: 34.272196531295776.\n",
      "  Batch   150  of  2,558.    Elapsed: 51.45991659164429.\n",
      "  Batch   200  of  2,558.    Elapsed: 68.60871648788452.\n",
      "  Batch   250  of  2,558.    Elapsed: 85.7571792602539.\n",
      "  Batch   300  of  2,558.    Elapsed: 102.89580297470093.\n",
      "  Batch   350  of  2,558.    Elapsed: 120.05932474136353.\n",
      "  Batch   400  of  2,558.    Elapsed: 137.2065863609314.\n",
      "  Batch   450  of  2,558.    Elapsed: 154.36956667900085.\n",
      "  Batch   500  of  2,558.    Elapsed: 171.48076510429382.\n",
      "  Batch   550  of  2,558.    Elapsed: 188.66837239265442.\n",
      "  Batch   600  of  2,558.    Elapsed: 205.97703647613525.\n",
      "  Batch   650  of  2,558.    Elapsed: 223.23671960830688.\n",
      "  Batch   700  of  2,558.    Elapsed: 240.41059398651123.\n",
      "  Batch   750  of  2,558.    Elapsed: 257.55398893356323.\n",
      "  Batch   800  of  2,558.    Elapsed: 274.7214651107788.\n",
      "  Batch   850  of  2,558.    Elapsed: 291.82327103614807.\n",
      "  Batch   900  of  2,558.    Elapsed: 308.99305963516235.\n",
      "  Batch   950  of  2,558.    Elapsed: 326.2884941101074.\n",
      "  Batch 1,000  of  2,558.    Elapsed: 343.57924151420593.\n",
      "  Batch 1,050  of  2,558.    Elapsed: 360.9406521320343.\n",
      "  Batch 1,100  of  2,558.    Elapsed: 378.1528697013855.\n",
      "  Batch 1,150  of  2,558.    Elapsed: 395.5046384334564.\n",
      "  Batch 1,200  of  2,558.    Elapsed: 412.7855591773987.\n",
      "  Batch 1,250  of  2,558.    Elapsed: 429.97315859794617.\n",
      "  Batch 1,300  of  2,558.    Elapsed: 447.1558635234833.\n",
      "  Batch 1,350  of  2,558.    Elapsed: 464.40736627578735.\n",
      "  Batch 1,400  of  2,558.    Elapsed: 481.5870316028595.\n",
      "  Batch 1,450  of  2,558.    Elapsed: 498.8429944515228.\n",
      "  Batch 1,500  of  2,558.    Elapsed: 516.0425493717194.\n",
      "  Batch 1,550  of  2,558.    Elapsed: 533.35471534729.\n",
      "  Batch 1,600  of  2,558.    Elapsed: 550.8106892108917.\n",
      "  Batch 1,650  of  2,558.    Elapsed: 568.088063955307.\n",
      "  Batch 1,700  of  2,558.    Elapsed: 585.5367136001587.\n",
      "  Batch 1,750  of  2,558.    Elapsed: 602.8042981624603.\n",
      "  Batch 1,800  of  2,558.    Elapsed: 620.1362023353577.\n",
      "  Batch 1,850  of  2,558.    Elapsed: 637.3124158382416.\n",
      "  Batch 1,900  of  2,558.    Elapsed: 654.5547814369202.\n",
      "  Batch 1,950  of  2,558.    Elapsed: 671.8968579769135.\n",
      "  Batch 2,000  of  2,558.    Elapsed: 689.132490158081.\n",
      "  Batch 2,050  of  2,558.    Elapsed: 706.3041026592255.\n",
      "  Batch 2,100  of  2,558.    Elapsed: 723.5037388801575.\n",
      "  Batch 2,150  of  2,558.    Elapsed: 740.7275876998901.\n",
      "  Batch 2,200  of  2,558.    Elapsed: 757.9484913349152.\n",
      "  Batch 2,250  of  2,558.    Elapsed: 775.1693530082703.\n",
      "  Batch 2,300  of  2,558.    Elapsed: 792.3583409786224.\n",
      "  Batch 2,350  of  2,558.    Elapsed: 809.5528607368469.\n",
      "  Batch 2,400  of  2,558.    Elapsed: 826.7183227539062.\n",
      "  Batch 2,450  of  2,558.    Elapsed: 843.9413294792175.\n",
      "  Batch 2,500  of  2,558.    Elapsed: 861.1282274723053.\n",
      "  Batch 2,550  of  2,558.    Elapsed: 878.3444621562958.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epoc h took: 881.0924963951111\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60664\n",
      "  F1: 0.54142\n",
      "  Validation Loss: 0.69818\n",
      "  Validation took: 94.40076756477356\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 5 -----\n",
      "  Batch    50  of  2,558.    Elapsed: 17.453788995742798.\n",
      "  Batch   100  of  2,558.    Elapsed: 34.885602712631226.\n",
      "  Batch   150  of  2,558.    Elapsed: 52.28077673912048.\n",
      "  Batch   200  of  2,558.    Elapsed: 69.48765635490417.\n",
      "  Batch   250  of  2,558.    Elapsed: 86.82859706878662.\n",
      "  Batch   300  of  2,558.    Elapsed: 104.6627299785614.\n",
      "  Batch   350  of  2,558.    Elapsed: 122.08595943450928.\n",
      "  Batch   400  of  2,558.    Elapsed: 139.33242893218994.\n",
      "  Batch   450  of  2,558.    Elapsed: 156.5719428062439.\n",
      "  Batch   500  of  2,558.    Elapsed: 173.83544397354126.\n",
      "  Batch   550  of  2,558.    Elapsed: 191.0825276374817.\n",
      "  Batch   600  of  2,558.    Elapsed: 208.27255082130432.\n",
      "  Batch   650  of  2,558.    Elapsed: 225.48036360740662.\n",
      "  Batch   700  of  2,558.    Elapsed: 242.71250176429749.\n",
      "  Batch   750  of  2,558.    Elapsed: 259.95501160621643.\n",
      "  Batch   800  of  2,558.    Elapsed: 277.39272379875183.\n",
      "  Batch   850  of  2,558.    Elapsed: 294.9259235858917.\n",
      "  Batch   900  of  2,558.    Elapsed: 312.18882632255554.\n",
      "  Batch   950  of  2,558.    Elapsed: 329.80309534072876.\n",
      "  Batch 1,000  of  2,558.    Elapsed: 347.462016582489.\n",
      "  Batch 1,050  of  2,558.    Elapsed: 365.11616921424866.\n",
      "  Batch 1,100  of  2,558.    Elapsed: 382.3642144203186.\n",
      "  Batch 1,150  of  2,558.    Elapsed: 399.97687458992004.\n",
      "  Batch 1,200  of  2,558.    Elapsed: 417.6491687297821.\n",
      "  Batch 1,250  of  2,558.    Elapsed: 435.30043840408325.\n",
      "  Batch 1,300  of  2,558.    Elapsed: 452.8763394355774.\n",
      "  Batch 1,350  of  2,558.    Elapsed: 470.35957503318787.\n",
      "  Batch 1,400  of  2,558.    Elapsed: 487.700790643692.\n",
      "  Batch 1,450  of  2,558.    Elapsed: 505.24647641181946.\n",
      "  Batch 1,500  of  2,558.    Elapsed: 522.6535634994507.\n",
      "  Batch 1,550  of  2,558.    Elapsed: 540.5919663906097.\n",
      "  Batch 1,600  of  2,558.    Elapsed: 557.9777224063873.\n",
      "  Batch 1,650  of  2,558.    Elapsed: 575.5860056877136.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,700  of  2,558.    Elapsed: 592.9430689811707.\n",
      "  Batch 1,750  of  2,558.    Elapsed: 610.5048551559448.\n",
      "  Batch 1,800  of  2,558.    Elapsed: 628.1325466632843.\n",
      "  Batch 1,850  of  2,558.    Elapsed: 645.6687848567963.\n",
      "  Batch 1,900  of  2,558.    Elapsed: 663.1005370616913.\n",
      "  Batch 1,950  of  2,558.    Elapsed: 680.7579939365387.\n",
      "  Batch 2,000  of  2,558.    Elapsed: 697.9522998332977.\n",
      "  Batch 2,050  of  2,558.    Elapsed: 715.4444420337677.\n",
      "  Batch 2,100  of  2,558.    Elapsed: 733.1167652606964.\n",
      "  Batch 2,150  of  2,558.    Elapsed: 751.2881557941437.\n",
      "  Batch 2,200  of  2,558.    Elapsed: 768.7774887084961.\n",
      "  Batch 2,250  of  2,558.    Elapsed: 786.2936744689941.\n",
      "  Batch 2,300  of  2,558.    Elapsed: 803.9449002742767.\n",
      "  Batch 2,350  of  2,558.    Elapsed: 821.4651713371277.\n",
      "  Batch 2,400  of  2,558.    Elapsed: 838.8354873657227.\n",
      "  Batch 2,450  of  2,558.    Elapsed: 856.1337380409241.\n",
      "  Batch 2,500  of  2,558.    Elapsed: 873.296046257019.\n",
      "  Batch 2,550  of  2,558.    Elapsed: 890.521556854248.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoc h took: 893.3472657203674\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60352\n",
      "  F1: 0.62967\n",
      "  Validation Loss: 1.15796\n",
      "  Validation took: 95.11472415924072\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 5 -----\n",
      "  Batch    50  of  2,558.    Elapsed: 17.347503185272217.\n",
      "  Batch   100  of  2,558.    Elapsed: 34.88267970085144.\n",
      "  Batch   150  of  2,558.    Elapsed: 52.37777328491211.\n",
      "  Batch   200  of  2,558.    Elapsed: 69.80165815353394.\n",
      "  Batch   250  of  2,558.    Elapsed: 87.14456081390381.\n",
      "  Batch   300  of  2,558.    Elapsed: 104.3613224029541.\n",
      "  Batch   350  of  2,558.    Elapsed: 121.59921598434448.\n",
      "  Batch   400  of  2,558.    Elapsed: 139.27638936042786.\n",
      "  Batch   450  of  2,558.    Elapsed: 156.81747317314148.\n",
      "  Batch   500  of  2,558.    Elapsed: 174.06295728683472.\n",
      "  Batch   550  of  2,558.    Elapsed: 191.55899286270142.\n",
      "  Batch   600  of  2,558.    Elapsed: 208.9081540107727.\n",
      "  Batch   650  of  2,558.    Elapsed: 226.05798029899597.\n",
      "  Batch   700  of  2,558.    Elapsed: 243.51247358322144.\n",
      "  Batch   750  of  2,558.    Elapsed: 260.7838168144226.\n",
      "  Batch   800  of  2,558.    Elapsed: 278.3519184589386.\n",
      "  Batch   850  of  2,558.    Elapsed: 295.986873626709.\n",
      "  Batch   900  of  2,558.    Elapsed: 313.6709237098694.\n",
      "  Batch   950  of  2,558.    Elapsed: 331.32954812049866.\n",
      "  Batch 1,000  of  2,558.    Elapsed: 348.97119188308716.\n",
      "  Batch 1,050  of  2,558.    Elapsed: 366.6281635761261.\n",
      "  Batch 1,100  of  2,558.    Elapsed: 384.2794632911682.\n",
      "  Batch 1,150  of  2,558.    Elapsed: 401.860413312912.\n",
      "  Batch 1,200  of  2,558.    Elapsed: 419.5811233520508.\n",
      "  Batch 1,250  of  2,558.    Elapsed: 437.24913930892944.\n",
      "  Batch 1,300  of  2,558.    Elapsed: 455.21046137809753.\n",
      "  Batch 1,350  of  2,558.    Elapsed: 472.7849061489105.\n",
      "  Batch 1,400  of  2,558.    Elapsed: 489.96123695373535.\n",
      "  Batch 1,450  of  2,558.    Elapsed: 507.1052248477936.\n",
      "  Batch 1,500  of  2,558.    Elapsed: 524.7760488986969.\n",
      "  Batch 1,550  of  2,558.    Elapsed: 542.3866727352142.\n",
      "  Batch 1,600  of  2,558.    Elapsed: 560.344756603241.\n",
      "  Batch 1,650  of  2,558.    Elapsed: 578.4321429729462.\n",
      "  Batch 1,700  of  2,558.    Elapsed: 595.8636355400085.\n",
      "  Batch 1,750  of  2,558.    Elapsed: 613.0893335342407.\n",
      "  Batch 1,800  of  2,558.    Elapsed: 630.2155919075012.\n",
      "  Batch 1,850  of  2,558.    Elapsed: 647.3975784778595.\n",
      "  Batch 1,900  of  2,558.    Elapsed: 664.5526962280273.\n",
      "  Batch 1,950  of  2,558.    Elapsed: 682.1558518409729.\n",
      "  Batch 2,000  of  2,558.    Elapsed: 699.5867123603821.\n",
      "  Batch 2,050  of  2,558.    Elapsed: 717.0230407714844.\n",
      "  Batch 2,100  of  2,558.    Elapsed: 734.2231395244598.\n",
      "  Batch 2,150  of  2,558.    Elapsed: 751.6441864967346.\n",
      "  Batch 2,200  of  2,558.    Elapsed: 768.8146929740906.\n",
      "  Batch 2,250  of  2,558.    Elapsed: 786.0226986408234.\n",
      "  Batch 2,300  of  2,558.    Elapsed: 803.1599986553192.\n",
      "  Batch 2,350  of  2,558.    Elapsed: 820.2836656570435.\n",
      "  Batch 2,400  of  2,558.    Elapsed: 837.485785484314.\n",
      "  Batch 2,450  of  2,558.    Elapsed: 854.6610867977142.\n",
      "  Batch 2,500  of  2,558.    Elapsed: 871.8123333454132.\n",
      "  Batch 2,550  of  2,558.    Elapsed: 888.9874129295349.\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epoc h took: 891.7520534992218\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.59297\n",
      "  F1: 0.60084\n",
      "  Validation Loss: 1.77610\n",
      "  Validation took: 94.05742383003235\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 5 -----\n",
      "  Batch    50  of  2,558.    Elapsed: 17.106865167617798.\n",
      "  Batch   100  of  2,558.    Elapsed: 34.25536823272705.\n",
      "  Batch   150  of  2,558.    Elapsed: 51.43011140823364.\n",
      "  Batch   200  of  2,558.    Elapsed: 68.6831443309784.\n",
      "  Batch   250  of  2,558.    Elapsed: 85.83351230621338.\n",
      "  Batch   300  of  2,558.    Elapsed: 102.96269726753235.\n",
      "  Batch   350  of  2,558.    Elapsed: 120.08073139190674.\n",
      "  Batch   400  of  2,558.    Elapsed: 137.22495698928833.\n",
      "  Batch   450  of  2,558.    Elapsed: 154.36015272140503.\n",
      "  Batch   500  of  2,558.    Elapsed: 171.5275411605835.\n",
      "  Batch   550  of  2,558.    Elapsed: 188.63535714149475.\n",
      "  Batch   600  of  2,558.    Elapsed: 205.8045835494995.\n",
      "  Batch   650  of  2,558.    Elapsed: 222.96186876296997.\n",
      "  Batch   700  of  2,558.    Elapsed: 240.10788702964783.\n",
      "  Batch   750  of  2,558.    Elapsed: 257.2075650691986.\n",
      "  Batch   800  of  2,558.    Elapsed: 274.31928396224976.\n",
      "  Batch   850  of  2,558.    Elapsed: 291.43848943710327.\n",
      "  Batch   900  of  2,558.    Elapsed: 308.5365197658539.\n",
      "  Batch   950  of  2,558.    Elapsed: 325.7386426925659.\n",
      "  Batch 1,000  of  2,558.    Elapsed: 342.87022733688354.\n",
      "  Batch 1,050  of  2,558.    Elapsed: 360.0565736293793.\n",
      "  Batch 1,100  of  2,558.    Elapsed: 377.2346742153168.\n",
      "  Batch 1,150  of  2,558.    Elapsed: 394.3698818683624.\n",
      "  Batch 1,200  of  2,558.    Elapsed: 411.51286363601685.\n",
      "  Batch 1,250  of  2,558.    Elapsed: 428.65742659568787.\n",
      "  Batch 1,300  of  2,558.    Elapsed: 445.8726372718811.\n",
      "  Batch 1,350  of  2,558.    Elapsed: 462.9771554470062.\n",
      "  Batch 1,400  of  2,558.    Elapsed: 480.0968105792999.\n",
      "  Batch 1,450  of  2,558.    Elapsed: 497.21319246292114.\n",
      "  Batch 1,500  of  2,558.    Elapsed: 514.3288242816925.\n",
      "  Batch 1,550  of  2,558.    Elapsed: 531.4632577896118.\n",
      "  Batch 1,600  of  2,558.    Elapsed: 548.6193146705627.\n",
      "  Batch 1,650  of  2,558.    Elapsed: 565.7783966064453.\n",
      "  Batch 1,700  of  2,558.    Elapsed: 582.9197700023651.\n",
      "  Batch 1,750  of  2,558.    Elapsed: 600.1341304779053.\n",
      "  Batch 1,800  of  2,558.    Elapsed: 617.3127841949463.\n",
      "  Batch 1,850  of  2,558.    Elapsed: 634.455851316452.\n",
      "  Batch 1,900  of  2,558.    Elapsed: 651.5651822090149.\n",
      "  Batch 1,950  of  2,558.    Elapsed: 668.6911127567291.\n",
      "  Batch 2,000  of  2,558.    Elapsed: 686.0332777500153.\n",
      "  Batch 2,050  of  2,558.    Elapsed: 703.4305293560028.\n",
      "  Batch 2,100  of  2,558.    Elapsed: 720.7249841690063.\n",
      "  Batch 2,150  of  2,558.    Elapsed: 737.9269738197327.\n",
      "  Batch 2,200  of  2,558.    Elapsed: 755.1847746372223.\n",
      "  Batch 2,250  of  2,558.    Elapsed: 772.6570725440979.\n",
      "  Batch 2,300  of  2,558.    Elapsed: 789.793304681778.\n",
      "  Batch 2,350  of  2,558.    Elapsed: 806.9691710472107.\n",
      "  Batch 2,400  of  2,558.    Elapsed: 824.1582953929901.\n",
      "  Batch 2,450  of  2,558.    Elapsed: 841.2877571582794.\n",
      "  Batch 2,500  of  2,558.    Elapsed: 858.4544320106506.\n",
      "  Batch 2,550  of  2,558.    Elapsed: 875.6103050708771.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoc h took: 878.3576714992523\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.58971\n",
      "  F1: 0.61278\n",
      "  Validation Loss: 2.16429\n",
      "  Validation took: 93.97856998443604\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b180fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/bert_liar_regexp_stopwords_lemmatization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
