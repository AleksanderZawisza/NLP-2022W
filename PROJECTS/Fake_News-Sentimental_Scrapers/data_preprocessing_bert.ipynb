{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0e3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e166dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26509ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isn',\n",
       " 'm',\n",
       " \"wouldn't\",\n",
       " 'again',\n",
       " \"doesn't\",\n",
       " 'not',\n",
       " 'own',\n",
       " 'he',\n",
       " \"needn't\",\n",
       " 'both']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "random.sample(stopwords.words('english'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac03f72",
   "metadata": {},
   "source": [
    "Consider removing some stop words like _no_, _yes_, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449db28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"FakeNews\": \"\",\n",
    "    \"ISOT\": \"\"\n",
    "}\n",
    "\n",
    "CUR_DATASET = \"FakeNews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/train.csv.zip\")\n",
    "test_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb85e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = pd.concat([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8793d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd0a899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0.000000\n",
       "title     0.026827\n",
       "author    0.094087\n",
       "text      0.001875\n",
       "label     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.isnull().sum() / train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "743ed40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26000 entries, 0 to 5199\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      26000 non-null  int64  \n",
      " 1   title   25320 non-null  object \n",
      " 2   author  23540 non-null  object \n",
      " 3   text    25954 non-null  object \n",
      " 4   label   20800 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "whole_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8871882a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.fillna(\"null data\")\n",
    "test_dataset = test_dataset.fillna(\"null data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', '', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697b466",
   "metadata": {},
   "source": [
    "Consider removing some of the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c45835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"haven't\",\n",
       " 'them',\n",
       " 'our',\n",
       " 'wouldn',\n",
       " 'those',\n",
       " 'they',\n",
       " 'off',\n",
       " \"don't\",\n",
       " 'itself',\n",
       " \"that'll\",\n",
       " 'same',\n",
       " 'an',\n",
       " 'weren',\n",
       " 'where',\n",
       " 'aren',\n",
       " 'then',\n",
       " 'shouldn',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 't']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "STOP_WORDS = [preprocessing_text_fn[\"no_punctuation\"](word) for word in stop_words]\n",
    "random.sample(stop_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f7b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_stopwords(text, stop_words=STOP_WORDS):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sequence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "    return filtered_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide We Didnt Even See Comeys Letter...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide We Didnt Even See Comeys Letter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN Hillary Clinton Big Woman on Campus Brei...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October The ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Civilians Killed In Single US Airstrike Have ...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos Civilians Killed In Single US Airstrike...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print An Iranian woman has been sentenced to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jackie Mason Hollywood Would Love Trump if He ...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>In these trying times Jackie Mason is the Voic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Life Life Of Luxury Elton Johns Favorite Shark...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Ever wonder how Britains most iconic pop piani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Benoît Hamon Wins French Socialist Partys Pres...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>PARIS France chose an idealistic traditional c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Excerpts FromDraft Script for Donald Trumps Qa...</td>\n",
       "      <td>null data</td>\n",
       "      <td>DonaldTrump is scheduled to makehighly anticip...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A BackChannel Plan for Ukraine and Russia Cour...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>A week before MichaelFlynn resigned as nationa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  House Dem Aide We Didnt Even See Comeys Letter...   \n",
       "1   1  FLYNN Hillary Clinton Big Woman on Campus Brei...   \n",
       "2   2                  Why the Truth Might Get You Fired   \n",
       "3   3   Civilians Killed In Single US Airstrike Have ...   \n",
       "4   4  Iranian woman jailed for fictional unpublished...   \n",
       "5   5  Jackie Mason Hollywood Would Love Trump if He ...   \n",
       "6   6  Life Life Of Luxury Elton Johns Favorite Shark...   \n",
       "7   7  Benoît Hamon Wins French Socialist Partys Pres...   \n",
       "8   8  Excerpts FromDraft Script for Donald Trumps Qa...   \n",
       "9   9  A BackChannel Plan for Ukraine and Russia Cour...   \n",
       "\n",
       "                         author  \\\n",
       "0                 Darrell Lucus   \n",
       "1               Daniel J. Flynn   \n",
       "2            Consortiumnews.com   \n",
       "3               Jessica Purkiss   \n",
       "4                Howard Portnoy   \n",
       "5               Daniel Nussbaum   \n",
       "6                     null data   \n",
       "7               Alissa J. Rubin   \n",
       "8                     null data   \n",
       "9  Megan Twohey and Scott Shane   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide We Didnt Even See Comeys Letter...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October The ...      1  \n",
       "3  Videos Civilians Killed In Single US Airstrike...      1  \n",
       "4  Print An Iranian woman has been sentenced to s...      1  \n",
       "5  In these trying times Jackie Mason is the Voic...      0  \n",
       "6  Ever wonder how Britains most iconic pop piani...      1  \n",
       "7  PARIS France chose an idealistic traditional c...      0  \n",
       "8  DonaldTrump is scheduled to makehighly anticip...      0  \n",
       "9  A week before MichaelFlynn resigned as nationa...      0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"title\"] = train_dataset[\"title\"].apply(preprocess_text)\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(preprocess_text)\n",
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c49c3345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues if Not Purse ...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO Calif After years of scorning the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>NoDAPL Native American Leaders Vow to Stay All...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos NoDAPL Native American Leaders Vow to S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback This T...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you dont succeed trydifferent spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report Meme Wars E</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>mins ago Views Comments Likes For the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>Pelosi Calls for FBI Investigation to Find Out...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Sunday on NBCs Meet the Press House Minority L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>Weekly Featured Profile Randy Shannon</td>\n",
       "      <td>Trevor Loudon</td>\n",
       "      <td>You are here Home Articles of the Bound Weekly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>null data</td>\n",
       "      <td>cognitive dissident</td>\n",
       "      <td>dont we have the receipt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues if Not Purse ...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  NoDAPL Native American Leaders Vow to Stay All...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback This T...   \n",
       "4  20804                          Keiser Report Meme Wars E   \n",
       "5  20805  Trump is USAs antique hero Clinton will be nex...   \n",
       "6  20806  Pelosi Calls for FBI Investigation to Find Out...   \n",
       "7  20807              Weekly Featured Profile Randy Shannon   \n",
       "8  20808  Urban Population Booms Will Make Climate Chang...   \n",
       "9  20809                                          null data   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO Calif After years of scorning the po...  \n",
       "1                null data  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos NoDAPL Native American Leaders Vow to S...  \n",
       "3            Daniel Victor  If at first you dont succeed trydifferent spor...  \n",
       "4  Truth Broadcast Network   mins ago Views Comments Likes For the first t...  \n",
       "5                null data  Trump is USAs antique hero Clinton will be nex...  \n",
       "6                  Pam Key  Sunday on NBCs Meet the Press House Minority L...  \n",
       "7            Trevor Loudon  You are here Home Articles of the Bound Weekly...  \n",
       "8                null data  Urban Population Booms Will Make Climate Chang...  \n",
       "9      cognitive dissident                           dont we have the receipt  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"title\"] = test_dataset[\"title\"].apply(preprocess_text)\n",
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(preprocess_text)\n",
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3f44060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[\"title\"] = train_dataset[\"title\"].apply(tokenize_without_stopwords)\n",
    "# train_dataset[\"text\"] = train_dataset[\"text\"].apply(tokenize_without_stopwords)\n",
    "# train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574b00aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_dataset[\"title\"] = test_dataset[\"title\"].apply(tokenize_without_stopwords)\n",
    "# test_dataset[\"text\"] = test_dataset[\"text\"].apply(tokenize_without_stopwords)\n",
    "# test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bb16b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text = train_dataset['text'].values\n",
    "train_text = (train_dataset['author'] + \" \" + train_dataset['title']).values\n",
    "test_text = (test_dataset['author'] + \" \" + test_dataset['title']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6899268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a6a97",
   "metadata": {},
   "source": [
    "In the Kaggle competition the best scores were obtained by using only 'author' and 'title' features. Let's take a look if it's possible to train BERT using text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7676622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Darrell Lucus House Dem Aide We Didnt Even See Comeys Letter Until Jason Chaffetz Tweeted It\n",
      "Tokenized:  ['darrell', 'luc', '##us', 'house', 'dem', 'aide', 'we', 'didn', '##t', 'even', 'see', 'come', '##ys', 'letter', 'until', 'jason', 'cha', '##ffe', '##tz', 't', '##wee', '##ted', 'it']\n",
      "Token IDs:  [23158, 12776, 2271, 2160, 17183, 14895, 2057, 2134, 2102, 2130, 2156, 2272, 7274, 3661, 2127, 4463, 15775, 16020, 5753, 1056, 28394, 3064, 2009]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_text[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_text[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 20800/20800 [00:03<00:00, 5885.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for text in tqdm(train_text):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence,labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in sentence:\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = 107,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800,), (5200,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, test_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19998/1833132543.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels = tokenize_map(train_text, labels)\n",
    "test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30770dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_size, val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cba20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 5 -----\n",
      "  Batch    40  of  2,080.    Elapsed: 7.569205045700073.\n",
      "  Batch    80  of  2,080.    Elapsed: 14.930202722549438.\n",
      "  Batch   120  of  2,080.    Elapsed: 22.323073625564575.\n",
      "  Batch   160  of  2,080.    Elapsed: 29.761128902435303.\n",
      "  Batch   200  of  2,080.    Elapsed: 37.256072998046875.\n",
      "  Batch   240  of  2,080.    Elapsed: 44.73497271537781.\n",
      "  Batch   280  of  2,080.    Elapsed: 52.26723623275757.\n",
      "  Batch   320  of  2,080.    Elapsed: 59.7916533946991.\n",
      "  Batch   360  of  2,080.    Elapsed: 67.35209608078003.\n",
      "  Batch   400  of  2,080.    Elapsed: 74.9380567073822.\n",
      "  Batch   440  of  2,080.    Elapsed: 82.5787570476532.\n",
      "  Batch   480  of  2,080.    Elapsed: 90.19231677055359.\n",
      "  Batch   520  of  2,080.    Elapsed: 97.85612797737122.\n",
      "  Batch   560  of  2,080.    Elapsed: 105.5098352432251.\n",
      "  Batch   600  of  2,080.    Elapsed: 113.17989873886108.\n",
      "  Batch   640  of  2,080.    Elapsed: 120.85448169708252.\n",
      "  Batch   680  of  2,080.    Elapsed: 128.53773856163025.\n",
      "  Batch   720  of  2,080.    Elapsed: 136.23176288604736.\n",
      "  Batch   760  of  2,080.    Elapsed: 144.0086567401886.\n",
      "  Batch   800  of  2,080.    Elapsed: 151.73849987983704.\n",
      "  Batch   840  of  2,080.    Elapsed: 159.4886829853058.\n",
      "  Batch   880  of  2,080.    Elapsed: 167.24067974090576.\n",
      "  Batch   920  of  2,080.    Elapsed: 174.96877813339233.\n",
      "  Batch   960  of  2,080.    Elapsed: 182.7267460823059.\n",
      "  Batch 1,000  of  2,080.    Elapsed: 190.475093126297.\n",
      "  Batch 1,040  of  2,080.    Elapsed: 198.2698187828064.\n",
      "  Batch 1,080  of  2,080.    Elapsed: 206.01512813568115.\n",
      "  Batch 1,120  of  2,080.    Elapsed: 213.7679362297058.\n",
      "  Batch 1,160  of  2,080.    Elapsed: 221.53452467918396.\n",
      "  Batch 1,200  of  2,080.    Elapsed: 229.3573305606842.\n",
      "  Batch 1,240  of  2,080.    Elapsed: 237.20047783851624.\n",
      "  Batch 1,280  of  2,080.    Elapsed: 245.0786895751953.\n",
      "  Batch 1,320  of  2,080.    Elapsed: 252.86657190322876.\n",
      "  Batch 1,360  of  2,080.    Elapsed: 260.68768978118896.\n",
      "  Batch 1,400  of  2,080.    Elapsed: 268.5305836200714.\n",
      "  Batch 1,440  of  2,080.    Elapsed: 276.3069517612457.\n",
      "  Batch 1,480  of  2,080.    Elapsed: 284.0839810371399.\n",
      "  Batch 1,520  of  2,080.    Elapsed: 291.9277722835541.\n",
      "  Batch 1,560  of  2,080.    Elapsed: 299.7587025165558.\n",
      "  Batch 1,600  of  2,080.    Elapsed: 307.6836178302765.\n",
      "  Batch 1,640  of  2,080.    Elapsed: 315.4884731769562.\n",
      "  Batch 1,680  of  2,080.    Elapsed: 323.3667502403259.\n",
      "  Batch 1,720  of  2,080.    Elapsed: 331.24100041389465.\n",
      "  Batch 1,760  of  2,080.    Elapsed: 339.0154778957367.\n",
      "  Batch 1,800  of  2,080.    Elapsed: 346.85561752319336.\n",
      "  Batch 1,840  of  2,080.    Elapsed: 354.70867395401.\n",
      "  Batch 1,880  of  2,080.    Elapsed: 362.4970762729645.\n",
      "  Batch 1,920  of  2,080.    Elapsed: 370.30775356292725.\n",
      "  Batch 1,960  of  2,080.    Elapsed: 378.1336143016815.\n",
      "  Batch 2,000  of  2,080.    Elapsed: 385.9916899204254.\n",
      "  Batch 2,040  of  2,080.    Elapsed: 393.91890144348145.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoc h took: 401.8309545516968\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99808\n",
      "  F1: 0.98839\n",
      "  Validation Loss: 0.02242\n",
      "  Validation took: 40.76230764389038\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 5 -----\n",
      "  Batch    40  of  2,080.    Elapsed: 7.899852752685547.\n",
      "  Batch    80  of  2,080.    Elapsed: 15.77142882347107.\n",
      "  Batch   120  of  2,080.    Elapsed: 23.598982095718384.\n",
      "  Batch   160  of  2,080.    Elapsed: 31.45575451850891.\n",
      "  Batch   200  of  2,080.    Elapsed: 39.346439361572266.\n",
      "  Batch   240  of  2,080.    Elapsed: 47.16419982910156.\n",
      "  Batch   280  of  2,080.    Elapsed: 55.053699016571045.\n",
      "  Batch   320  of  2,080.    Elapsed: 62.82506203651428.\n",
      "  Batch   360  of  2,080.    Elapsed: 70.7035710811615.\n",
      "  Batch   400  of  2,080.    Elapsed: 78.53822374343872.\n",
      "  Batch   440  of  2,080.    Elapsed: 86.41547727584839.\n",
      "  Batch   480  of  2,080.    Elapsed: 94.25663185119629.\n",
      "  Batch   520  of  2,080.    Elapsed: 102.20321726799011.\n",
      "  Batch   560  of  2,080.    Elapsed: 110.05482149124146.\n",
      "  Batch   600  of  2,080.    Elapsed: 117.85931444168091.\n",
      "  Batch   640  of  2,080.    Elapsed: 125.68719601631165.\n",
      "  Batch   680  of  2,080.    Elapsed: 133.54888772964478.\n",
      "  Batch   720  of  2,080.    Elapsed: 141.38322496414185.\n",
      "  Batch   760  of  2,080.    Elapsed: 149.23104310035706.\n",
      "  Batch   800  of  2,080.    Elapsed: 157.08132529258728.\n",
      "  Batch   840  of  2,080.    Elapsed: 164.94330668449402.\n",
      "  Batch   880  of  2,080.    Elapsed: 172.83599185943604.\n",
      "  Batch   920  of  2,080.    Elapsed: 180.69620990753174.\n",
      "  Batch   960  of  2,080.    Elapsed: 188.6176974773407.\n",
      "  Batch 1,000  of  2,080.    Elapsed: 196.4727761745453.\n",
      "  Batch 1,040  of  2,080.    Elapsed: 204.31336855888367.\n",
      "  Batch 1,080  of  2,080.    Elapsed: 212.19103121757507.\n",
      "  Batch 1,120  of  2,080.    Elapsed: 220.35135769844055.\n",
      "  Batch 1,160  of  2,080.    Elapsed: 228.35797023773193.\n",
      "  Batch 1,200  of  2,080.    Elapsed: 236.26573657989502.\n",
      "  Batch 1,240  of  2,080.    Elapsed: 244.15738534927368.\n",
      "  Batch 1,280  of  2,080.    Elapsed: 252.17494416236877.\n",
      "  Batch 1,320  of  2,080.    Elapsed: 260.19101452827454.\n",
      "  Batch 1,360  of  2,080.    Elapsed: 268.0231444835663.\n",
      "  Batch 1,400  of  2,080.    Elapsed: 276.092027425766.\n",
      "  Batch 1,440  of  2,080.    Elapsed: 284.2018029689789.\n",
      "  Batch 1,480  of  2,080.    Elapsed: 292.36173701286316.\n",
      "  Batch 1,520  of  2,080.    Elapsed: 300.5488555431366.\n",
      "  Batch 1,560  of  2,080.    Elapsed: 308.78127884864807.\n",
      "  Batch 1,600  of  2,080.    Elapsed: 316.93701577186584.\n",
      "  Batch 1,640  of  2,080.    Elapsed: 325.12223720550537.\n",
      "  Batch 1,680  of  2,080.    Elapsed: 333.31059861183167.\n",
      "  Batch 1,720  of  2,080.    Elapsed: 341.5112793445587.\n",
      "  Batch 1,760  of  2,080.    Elapsed: 349.6761064529419.\n",
      "  Batch 1,800  of  2,080.    Elapsed: 357.8914031982422.\n",
      "  Batch 1,840  of  2,080.    Elapsed: 366.0982882976532.\n",
      "  Batch 1,880  of  2,080.    Elapsed: 374.26854515075684.\n",
      "  Batch 1,920  of  2,080.    Elapsed: 382.4700381755829.\n",
      "  Batch 1,960  of  2,080.    Elapsed: 390.6065363883972.\n",
      "  Batch 2,000  of  2,080.    Elapsed: 398.80942034721375.\n",
      "  Batch 2,040  of  2,080.    Elapsed: 407.0115473270416.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoc h took: 415.18373942375183\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99760\n",
      "  F1: 0.98753\n",
      "  Validation Loss: 0.03299\n",
      "  Validation took: 41.11458468437195\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 5 -----\n",
      "  Batch    40  of  2,080.    Elapsed: 8.206607818603516.\n",
      "  Batch    80  of  2,080.    Elapsed: 16.38247513771057.\n",
      "  Batch   120  of  2,080.    Elapsed: 24.434902667999268.\n",
      "  Batch   160  of  2,080.    Elapsed: 32.47109818458557.\n",
      "  Batch   200  of  2,080.    Elapsed: 40.530410051345825.\n",
      "  Batch   240  of  2,080.    Elapsed: 48.441991567611694.\n",
      "  Batch   280  of  2,080.    Elapsed: 56.553415298461914.\n",
      "  Batch   320  of  2,080.    Elapsed: 64.60986590385437.\n",
      "  Batch   360  of  2,080.    Elapsed: 72.62580800056458.\n",
      "  Batch   400  of  2,080.    Elapsed: 80.69424486160278.\n",
      "  Batch   440  of  2,080.    Elapsed: 88.74522542953491.\n",
      "  Batch   480  of  2,080.    Elapsed: 96.79158139228821.\n",
      "  Batch   520  of  2,080.    Elapsed: 104.80403590202332.\n",
      "  Batch   560  of  2,080.    Elapsed: 112.80157375335693.\n",
      "  Batch   600  of  2,080.    Elapsed: 120.77660870552063.\n",
      "  Batch   640  of  2,080.    Elapsed: 128.80029678344727.\n",
      "  Batch   680  of  2,080.    Elapsed: 136.79467725753784.\n",
      "  Batch   720  of  2,080.    Elapsed: 144.67773914337158.\n",
      "  Batch   760  of  2,080.    Elapsed: 152.84143781661987.\n",
      "  Batch   800  of  2,080.    Elapsed: 160.8576283454895.\n",
      "  Batch   840  of  2,080.    Elapsed: 168.80260229110718.\n",
      "  Batch   880  of  2,080.    Elapsed: 176.81180810928345.\n",
      "  Batch   920  of  2,080.    Elapsed: 184.9711537361145.\n",
      "  Batch   960  of  2,080.    Elapsed: 192.97779822349548.\n",
      "  Batch 1,000  of  2,080.    Elapsed: 200.97261762619019.\n",
      "  Batch 1,040  of  2,080.    Elapsed: 209.01299023628235.\n",
      "  Batch 1,080  of  2,080.    Elapsed: 216.99027562141418.\n",
      "  Batch 1,120  of  2,080.    Elapsed: 224.97006702423096.\n",
      "  Batch 1,160  of  2,080.    Elapsed: 233.00244116783142.\n",
      "  Batch 1,200  of  2,080.    Elapsed: 241.1060643196106.\n",
      "  Batch 1,240  of  2,080.    Elapsed: 249.14216494560242.\n",
      "  Batch 1,280  of  2,080.    Elapsed: 257.09818410873413.\n",
      "  Batch 1,320  of  2,080.    Elapsed: 265.2111463546753.\n",
      "  Batch 1,360  of  2,080.    Elapsed: 273.2426280975342.\n",
      "  Batch 1,400  of  2,080.    Elapsed: 281.25758814811707.\n",
      "  Batch 1,440  of  2,080.    Elapsed: 289.2607789039612.\n",
      "  Batch 1,480  of  2,080.    Elapsed: 297.2685778141022.\n",
      "  Batch 1,520  of  2,080.    Elapsed: 305.28266882896423.\n",
      "  Batch 1,560  of  2,080.    Elapsed: 313.33387780189514.\n",
      "  Batch 1,600  of  2,080.    Elapsed: 321.3240327835083.\n",
      "  Batch 1,640  of  2,080.    Elapsed: 329.37753772735596.\n",
      "  Batch 1,680  of  2,080.    Elapsed: 337.5399806499481.\n",
      "  Batch 1,720  of  2,080.    Elapsed: 345.5234627723694.\n",
      "  Batch 1,760  of  2,080.    Elapsed: 353.57266879081726.\n",
      "  Batch 1,800  of  2,080.    Elapsed: 361.7075698375702.\n",
      "  Batch 1,840  of  2,080.    Elapsed: 369.76204800605774.\n",
      "  Batch 1,880  of  2,080.    Elapsed: 377.86912083625793.\n",
      "  Batch 1,920  of  2,080.    Elapsed: 385.96449089050293.\n",
      "  Batch 1,960  of  2,080.    Elapsed: 394.14170122146606.\n",
      "  Batch 2,000  of  2,080.    Elapsed: 402.1877315044403.\n",
      "  Batch 2,040  of  2,080.    Elapsed: 410.21343541145325.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoc h took: 418.25137424468994\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99832\n",
      "  F1: 0.98872\n",
      "  Validation Loss: 0.01859\n",
      "  Validation took: 40.93830060958862\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 5 -----\n",
      "  Batch    40  of  2,080.    Elapsed: 8.113901853561401.\n",
      "  Batch    80  of  2,080.    Elapsed: 16.139320135116577.\n",
      "  Batch   120  of  2,080.    Elapsed: 24.164429187774658.\n",
      "  Batch   160  of  2,080.    Elapsed: 32.19780731201172.\n",
      "  Batch   200  of  2,080.    Elapsed: 40.285669565200806.\n",
      "  Batch   240  of  2,080.    Elapsed: 48.30656099319458.\n",
      "  Batch   280  of  2,080.    Elapsed: 56.366872787475586.\n",
      "  Batch   320  of  2,080.    Elapsed: 64.48837733268738.\n",
      "  Batch   360  of  2,080.    Elapsed: 72.59440493583679.\n",
      "  Batch   400  of  2,080.    Elapsed: 80.82812237739563.\n",
      "  Batch   440  of  2,080.    Elapsed: 88.98242545127869.\n",
      "  Batch   480  of  2,080.    Elapsed: 97.14892411231995.\n",
      "  Batch   520  of  2,080.    Elapsed: 105.33432054519653.\n",
      "  Batch   560  of  2,080.    Elapsed: 113.50136280059814.\n",
      "  Batch   600  of  2,080.    Elapsed: 121.74960494041443.\n",
      "  Batch   640  of  2,080.    Elapsed: 130.05624985694885.\n",
      "  Batch   680  of  2,080.    Elapsed: 138.2246584892273.\n",
      "  Batch   720  of  2,080.    Elapsed: 146.37899160385132.\n",
      "  Batch   760  of  2,080.    Elapsed: 154.48185515403748.\n",
      "  Batch   800  of  2,080.    Elapsed: 162.68277859687805.\n",
      "  Batch   840  of  2,080.    Elapsed: 170.91719722747803.\n",
      "  Batch   880  of  2,080.    Elapsed: 178.82092666625977.\n",
      "  Batch   920  of  2,080.    Elapsed: 186.58481168746948.\n",
      "  Batch   960  of  2,080.    Elapsed: 194.20537900924683.\n",
      "  Batch 1,000  of  2,080.    Elapsed: 201.74464106559753.\n",
      "  Batch 1,040  of  2,080.    Elapsed: 209.28844571113586.\n",
      "  Batch 1,080  of  2,080.    Elapsed: 216.8612926006317.\n",
      "  Batch 1,120  of  2,080.    Elapsed: 224.45729565620422.\n",
      "  Batch 1,160  of  2,080.    Elapsed: 232.0677673816681.\n",
      "  Batch 1,200  of  2,080.    Elapsed: 239.77210521697998.\n",
      "  Batch 1,240  of  2,080.    Elapsed: 247.52675938606262.\n",
      "  Batch 1,280  of  2,080.    Elapsed: 255.43965768814087.\n",
      "  Batch 1,320  of  2,080.    Elapsed: 263.37871527671814.\n",
      "  Batch 1,360  of  2,080.    Elapsed: 271.3884084224701.\n",
      "  Batch 1,400  of  2,080.    Elapsed: 279.3641493320465.\n",
      "  Batch 1,440  of  2,080.    Elapsed: 287.40744042396545.\n",
      "  Batch 1,480  of  2,080.    Elapsed: 295.4783227443695.\n",
      "  Batch 1,520  of  2,080.    Elapsed: 303.6190974712372.\n",
      "  Batch 1,560  of  2,080.    Elapsed: 311.9522273540497.\n",
      "  Batch 1,600  of  2,080.    Elapsed: 320.00246238708496.\n",
      "  Batch 1,640  of  2,080.    Elapsed: 328.1930260658264.\n",
      "  Batch 1,680  of  2,080.    Elapsed: 336.3459463119507.\n",
      "  Batch 1,720  of  2,080.    Elapsed: 344.62898874282837.\n",
      "  Batch 1,760  of  2,080.    Elapsed: 352.80913829803467.\n",
      "  Batch 1,800  of  2,080.    Elapsed: 361.0504961013794.\n",
      "  Batch 1,840  of  2,080.    Elapsed: 369.42343974113464.\n",
      "  Batch 1,880  of  2,080.    Elapsed: 377.7440872192383.\n",
      "  Batch 1,920  of  2,080.    Elapsed: 385.9182195663452.\n",
      "  Batch 1,960  of  2,080.    Elapsed: 394.08615589141846.\n",
      "  Batch 2,000  of  2,080.    Elapsed: 402.2972891330719.\n",
      "  Batch 2,040  of  2,080.    Elapsed: 410.4680423736572.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoc h took: 418.65809392929077\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99832\n",
      "  F1: 0.98872\n",
      "  Validation Loss: 0.02278\n",
      "  Validation took: 41.20144200325012\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 5 -----\n",
      "  Batch    40  of  2,080.    Elapsed: 8.1109037399292.\n",
      "  Batch    80  of  2,080.    Elapsed: 16.235691785812378.\n",
      "  Batch   120  of  2,080.    Elapsed: 24.343839168548584.\n",
      "  Batch   160  of  2,080.    Elapsed: 32.362966537475586.\n",
      "  Batch   200  of  2,080.    Elapsed: 40.380187034606934.\n",
      "  Batch   240  of  2,080.    Elapsed: 48.43663239479065.\n",
      "  Batch   280  of  2,080.    Elapsed: 56.47460126876831.\n",
      "  Batch   320  of  2,080.    Elapsed: 64.47854566574097.\n",
      "  Batch   360  of  2,080.    Elapsed: 72.49240350723267.\n",
      "  Batch   400  of  2,080.    Elapsed: 80.50635981559753.\n",
      "  Batch   440  of  2,080.    Elapsed: 88.50810241699219.\n",
      "  Batch   480  of  2,080.    Elapsed: 96.47233080863953.\n",
      "  Batch   520  of  2,080.    Elapsed: 104.56666278839111.\n",
      "  Batch   560  of  2,080.    Elapsed: 112.56584453582764.\n",
      "  Batch   600  of  2,080.    Elapsed: 120.62103796005249.\n",
      "  Batch   640  of  2,080.    Elapsed: 128.68839836120605.\n",
      "  Batch   680  of  2,080.    Elapsed: 136.70054483413696.\n",
      "  Batch   720  of  2,080.    Elapsed: 144.62645649909973.\n",
      "  Batch   760  of  2,080.    Elapsed: 152.52691435813904.\n",
      "  Batch   800  of  2,080.    Elapsed: 160.50618886947632.\n",
      "  Batch   840  of  2,080.    Elapsed: 168.47232246398926.\n",
      "  Batch   880  of  2,080.    Elapsed: 176.43914937973022.\n",
      "  Batch   920  of  2,080.    Elapsed: 184.43154668807983.\n",
      "  Batch   960  of  2,080.    Elapsed: 192.4138855934143.\n",
      "  Batch 1,000  of  2,080.    Elapsed: 200.42626881599426.\n",
      "  Batch 1,040  of  2,080.    Elapsed: 208.47753977775574.\n",
      "  Batch 1,080  of  2,080.    Elapsed: 216.44716048240662.\n",
      "  Batch 1,120  of  2,080.    Elapsed: 224.56021094322205.\n",
      "  Batch 1,160  of  2,080.    Elapsed: 232.7428333759308.\n",
      "  Batch 1,200  of  2,080.    Elapsed: 240.8056185245514.\n",
      "  Batch 1,240  of  2,080.    Elapsed: 248.82204794883728.\n",
      "  Batch 1,280  of  2,080.    Elapsed: 256.9573862552643.\n",
      "  Batch 1,320  of  2,080.    Elapsed: 265.0982539653778.\n",
      "  Batch 1,360  of  2,080.    Elapsed: 273.1464078426361.\n",
      "  Batch 1,400  of  2,080.    Elapsed: 281.1478519439697.\n",
      "  Batch 1,440  of  2,080.    Elapsed: 289.2017385959625.\n",
      "  Batch 1,480  of  2,080.    Elapsed: 297.28459668159485.\n",
      "  Batch 1,520  of  2,080.    Elapsed: 305.33336353302.\n",
      "  Batch 1,560  of  2,080.    Elapsed: 313.3780040740967.\n",
      "  Batch 1,600  of  2,080.    Elapsed: 321.4665856361389.\n",
      "  Batch 1,640  of  2,080.    Elapsed: 329.60924434661865.\n",
      "  Batch 1,680  of  2,080.    Elapsed: 337.66364884376526.\n",
      "  Batch 1,720  of  2,080.    Elapsed: 345.8579959869385.\n",
      "  Batch 1,760  of  2,080.    Elapsed: 353.94080901145935.\n",
      "  Batch 1,800  of  2,080.    Elapsed: 362.024906873703.\n",
      "  Batch 1,840  of  2,080.    Elapsed: 370.12746119499207.\n",
      "  Batch 1,880  of  2,080.    Elapsed: 378.16103076934814.\n",
      "  Batch 1,920  of  2,080.    Elapsed: 386.212055683136.\n",
      "  Batch 1,960  of  2,080.    Elapsed: 394.22684383392334.\n",
      "  Batch 2,000  of  2,080.    Elapsed: 402.26875472068787.\n",
      "  Batch 2,040  of  2,080.    Elapsed: 410.2809143066406.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoc h took: 418.2724952697754\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/home/marneusz/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99832\n",
      "  F1: 0.98872\n",
      "  Validation Loss: 0.02389\n",
      "  Validation took: 41.05749845504761\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
