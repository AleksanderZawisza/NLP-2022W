{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0e3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e166dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26509ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['won',\n",
       " 'yourselves',\n",
       " \"don't\",\n",
       " \"that'll\",\n",
       " \"you'd\",\n",
       " 'theirs',\n",
       " 'doesn',\n",
       " \"needn't\",\n",
       " 'with',\n",
       " 'that']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "random.sample(stopwords.words('english'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac03f72",
   "metadata": {},
   "source": [
    "Consider removing some stop words like _no_, _yes_, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449db28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"FakeNews\": \"\",\n",
    "    \"ISOT\": \"\"\n",
    "}\n",
    "\n",
    "CUR_DATASET = \"FakeNews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/train.csv.zip\")\n",
    "test_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb85e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = pd.concat([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd0a899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0.000000\n",
       "title     0.026827\n",
       "author    0.094087\n",
       "text      0.001875\n",
       "label     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.isnull().sum() / train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "743ed40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26000 entries, 0 to 5199\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      26000 non-null  int64  \n",
      " 1   title   25320 non-null  object \n",
      " 2   author  23540 non-null  object \n",
      " 3   text    25954 non-null  object \n",
      " 4   label   20800 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "whole_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8871882a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.fillna(\"null data\")\n",
    "test_dataset = test_dataset.fillna(\"null data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff247855",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc83d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44734eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33c92e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97fa597b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>house dem aide: even see comey's letter jason ...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>house dem aide: even see comey's letter jason ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flynn: hillary clinton, big woman campus - bre...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fired october 29, 2016 tension...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 civilians killed single us airstrike identi...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>videos 15 civilians killed single us airstrike...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  house dem aide: even see comey's letter jason ...       Darrell Lucus   \n",
       "1   1  flynn: hillary clinton, big woman campus - bre...     Daniel J. Flynn   \n",
       "2   2                              truth might get fired  Consortiumnews.com   \n",
       "3   3  15 civilians killed single us airstrike identi...     Jessica Purkiss   \n",
       "4   4  iranian woman jailed fictional unpublished sto...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide: even see comey's letter jason ...      1  \n",
       "1  ever get feeling life circles roundabout rathe...      0  \n",
       "2  truth might get fired october 29, 2016 tension...      1  \n",
       "3  videos 15 civilians killed single us airstrike...      1  \n",
       "4  print iranian woman sentenced six years prison...      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07c4fc",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', '', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697b466",
   "metadata": {},
   "source": [
    "Consider removing some of the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42c45835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it's\",\n",
       " 'there',\n",
       " 'the',\n",
       " 'why',\n",
       " 'yourselves',\n",
       " 'aren',\n",
       " 'further',\n",
       " 'few',\n",
       " 'isn',\n",
       " 'shan',\n",
       " 'because',\n",
       " 'their',\n",
       " 'will',\n",
       " 'ourselves',\n",
       " 'these',\n",
       " 'ain',\n",
       " 'out',\n",
       " 'its',\n",
       " 'whom',\n",
       " 'too']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = [preprocessing_text_fn[\"no_punctuation\"](word) for word in stop_words]\n",
    "random.sample(stop_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f7b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_stopwords(text, stop_words=STOP_WORDS):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sequence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "    return filtered_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>house dem aide even see comeys letter jason ch...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>house dem aide even see comeys letter jason ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flynn hillary clinton big woman campus breitbart</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>civilians killed single us airstrike identified</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>videos civilians killed single us airstrike id...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jackie mason hollywood would love trump bombed...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>trying times jackie mason voice reason in week...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>life life luxury elton johns favorite shark pi...</td>\n",
       "      <td>null data</td>\n",
       "      <td>ever wonder britains iconic pop pianist gets l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>benoît hamon wins french socialist partys pres...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>paris france chose idealistic traditional cand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>excerpts draft script donald trumps qampa blac...</td>\n",
       "      <td>null data</td>\n",
       "      <td>donaldtrump scheduled make highly anticipated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>backchannel plan ukraine russia courtesy trump...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>week michaelflynn resigned national security a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  house dem aide even see comeys letter jason ch...   \n",
       "1   1   flynn hillary clinton big woman campus breitbart   \n",
       "2   2                              truth might get fired   \n",
       "3   3    civilians killed single us airstrike identified   \n",
       "4   4  iranian woman jailed fictional unpublished sto...   \n",
       "5   5  jackie mason hollywood would love trump bombed...   \n",
       "6   6  life life luxury elton johns favorite shark pi...   \n",
       "7   7  benoît hamon wins french socialist partys pres...   \n",
       "8   8  excerpts draft script donald trumps qampa blac...   \n",
       "9   9  backchannel plan ukraine russia courtesy trump...   \n",
       "\n",
       "                         author  \\\n",
       "0                 Darrell Lucus   \n",
       "1               Daniel J. Flynn   \n",
       "2            Consortiumnews.com   \n",
       "3               Jessica Purkiss   \n",
       "4                Howard Portnoy   \n",
       "5               Daniel Nussbaum   \n",
       "6                     null data   \n",
       "7               Alissa J. Rubin   \n",
       "8                     null data   \n",
       "9  Megan Twohey and Scott Shane   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide even see comeys letter jason ch...      1  \n",
       "1  ever get feeling life circles roundabout rathe...      0  \n",
       "2  truth might get fired october tension intellig...      1  \n",
       "3  videos civilians killed single us airstrike id...      1  \n",
       "4  print iranian woman sentenced six years prison...      1  \n",
       "5  trying times jackie mason voice reason in week...      0  \n",
       "6  ever wonder britains iconic pop pianist gets l...      1  \n",
       "7  paris france chose idealistic traditional cand...      0  \n",
       "8  donaldtrump scheduled make highly anticipated ...      0  \n",
       "9  week michaelflynn resigned national security a...      0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"title\"] = train_dataset[\"title\"].apply(preprocess_text)\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(preprocess_text)\n",
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c49c3345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues if Not Purse ...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO Calif After years of scorning the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>NoDAPL Native American Leaders Vow to Stay All...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos NoDAPL Native American Leaders Vow to S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback This T...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you dont succeed trydifferent spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report Meme Wars E</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>mins ago Views Comments Likes For the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>Pelosi Calls for FBI Investigation to Find Out...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Sunday on NBCs Meet the Press House Minority L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>Weekly Featured Profile Randy Shannon</td>\n",
       "      <td>Trevor Loudon</td>\n",
       "      <td>You are here Home Articles of the Bound Weekly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>null data</td>\n",
       "      <td>cognitive dissident</td>\n",
       "      <td>dont we have the receipt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues if Not Purse ...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  NoDAPL Native American Leaders Vow to Stay All...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback This T...   \n",
       "4  20804                          Keiser Report Meme Wars E   \n",
       "5  20805  Trump is USAs antique hero Clinton will be nex...   \n",
       "6  20806  Pelosi Calls for FBI Investigation to Find Out...   \n",
       "7  20807              Weekly Featured Profile Randy Shannon   \n",
       "8  20808  Urban Population Booms Will Make Climate Chang...   \n",
       "9  20809                                          null data   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO Calif After years of scorning the po...  \n",
       "1                null data  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos NoDAPL Native American Leaders Vow to S...  \n",
       "3            Daniel Victor  If at first you dont succeed trydifferent spor...  \n",
       "4  Truth Broadcast Network   mins ago Views Comments Likes For the first t...  \n",
       "5                null data  Trump is USAs antique hero Clinton will be nex...  \n",
       "6                  Pam Key  Sunday on NBCs Meet the Press House Minority L...  \n",
       "7            Trevor Loudon  You are here Home Articles of the Bound Weekly...  \n",
       "8                null data  Urban Population Booms Will Make Climate Chang...  \n",
       "9      cognitive dissident                           dont we have the receipt  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"title\"] = test_dataset[\"title\"].apply(preprocess_text)\n",
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(preprocess_text)\n",
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b3215",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ad3d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "142687dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3f44060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[\"title\"] = train_dataset[\"title\"].apply(tokenize_without_stopwords)\n",
    "# train_dataset[\"text\"] = train_dataset[\"text\"].apply(tokenize_without_stopwords)\n",
    "# train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "574b00aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_dataset[\"title\"] = test_dataset[\"title\"].apply(tokenize_without_stopwords)\n",
    "# test_dataset[\"text\"] = test_dataset[\"text\"].apply(tokenize_without_stopwords)\n",
    "# test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb16b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text = train_dataset['text'].values\n",
    "train_text = (train_dataset['title'] + \" \" + train_dataset['text']).values\n",
    "test_text = (test_dataset['title'] + \" \" + test_dataset['text']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6899268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a6a97",
   "metadata": {},
   "source": [
    "In the Kaggle competition the best scores were obtained by using only 'author' and 'title' features. Let's take a look if it's possible to train BERT using text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp-transformers/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7676622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  house dem aide even see comeys letter jason chaffetz tweeted house dem aide even see comeys letter jason chaffetz tweeted darrell lucus october subscribe jason chaffetz stump american fork utah image courtesy michael jolley available creative commonsby license apology keith olbermann doubt worst person world weekfbi director james comey according house democratic aide look like also know secondworst person well turn comey sent nowinfamous letter announcing fbi looking email may related hillary clinton email server ranking democrat relevant committee hear comey found via tweet one republican committee chairman know comey notified republican chairman democratic ranking member house intelligence judiciary oversight committee agency reviewing email recently discovered order see contained classified information long letter went out oversight committee chairman jason chaffetz set political world ablaze tweet fbi dir informed me the fbi learned existence email appear pertinent investigation case reopened jason chaffetz jasoninthehouse october course know case comey actually saying reviewing email light an unrelated casewhich know anthony weiners sexting teenager apparently little thing fact matter chaffetz utah republican already vowed initiate raft investigation hillary winsat least two year worth possibly entire term worth them apparently chaffetz thought fbi already work himresulting tweet briefly roiled nation cooler head realized dud according senior house democratic aide misreading letter may least chaffetz sin aide told shareblue bos democrat even know comeys letter timeand found checked twitter democratic ranking member relevant committee receive comeys letter republican chairman fact democratic ranking member didn receive chairman oversight government reform committee jason chaffetz tweeted made public let see weve got right fbi director tell chaffetz gop committee chairman major development potentially politically explosive investigation neither chaffetz colleague courtesy let democratic counterpart know it instead according aide made find twitter already talk daily ko comey provided advance notice letter chaffetz republican giving time turn spin machine may make good theater nothing far even suggests case all nothing far suggests comey anything grossly incompetent tonedeaf suggest however chaffetz acting way make dan burton darrell issa look like model responsibility bipartisanship even decency notify ranking member elijah cummings something explosive trample basic standard fairness know doe granted likely chaffetz answer this sits ridiculously republican district anchored provo orem cook partisan voting indexgave mitt romney punishing percent vote moreover republican house leadership given full support chaffetz planned fishing expedition mean cant turn hot light him all textbook example house become republican control also second worst person world darrell lucus darrell something graduate university north carolina considers journalist old school attempt turn member religious right college succeeded turning religious right worst nightmarea charismatic christian unapologetic liberal desire stand scared silence increased survived abusive threeyear marriage may know daily ko christian dem nc follow twitter darrelllucus connect facebook click buy darrell mello yello connect\n",
      "Tokenized:  ['house', 'dem', 'aide', 'even', 'see', 'come', '##ys', 'letter', 'jason', 'cha', '##ffe', '##tz', 't', '##wee', '##ted', 'house', 'dem', 'aide', 'even', 'see', 'come', '##ys', 'letter', 'jason', 'cha', '##ffe', '##tz', 't', '##wee', '##ted', 'darrell', 'luc', '##us', 'october', 'sub', '##scribe', 'jason', 'cha', '##ffe', '##tz', 'stump', 'american', 'fork', 'utah', 'image', 'courtesy', 'michael', 'jo', '##lley', 'available', 'creative', 'commons', '##by', 'license', 'apology', 'keith', 'ol', '##berman', '##n', 'doubt', 'worst', 'person', 'world', 'week', '##fb', '##i', 'director', 'james', 'come', '##y', 'according', 'house', 'democratic', 'aide', 'look', 'like', 'also', 'know', 'second', '##wo', '##rst', 'person', 'well', 'turn', 'come', '##y', 'sent', 'now', '##in', '##fa', '##mous', 'letter', 'announcing', 'fbi', 'looking', 'email', 'may', 'related', 'hillary', 'clinton', 'email', 'server', 'ranking', 'democrat', 'relevant', 'committee', 'hear', 'come', '##y', 'found', 'via', 't', '##wee', '##t', 'one', 'republican', 'committee', 'chairman', 'know', 'come', '##y', 'notified', 'republican', 'chairman', 'democratic', 'ranking', 'member', 'house', 'intelligence', 'judiciary', 'oversight', 'committee', 'agency', 'reviewing', 'email', 'recently', 'discovered', 'order', 'see', 'contained', 'classified', 'information', 'long', 'letter', 'went', 'out', 'oversight', 'committee', 'chairman', 'jason', 'cha', '##ffe', '##tz', 'set', 'political', 'world', 'ab', '##laze', 't', '##wee', '##t', 'fbi', 'dir', 'informed', 'me', 'the', 'fbi', 'learned', 'existence', 'email', 'appear', 'per', '##tine', '##nt', 'investigation', 'case', 'reopened', 'jason', 'cha', '##ffe', '##tz', 'jason', '##int', '##he', '##house', 'october', 'course', 'know', 'case', 'come', '##y', 'actually', 'saying', 'reviewing', 'email', 'light', 'an', 'unrelated', 'case', '##w', '##hic', '##h', 'know', 'anthony', 'wei', '##ners', 'sex', '##ting', 'teenager', 'apparently', 'little', 'thing', 'fact', 'matter', 'cha', '##ffe', '##tz', 'utah', 'republican', 'already', 'vowed', 'initiate', 'raft', 'investigation', 'hillary', 'wins', '##at', 'least', 'two', 'year', 'worth', 'possibly', 'entire', 'term', 'worth', 'them', 'apparently', 'cha', '##ffe', '##tz', 'thought', 'fbi', 'already', 'work', 'him', '##res', '##ult', '##ing', 't', '##wee', '##t', 'briefly', 'roi', '##led', 'nation', 'cooler', 'head', 'realized', 'du', '##d', 'according', 'senior', 'house', 'democratic', 'aide', 'mis', '##rea', '##ding', 'letter', 'may', 'least', 'cha', '##ffe', '##tz', 'sin', 'aide', 'told', 'share', '##bl', '##ue', 'bo', '##s', 'democrat', 'even', 'know', 'come', '##ys', 'letter', 'time', '##and', 'found', 'checked', 'twitter', 'democratic', 'ranking', 'member', 'relevant', 'committee', 'receive', 'come', '##ys', 'letter', 'republican', 'chairman', 'fact', 'democratic', 'ranking', 'member', 'didn', 'receive', 'chairman', 'oversight', 'government', 'reform', 'committee', 'jason', 'cha', '##ffe', '##tz', 't', '##wee', '##ted', 'made', 'public', 'let', 'see', 'we', '##ve', 'got', 'right', 'fbi', 'director', 'tell', 'cha', '##ffe', '##tz', 'go', '##p', 'committee', 'chairman', 'major', 'development', 'potentially', 'politically', 'explosive', 'investigation', 'neither', 'cha', '##ffe', '##tz', 'colleague', 'courtesy', 'let', 'democratic', 'counterpart', 'know', 'it', 'instead', 'according', 'aide', 'made', 'find', 'twitter', 'already', 'talk', 'daily', 'ko', 'come', '##y', 'provided', 'advance', 'notice', 'letter', 'cha', '##ffe', '##tz', 'republican', 'giving', 'time', 'turn', 'spin', 'machine', 'may', 'make', 'good', 'theater', 'nothing', 'far', 'even', 'suggests', 'case', 'all', 'nothing', 'far', 'suggests', 'come', '##y', 'anything', 'gross', '##ly', 'inc', '##omp', '##ete', '##nt', 'toned', '##ea', '##f', 'suggest', 'however', 'cha', '##ffe', '##tz', 'acting', 'way', 'make', 'dan', 'burton', 'darrell', 'iss', '##a', 'look', 'like', 'model', 'responsibility', 'bi', '##partisan', '##ship', 'even', 'dec', '##ency', 'not', '##ify', 'ranking', 'member', 'elijah', 'cummings', 'something', 'explosive', 'tram', '##ple', 'basic', 'standard', 'fairness', 'know', 'doe', 'granted', 'likely', 'cha', '##ffe', '##tz', 'answer', 'this', 'sits', 'ridiculous', '##ly', 'republican', 'district', 'anchored', 'pro', '##vo', 'ore', '##m', 'cook', 'partisan', 'voting', 'index', '##ga', '##ve', 'mit', '##t', 'romney', 'punish', '##ing', 'percent', 'vote', 'moreover', 'republican', 'house', 'leadership', 'given', 'full', 'support', 'cha', '##ffe', '##tz', 'planned', 'fishing', 'expedition', 'mean', 'can', '##t', 'turn', 'hot', 'light', 'him', 'all', 'textbook', 'example', 'house', 'become', 'republican', 'control', 'also', 'second', 'worst', 'person', 'world', 'darrell', 'luc', '##us', 'darrell', 'something', 'graduate', 'university', 'north', 'carolina', 'considers', 'journalist', 'old', 'school', 'attempt', 'turn', 'member', 'religious', 'right', 'college', 'succeeded', 'turning', 'religious', 'right', 'worst', 'nightmare', '##a', 'charismatic', 'christian', 'una', '##pol', '##oge', '##tic', 'liberal', 'desire', 'stand', 'scared', 'silence', 'increased', 'survived', 'abusive', 'three', '##year', 'marriage', 'may', 'know', 'daily', 'ko', 'christian', 'dem', 'nc', 'follow', 'twitter', 'darrell', '##lu', '##cus', 'connect', 'facebook', 'click', 'buy', 'darrell', 'mel', '##lo', 'yell', '##o', 'connect']\n",
      "Token IDs:  [2160, 17183, 14895, 2130, 2156, 2272, 7274, 3661, 4463, 15775, 16020, 5753, 1056, 28394, 3064, 2160, 17183, 14895, 2130, 2156, 2272, 7274, 3661, 4463, 15775, 16020, 5753, 1056, 28394, 3064, 23158, 12776, 2271, 2255, 4942, 29234, 4463, 15775, 16020, 5753, 22475, 2137, 9292, 6646, 3746, 14571, 2745, 8183, 25105, 2800, 5541, 7674, 3762, 6105, 12480, 6766, 19330, 23991, 2078, 4797, 5409, 2711, 2088, 2733, 26337, 2072, 2472, 2508, 2272, 2100, 2429, 2160, 3537, 14895, 2298, 2066, 2036, 2113, 2117, 12155, 12096, 2711, 2092, 2735, 2272, 2100, 2741, 2085, 2378, 7011, 27711, 3661, 13856, 8495, 2559, 10373, 2089, 3141, 18520, 7207, 10373, 8241, 5464, 7672, 7882, 2837, 2963, 2272, 2100, 2179, 3081, 1056, 28394, 2102, 2028, 3951, 2837, 3472, 2113, 2272, 2100, 19488, 3951, 3472, 3537, 5464, 2266, 2160, 4454, 14814, 15709, 2837, 4034, 15252, 10373, 3728, 3603, 2344, 2156, 4838, 6219, 2592, 2146, 3661, 2253, 2041, 15709, 2837, 3472, 4463, 15775, 16020, 5753, 2275, 2576, 2088, 11113, 24472, 1056, 28394, 2102, 8495, 16101, 6727, 2033, 1996, 8495, 4342, 4598, 10373, 3711, 2566, 10196, 3372, 4812, 2553, 11882, 4463, 15775, 16020, 5753, 4463, 18447, 5369, 4580, 2255, 2607, 2113, 2553, 2272, 2100, 2941, 3038, 15252, 10373, 2422, 2019, 15142, 2553, 2860, 16066, 2232, 2113, 4938, 11417, 16912, 3348, 3436, 10563, 4593, 2210, 2518, 2755, 3043, 15775, 16020, 5753, 6646, 3951, 2525, 18152, 17820, 21298, 4812, 18520, 5222, 4017, 2560, 2048, 2095, 4276, 4298, 2972, 2744, 4276, 2068, 4593, 15775, 16020, 5753, 2245, 8495, 2525, 2147, 2032, 6072, 11314, 2075, 1056, 28394, 2102, 4780, 25223, 3709, 3842, 14976, 2132, 3651, 4241, 2094, 2429, 3026, 2160, 3537, 14895, 28616, 16416, 4667, 3661, 2089, 2560, 15775, 16020, 5753, 8254, 14895, 2409, 3745, 16558, 5657, 8945, 2015, 7672, 2130, 2113, 2272, 7274, 3661, 2051, 5685, 2179, 7039, 10474, 3537, 5464, 2266, 7882, 2837, 4374, 2272, 7274, 3661, 3951, 3472, 2755, 3537, 5464, 2266, 2134, 4374, 3472, 15709, 2231, 5290, 2837, 4463, 15775, 16020, 5753, 1056, 28394, 3064, 2081, 2270, 2292, 2156, 2057, 3726, 2288, 2157, 8495, 2472, 2425, 15775, 16020, 5753, 2175, 2361, 2837, 3472, 2350, 2458, 9280, 10317, 11355, 4812, 4445, 15775, 16020, 5753, 11729, 14571, 2292, 3537, 13637, 2113, 2009, 2612, 2429, 14895, 2081, 2424, 10474, 2525, 2831, 3679, 12849, 2272, 2100, 3024, 5083, 5060, 3661, 15775, 16020, 5753, 3951, 3228, 2051, 2735, 6714, 3698, 2089, 2191, 2204, 4258, 2498, 2521, 2130, 6083, 2553, 2035, 2498, 2521, 6083, 2272, 2100, 2505, 7977, 2135, 4297, 25377, 12870, 3372, 27604, 5243, 2546, 6592, 2174, 15775, 16020, 5753, 3772, 2126, 2191, 4907, 9658, 23158, 26354, 2050, 2298, 2066, 2944, 5368, 12170, 26053, 9650, 2130, 11703, 11916, 2025, 8757, 5464, 2266, 14063, 20750, 2242, 11355, 12517, 10814, 3937, 3115, 26935, 2113, 18629, 4379, 3497, 15775, 16020, 5753, 3437, 2023, 7719, 9951, 2135, 3951, 2212, 14453, 4013, 6767, 10848, 2213, 5660, 14254, 6830, 5950, 3654, 3726, 10210, 2102, 19615, 16385, 2075, 3867, 3789, 9308, 3951, 2160, 4105, 2445, 2440, 2490, 15775, 16020, 5753, 3740, 5645, 5590, 2812, 2064, 2102, 2735, 2980, 2422, 2032, 2035, 16432, 2742, 2160, 2468, 3951, 2491, 2036, 2117, 5409, 2711, 2088, 23158, 12776, 2271, 23158, 2242, 4619, 2118, 2167, 3792, 10592, 4988, 2214, 2082, 3535, 2735, 2266, 3412, 2157, 2267, 4594, 3810, 3412, 2157, 5409, 10103, 2050, 23916, 3017, 14477, 18155, 23884, 4588, 4314, 4792, 3233, 6015, 4223, 3445, 5175, 20676, 2093, 29100, 3510, 2089, 2113, 3679, 12849, 3017, 17183, 13316, 3582, 10474, 23158, 7630, 7874, 7532, 9130, 11562, 4965, 23158, 11463, 4135, 14315, 2080, 7532]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_text[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_text[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615e018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/20800 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20800/20800 [01:18<00:00, 265.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  113463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f54957d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20799,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe124ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800,), (20799,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, train_text_filtered.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20799/20799 [01:21<00:00, 254.28it/s]\n",
      "/tmp/ipykernel_25940/2816128611.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels_filtered = tokenize_map(train_text_filtered, labels_filtered)\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e778bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1496620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb24c116850>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "312f41ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db120363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20800]), torch.Size([20799, 512]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1952025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20799])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30770dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_size, val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 5 -----\n",
      "  Batch    50  of  4,160.    Elapsed: 18.190900802612305.\n",
      "  Batch   100  of  4,160.    Elapsed: 35.57329249382019.\n",
      "  Batch   150  of  4,160.    Elapsed: 52.82647728919983.\n",
      "  Batch   200  of  4,160.    Elapsed: 70.12784576416016.\n",
      "  Batch   250  of  4,160.    Elapsed: 87.47257828712463.\n",
      "  Batch   300  of  4,160.    Elapsed: 104.73115277290344.\n",
      "  Batch   350  of  4,160.    Elapsed: 122.05042028427124.\n",
      "  Batch   400  of  4,160.    Elapsed: 139.36284804344177.\n",
      "  Batch   450  of  4,160.    Elapsed: 156.6610827445984.\n",
      "  Batch   500  of  4,160.    Elapsed: 174.0032639503479.\n",
      "  Batch   550  of  4,160.    Elapsed: 191.36982226371765.\n",
      "  Batch   600  of  4,160.    Elapsed: 208.6692774295807.\n",
      "  Batch   650  of  4,160.    Elapsed: 226.00262069702148.\n",
      "  Batch   700  of  4,160.    Elapsed: 243.2904953956604.\n",
      "  Batch   750  of  4,160.    Elapsed: 260.5849645137787.\n",
      "  Batch   800  of  4,160.    Elapsed: 277.8481729030609.\n",
      "  Batch   850  of  4,160.    Elapsed: 295.21899127960205.\n",
      "  Batch   900  of  4,160.    Elapsed: 312.56657218933105.\n",
      "  Batch   950  of  4,160.    Elapsed: 329.88375902175903.\n",
      "  Batch 1,000  of  4,160.    Elapsed: 347.2237994670868.\n",
      "  Batch 1,050  of  4,160.    Elapsed: 364.56956219673157.\n",
      "  Batch 1,100  of  4,160.    Elapsed: 381.9367082118988.\n",
      "  Batch 1,150  of  4,160.    Elapsed: 399.2859511375427.\n",
      "  Batch 1,200  of  4,160.    Elapsed: 416.64591693878174.\n",
      "  Batch 1,250  of  4,160.    Elapsed: 433.94490671157837.\n",
      "  Batch 1,300  of  4,160.    Elapsed: 451.2131652832031.\n",
      "  Batch 1,350  of  4,160.    Elapsed: 468.4757251739502.\n",
      "  Batch 1,400  of  4,160.    Elapsed: 485.7847533226013.\n",
      "  Batch 1,450  of  4,160.    Elapsed: 503.65735697746277.\n",
      "  Batch 1,500  of  4,160.    Elapsed: 521.36945104599.\n",
      "  Batch 1,550  of  4,160.    Elapsed: 538.7344715595245.\n",
      "  Batch 1,600  of  4,160.    Elapsed: 556.0295889377594.\n",
      "  Batch 1,650  of  4,160.    Elapsed: 573.3762183189392.\n",
      "  Batch 1,700  of  4,160.    Elapsed: 590.7145843505859.\n",
      "  Batch 1,750  of  4,160.    Elapsed: 608.0274195671082.\n",
      "  Batch 1,800  of  4,160.    Elapsed: 625.3932554721832.\n",
      "  Batch 1,850  of  4,160.    Elapsed: 642.7072019577026.\n",
      "  Batch 1,900  of  4,160.    Elapsed: 659.9689583778381.\n",
      "  Batch 1,950  of  4,160.    Elapsed: 677.3152945041656.\n",
      "  Batch 2,000  of  4,160.    Elapsed: 694.5991296768188.\n",
      "  Batch 2,050  of  4,160.    Elapsed: 711.9537448883057.\n",
      "  Batch 2,100  of  4,160.    Elapsed: 729.2821981906891.\n",
      "  Batch 2,150  of  4,160.    Elapsed: 746.5591447353363.\n",
      "  Batch 2,200  of  4,160.    Elapsed: 763.8204517364502.\n",
      "  Batch 2,250  of  4,160.    Elapsed: 781.1874010562897.\n",
      "  Batch 2,300  of  4,160.    Elapsed: 798.528046131134.\n",
      "  Batch 2,350  of  4,160.    Elapsed: 815.8527896404266.\n",
      "  Batch 2,400  of  4,160.    Elapsed: 833.1488864421844.\n",
      "  Batch 2,450  of  4,160.    Elapsed: 850.469012260437.\n",
      "  Batch 2,500  of  4,160.    Elapsed: 867.8124861717224.\n",
      "  Batch 2,550  of  4,160.    Elapsed: 885.1595582962036.\n",
      "  Batch 2,600  of  4,160.    Elapsed: 902.5940313339233.\n",
      "  Batch 2,650  of  4,160.    Elapsed: 919.9934668540955.\n",
      "  Batch 2,700  of  4,160.    Elapsed: 937.3692014217377.\n",
      "  Batch 2,750  of  4,160.    Elapsed: 954.6677651405334.\n",
      "  Batch 2,800  of  4,160.    Elapsed: 971.9427011013031.\n",
      "  Batch 2,850  of  4,160.    Elapsed: 989.2453105449677.\n",
      "  Batch 2,900  of  4,160.    Elapsed: 1006.548045873642.\n",
      "  Batch 2,950  of  4,160.    Elapsed: 1023.9534902572632.\n",
      "  Batch 3,000  of  4,160.    Elapsed: 1041.298840522766.\n",
      "  Batch 3,050  of  4,160.    Elapsed: 1058.6023771762848.\n",
      "  Batch 3,100  of  4,160.    Elapsed: 1075.8823034763336.\n",
      "  Batch 3,150  of  4,160.    Elapsed: 1093.1838188171387.\n",
      "  Batch 3,200  of  4,160.    Elapsed: 1110.4532008171082.\n",
      "  Batch 3,250  of  4,160.    Elapsed: 1127.7415263652802.\n",
      "  Batch 3,300  of  4,160.    Elapsed: 1145.0989983081818.\n",
      "  Batch 3,350  of  4,160.    Elapsed: 1162.4061472415924.\n",
      "  Batch 3,400  of  4,160.    Elapsed: 1179.7514383792877.\n",
      "  Batch 3,450  of  4,160.    Elapsed: 1197.0313847064972.\n",
      "  Batch 3,500  of  4,160.    Elapsed: 1214.334822177887.\n",
      "  Batch 3,550  of  4,160.    Elapsed: 1231.627904176712.\n",
      "  Batch 3,600  of  4,160.    Elapsed: 1248.9120275974274.\n",
      "  Batch 3,650  of  4,160.    Elapsed: 1266.2421910762787.\n",
      "  Batch 3,700  of  4,160.    Elapsed: 1283.5205965042114.\n",
      "  Batch 3,750  of  4,160.    Elapsed: 1300.8021676540375.\n",
      "  Batch 3,800  of  4,160.    Elapsed: 1318.092221736908.\n",
      "  Batch 3,850  of  4,160.    Elapsed: 1335.3836123943329.\n",
      "  Batch 3,900  of  4,160.    Elapsed: 1352.6833794116974.\n",
      "  Batch 3,950  of  4,160.    Elapsed: 1370.2783818244934.\n",
      "  Batch 4,000  of  4,160.    Elapsed: 1387.6747872829437.\n",
      "  Batch 4,050  of  4,160.    Elapsed: 1404.9506204128265.\n",
      "  Batch 4,100  of  4,160.    Elapsed: 1422.2666919231415.\n",
      "  Batch 4,150  of  4,160.    Elapsed: 1439.5758283138275.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoc h took: 1442.9923424720764\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98269\n",
      "  F1: 0.92962\n",
      "  Validation Loss: 0.09254\n",
      "  Validation took: 153.36661767959595\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 5 -----\n",
      "  Batch    50  of  4,160.    Elapsed: 17.341869831085205.\n",
      "  Batch   100  of  4,160.    Elapsed: 34.65344977378845.\n",
      "  Batch   150  of  4,160.    Elapsed: 52.01434326171875.\n",
      "  Batch   200  of  4,160.    Elapsed: 69.35331225395203.\n",
      "  Batch   250  of  4,160.    Elapsed: 86.68552374839783.\n",
      "  Batch   300  of  4,160.    Elapsed: 104.02633786201477.\n",
      "  Batch   350  of  4,160.    Elapsed: 121.37660932540894.\n",
      "  Batch   400  of  4,160.    Elapsed: 138.8132140636444.\n",
      "  Batch   450  of  4,160.    Elapsed: 156.217298746109.\n",
      "  Batch   500  of  4,160.    Elapsed: 173.57943201065063.\n",
      "  Batch   550  of  4,160.    Elapsed: 190.95828342437744.\n",
      "  Batch   600  of  4,160.    Elapsed: 208.26195740699768.\n",
      "  Batch   650  of  4,160.    Elapsed: 226.68833804130554.\n",
      "  Batch   700  of  4,160.    Elapsed: 244.50017142295837.\n",
      "  Batch   750  of  4,160.    Elapsed: 262.24450039863586.\n",
      "  Batch   800  of  4,160.    Elapsed: 280.46282625198364.\n",
      "  Batch   850  of  4,160.    Elapsed: 297.7915370464325.\n",
      "  Batch   900  of  4,160.    Elapsed: 315.3269393444061.\n",
      "  Batch   950  of  4,160.    Elapsed: 333.46546626091003.\n",
      "  Batch 1,000  of  4,160.    Elapsed: 351.82468271255493.\n",
      "  Batch 1,050  of  4,160.    Elapsed: 369.76382780075073.\n",
      "  Batch 1,100  of  4,160.    Elapsed: 387.83319902420044.\n",
      "  Batch 1,150  of  4,160.    Elapsed: 405.7549822330475.\n",
      "  Batch 1,200  of  4,160.    Elapsed: 423.241317987442.\n",
      "  Batch 1,250  of  4,160.    Elapsed: 443.0942780971527.\n",
      "  Batch 1,300  of  4,160.    Elapsed: 463.5297977924347.\n",
      "  Batch 1,350  of  4,160.    Elapsed: 481.5717406272888.\n",
      "  Batch 1,400  of  4,160.    Elapsed: 501.2195188999176.\n",
      "  Batch 1,450  of  4,160.    Elapsed: 521.1550424098969.\n",
      "  Batch 1,500  of  4,160.    Elapsed: 540.4112870693207.\n",
      "  Batch 1,550  of  4,160.    Elapsed: 559.0805835723877.\n",
      "  Batch 1,600  of  4,160.    Elapsed: 577.6132254600525.\n",
      "  Batch 1,650  of  4,160.    Elapsed: 595.426869392395.\n",
      "  Batch 1,700  of  4,160.    Elapsed: 613.847629070282.\n",
      "  Batch 1,750  of  4,160.    Elapsed: 633.5179018974304.\n",
      "  Batch 1,800  of  4,160.    Elapsed: 651.6020114421844.\n",
      "  Batch 1,850  of  4,160.    Elapsed: 671.4079978466034.\n",
      "  Batch 1,900  of  4,160.    Elapsed: 689.4765729904175.\n",
      "  Batch 1,950  of  4,160.    Elapsed: 707.2616276741028.\n",
      "  Batch 2,000  of  4,160.    Elapsed: 725.1947455406189.\n",
      "  Batch 2,050  of  4,160.    Elapsed: 742.9377286434174.\n",
      "  Batch 2,100  of  4,160.    Elapsed: 763.0775456428528.\n",
      "  Batch 2,150  of  4,160.    Elapsed: 783.354691028595.\n",
      "  Batch 2,200  of  4,160.    Elapsed: 802.4651696681976.\n",
      "  Batch 2,250  of  4,160.    Elapsed: 820.2327265739441.\n",
      "  Batch 2,300  of  4,160.    Elapsed: 838.5763640403748.\n",
      "  Batch 2,350  of  4,160.    Elapsed: 856.5018010139465.\n",
      "  Batch 2,400  of  4,160.    Elapsed: 876.0456712245941.\n",
      "  Batch 2,450  of  4,160.    Elapsed: 895.634548664093.\n",
      "  Batch 2,500  of  4,160.    Elapsed: 915.3741755485535.\n",
      "  Batch 2,550  of  4,160.    Elapsed: 935.217945098877.\n",
      "  Batch 2,600  of  4,160.    Elapsed: 954.8430726528168.\n",
      "  Batch 2,650  of  4,160.    Elapsed: 974.3141646385193.\n",
      "  Batch 2,700  of  4,160.    Elapsed: 992.6986825466156.\n",
      "  Batch 2,750  of  4,160.    Elapsed: 1010.5586161613464.\n",
      "  Batch 2,800  of  4,160.    Elapsed: 1030.9341433048248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,850  of  4,160.    Elapsed: 1049.5795638561249.\n",
      "  Batch 2,900  of  4,160.    Elapsed: 1067.3617701530457.\n",
      "  Batch 2,950  of  4,160.    Elapsed: 1085.8282976150513.\n",
      "  Batch 3,000  of  4,160.    Elapsed: 1105.411389350891.\n",
      "  Batch 3,050  of  4,160.    Elapsed: 1123.8294179439545.\n",
      "  Batch 3,100  of  4,160.    Elapsed: 1142.4479703903198.\n",
      "  Batch 3,150  of  4,160.    Elapsed: 1161.8522758483887.\n",
      "  Batch 3,200  of  4,160.    Elapsed: 1179.780639886856.\n",
      "  Batch 3,250  of  4,160.    Elapsed: 1197.7894141674042.\n",
      "  Batch 3,300  of  4,160.    Elapsed: 1217.3441450595856.\n",
      "  Batch 3,350  of  4,160.    Elapsed: 1236.9851546287537.\n",
      "  Batch 3,400  of  4,160.    Elapsed: 1256.1564092636108.\n",
      "  Batch 3,450  of  4,160.    Elapsed: 1275.4608218669891.\n",
      "  Batch 3,500  of  4,160.    Elapsed: 1293.1530985832214.\n",
      "  Batch 3,550  of  4,160.    Elapsed: 1310.5729389190674.\n",
      "  Batch 3,600  of  4,160.    Elapsed: 1328.1260006427765.\n",
      "  Batch 3,650  of  4,160.    Elapsed: 1345.5357766151428.\n",
      "  Batch 3,700  of  4,160.    Elapsed: 1362.8600723743439.\n",
      "  Batch 3,750  of  4,160.    Elapsed: 1380.8237681388855.\n",
      "  Batch 3,800  of  4,160.    Elapsed: 1399.5942392349243.\n",
      "  Batch 3,850  of  4,160.    Elapsed: 1418.221899986267.\n",
      "  Batch 3,900  of  4,160.    Elapsed: 1437.7725446224213.\n",
      "  Batch 3,950  of  4,160.    Elapsed: 1455.0829768180847.\n",
      "  Batch 4,000  of  4,160.    Elapsed: 1473.4399836063385.\n",
      "  Batch 4,050  of  4,160.    Elapsed: 1493.2903881072998.\n",
      "  Batch 4,100  of  4,160.    Elapsed: 1512.579672574997.\n",
      "  Batch 4,150  of  4,160.    Elapsed: 1530.912129163742.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoc h took: 1534.5936434268951\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99207\n",
      "  F1: 0.94342\n",
      "  Validation Loss: 0.04514\n",
      "  Validation took: 162.1605544090271\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 5 -----\n",
      "  Batch    50  of  4,160.    Elapsed: 17.58150053024292.\n",
      "  Batch   100  of  4,160.    Elapsed: 35.6111421585083.\n",
      "  Batch   150  of  4,160.    Elapsed: 54.087809562683105.\n",
      "  Batch   200  of  4,160.    Elapsed: 74.3545229434967.\n",
      "  Batch   250  of  4,160.    Elapsed: 92.31636834144592.\n",
      "  Batch   300  of  4,160.    Elapsed: 110.12882375717163.\n",
      "  Batch   350  of  4,160.    Elapsed: 128.19047117233276.\n",
      "  Batch   400  of  4,160.    Elapsed: 146.25011348724365.\n",
      "  Batch   450  of  4,160.    Elapsed: 164.57548594474792.\n",
      "  Batch   500  of  4,160.    Elapsed: 183.92187976837158.\n",
      "  Batch   550  of  4,160.    Elapsed: 204.585289478302.\n",
      "  Batch   600  of  4,160.    Elapsed: 224.1953320503235.\n",
      "  Batch   650  of  4,160.    Elapsed: 243.24709582328796.\n",
      "  Batch   700  of  4,160.    Elapsed: 261.9968407154083.\n",
      "  Batch   750  of  4,160.    Elapsed: 280.38054299354553.\n",
      "  Batch   800  of  4,160.    Elapsed: 300.19137239456177.\n",
      "  Batch   850  of  4,160.    Elapsed: 319.1016230583191.\n",
      "  Batch   900  of  4,160.    Elapsed: 337.91645336151123.\n",
      "  Batch   950  of  4,160.    Elapsed: 356.553186416626.\n",
      "  Batch 1,000  of  4,160.    Elapsed: 374.8505108356476.\n",
      "  Batch 1,050  of  4,160.    Elapsed: 393.21954107284546.\n",
      "  Batch 1,100  of  4,160.    Elapsed: 410.8209698200226.\n",
      "  Batch 1,150  of  4,160.    Elapsed: 428.42523980140686.\n",
      "  Batch 1,200  of  4,160.    Elapsed: 446.3296785354614.\n",
      "  Batch 1,250  of  4,160.    Elapsed: 464.38499999046326.\n",
      "  Batch 1,300  of  4,160.    Elapsed: 482.47905588150024.\n",
      "  Batch 1,350  of  4,160.    Elapsed: 500.9150741100311.\n",
      "  Batch 1,400  of  4,160.    Elapsed: 518.7963185310364.\n",
      "  Batch 1,450  of  4,160.    Elapsed: 538.6685168743134.\n",
      "  Batch 1,500  of  4,160.    Elapsed: 557.80206823349.\n",
      "  Batch 1,550  of  4,160.    Elapsed: 576.1568145751953.\n",
      "  Batch 1,600  of  4,160.    Elapsed: 593.598819732666.\n",
      "  Batch 1,650  of  4,160.    Elapsed: 611.7134330272675.\n",
      "  Batch 1,700  of  4,160.    Elapsed: 629.2831871509552.\n",
      "  Batch 1,750  of  4,160.    Elapsed: 648.2954108715057.\n",
      "  Batch 1,800  of  4,160.    Elapsed: 665.9214508533478.\n",
      "  Batch 1,850  of  4,160.    Elapsed: 684.2242052555084.\n",
      "  Batch 1,900  of  4,160.    Elapsed: 701.9026627540588.\n",
      "  Batch 1,950  of  4,160.    Elapsed: 721.3752896785736.\n",
      "  Batch 2,000  of  4,160.    Elapsed: 741.3371803760529.\n",
      "  Batch 2,050  of  4,160.    Elapsed: 759.4880094528198.\n",
      "  Batch 2,100  of  4,160.    Elapsed: 778.0833401679993.\n",
      "  Batch 2,150  of  4,160.    Elapsed: 795.5109922885895.\n",
      "  Batch 2,200  of  4,160.    Elapsed: 813.1391010284424.\n",
      "  Batch 2,250  of  4,160.    Elapsed: 831.9839293956757.\n",
      "  Batch 2,300  of  4,160.    Elapsed: 852.7501873970032.\n",
      "  Batch 2,350  of  4,160.    Elapsed: 872.938047170639.\n",
      "  Batch 2,400  of  4,160.    Elapsed: 891.2962419986725.\n",
      "  Batch 2,450  of  4,160.    Elapsed: 909.758535861969.\n",
      "  Batch 2,500  of  4,160.    Elapsed: 928.8474977016449.\n",
      "  Batch 2,550  of  4,160.    Elapsed: 948.5504853725433.\n",
      "  Batch 2,600  of  4,160.    Elapsed: 966.3010172843933.\n",
      "  Batch 2,650  of  4,160.    Elapsed: 984.4011232852936.\n",
      "  Batch 2,700  of  4,160.    Elapsed: 1002.4595966339111.\n",
      "  Batch 2,750  of  4,160.    Elapsed: 1020.0277976989746.\n",
      "  Batch 2,800  of  4,160.    Elapsed: 1038.0626571178436.\n",
      "  Batch 2,850  of  4,160.    Elapsed: 1057.257891178131.\n",
      "  Batch 2,900  of  4,160.    Elapsed: 1076.4180755615234.\n",
      "  Batch 2,950  of  4,160.    Elapsed: 1094.3284740447998.\n",
      "  Batch 3,000  of  4,160.    Elapsed: 1112.6903743743896.\n",
      "  Batch 3,050  of  4,160.    Elapsed: 1130.5142357349396.\n",
      "  Batch 3,100  of  4,160.    Elapsed: 1148.0225267410278.\n",
      "  Batch 3,150  of  4,160.    Elapsed: 1166.0465528964996.\n",
      "  Batch 3,200  of  4,160.    Elapsed: 1184.9508154392242.\n",
      "  Batch 3,250  of  4,160.    Elapsed: 1203.3502895832062.\n",
      "  Batch 3,300  of  4,160.    Elapsed: 1221.356458902359.\n",
      "  Batch 3,350  of  4,160.    Elapsed: 1239.517017364502.\n",
      "  Batch 3,400  of  4,160.    Elapsed: 1257.3410029411316.\n",
      "  Batch 3,450  of  4,160.    Elapsed: 1276.2720577716827.\n",
      "  Batch 3,500  of  4,160.    Elapsed: 1293.9670391082764.\n",
      "  Batch 3,550  of  4,160.    Elapsed: 1311.749175310135.\n",
      "  Batch 3,600  of  4,160.    Elapsed: 1330.740215063095.\n",
      "  Batch 3,650  of  4,160.    Elapsed: 1349.3468582630157.\n",
      "  Batch 3,700  of  4,160.    Elapsed: 1368.2644412517548.\n",
      "  Batch 3,750  of  4,160.    Elapsed: 1387.1909537315369.\n",
      "  Batch 3,800  of  4,160.    Elapsed: 1406.1498866081238.\n",
      "  Batch 3,850  of  4,160.    Elapsed: 1426.0153994560242.\n",
      "  Batch 3,900  of  4,160.    Elapsed: 1445.0118296146393.\n",
      "  Batch 3,950  of  4,160.    Elapsed: 1464.5882823467255.\n",
      "  Batch 4,000  of  4,160.    Elapsed: 1484.120884180069.\n",
      "  Batch 4,050  of  4,160.    Elapsed: 1502.448063135147.\n",
      "  Batch 4,100  of  4,160.    Elapsed: 1520.0121266841888.\n",
      "  Batch 4,150  of  4,160.    Elapsed: 1537.4532072544098.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoc h took: 1540.8887257575989\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99111\n",
      "  F1: 0.94340\n",
      "  Validation Loss: 0.05414\n",
      "  Validation took: 164.2201292514801\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 5 -----\n",
      "  Batch    50  of  4,160.    Elapsed: 17.85563611984253.\n",
      "  Batch   100  of  4,160.    Elapsed: 36.98500156402588.\n",
      "  Batch   150  of  4,160.    Elapsed: 55.890833616256714.\n",
      "  Batch   200  of  4,160.    Elapsed: 73.74261260032654.\n",
      "  Batch   250  of  4,160.    Elapsed: 92.23317050933838.\n",
      "  Batch   300  of  4,160.    Elapsed: 110.50770139694214.\n",
      "  Batch   350  of  4,160.    Elapsed: 130.08100652694702.\n",
      "  Batch   400  of  4,160.    Elapsed: 149.27398824691772.\n",
      "  Batch   450  of  4,160.    Elapsed: 168.21980214118958.\n",
      "  Batch   500  of  4,160.    Elapsed: 185.77413129806519.\n",
      "  Batch   550  of  4,160.    Elapsed: 204.2494192123413.\n",
      "  Batch   600  of  4,160.    Elapsed: 221.98260688781738.\n",
      "  Batch   650  of  4,160.    Elapsed: 239.94244527816772.\n",
      "  Batch   700  of  4,160.    Elapsed: 258.017648935318.\n",
      "  Batch   750  of  4,160.    Elapsed: 276.66926312446594.\n",
      "  Batch   800  of  4,160.    Elapsed: 295.2996983528137.\n",
      "  Batch   850  of  4,160.    Elapsed: 313.7805206775665.\n",
      "  Batch   900  of  4,160.    Elapsed: 332.65572571754456.\n",
      "  Batch   950  of  4,160.    Elapsed: 351.83656907081604.\n",
      "  Batch 1,000  of  4,160.    Elapsed: 371.0102310180664.\n",
      "  Batch 1,050  of  4,160.    Elapsed: 388.7362582683563.\n",
      "  Batch 1,100  of  4,160.    Elapsed: 407.1280589103699.\n",
      "  Batch 1,150  of  4,160.    Elapsed: 426.5362060070038.\n",
      "  Batch 1,200  of  4,160.    Elapsed: 446.943754196167.\n",
      "  Batch 1,250  of  4,160.    Elapsed: 465.33194398880005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,300  of  4,160.    Elapsed: 484.4105224609375.\n",
      "  Batch 1,350  of  4,160.    Elapsed: 503.1155822277069.\n",
      "  Batch 1,400  of  4,160.    Elapsed: 522.0715494155884.\n",
      "  Batch 1,450  of  4,160.    Elapsed: 540.4148154258728.\n",
      "  Batch 1,500  of  4,160.    Elapsed: 559.7886364459991.\n",
      "  Batch 1,550  of  4,160.    Elapsed: 578.3877775669098.\n",
      "  Batch 1,600  of  4,160.    Elapsed: 597.8031077384949.\n",
      "  Batch 1,650  of  4,160.    Elapsed: 617.293134689331.\n",
      "  Batch 1,700  of  4,160.    Elapsed: 635.2435967922211.\n",
      "  Batch 1,750  of  4,160.    Elapsed: 653.6979978084564.\n",
      "  Batch 1,800  of  4,160.    Elapsed: 672.6571073532104.\n",
      "  Batch 1,850  of  4,160.    Elapsed: 690.9754354953766.\n",
      "  Batch 1,900  of  4,160.    Elapsed: 709.8436243534088.\n",
      "  Batch 1,950  of  4,160.    Elapsed: 727.242707490921.\n",
      "  Batch 2,000  of  4,160.    Elapsed: 746.8545088768005.\n",
      "  Batch 2,050  of  4,160.    Elapsed: 766.3234102725983.\n",
      "  Batch 2,100  of  4,160.    Elapsed: 784.6837890148163.\n",
      "  Batch 2,150  of  4,160.    Elapsed: 802.2816622257233.\n",
      "  Batch 2,200  of  4,160.    Elapsed: 820.2607758045197.\n",
      "  Batch 2,250  of  4,160.    Elapsed: 837.6170337200165.\n",
      "  Batch 2,300  of  4,160.    Elapsed: 854.9484610557556.\n",
      "  Batch 2,350  of  4,160.    Elapsed: 872.9216752052307.\n",
      "  Batch 2,400  of  4,160.    Elapsed: 890.6499774456024.\n",
      "  Batch 2,450  of  4,160.    Elapsed: 908.2245826721191.\n",
      "  Batch 2,500  of  4,160.    Elapsed: 925.6067426204681.\n",
      "  Batch 2,550  of  4,160.    Elapsed: 943.0402121543884.\n",
      "  Batch 2,600  of  4,160.    Elapsed: 960.3845658302307.\n",
      "  Batch 2,650  of  4,160.    Elapsed: 977.8025450706482.\n",
      "  Batch 2,700  of  4,160.    Elapsed: 995.2083857059479.\n",
      "  Batch 2,750  of  4,160.    Elapsed: 1012.7017731666565.\n",
      "  Batch 2,800  of  4,160.    Elapsed: 1030.1181316375732.\n",
      "  Batch 2,850  of  4,160.    Elapsed: 1047.5223598480225.\n",
      "  Batch 2,900  of  4,160.    Elapsed: 1064.8596765995026.\n",
      "  Batch 2,950  of  4,160.    Elapsed: 1082.234793663025.\n",
      "  Batch 3,000  of  4,160.    Elapsed: 1099.6349775791168.\n",
      "  Batch 3,050  of  4,160.    Elapsed: 1117.0211045742035.\n",
      "  Batch 3,100  of  4,160.    Elapsed: 1134.3133389949799.\n",
      "  Batch 3,150  of  4,160.    Elapsed: 1151.7758774757385.\n",
      "  Batch 3,200  of  4,160.    Elapsed: 1169.2585957050323.\n",
      "  Batch 3,250  of  4,160.    Elapsed: 1186.6301827430725.\n",
      "  Batch 3,300  of  4,160.    Elapsed: 1204.0048224925995.\n",
      "  Batch 3,350  of  4,160.    Elapsed: 1221.4025535583496.\n",
      "  Batch 3,400  of  4,160.    Elapsed: 1238.8405821323395.\n",
      "  Batch 3,450  of  4,160.    Elapsed: 1256.2785613536835.\n",
      "  Batch 3,500  of  4,160.    Elapsed: 1273.65416097641.\n",
      "  Batch 3,550  of  4,160.    Elapsed: 1290.9935820102692.\n",
      "  Batch 3,600  of  4,160.    Elapsed: 1308.4097487926483.\n",
      "  Batch 3,650  of  4,160.    Elapsed: 1325.8578765392303.\n",
      "  Batch 3,700  of  4,160.    Elapsed: 1343.3117797374725.\n",
      "  Batch 3,750  of  4,160.    Elapsed: 1360.6827194690704.\n",
      "  Batch 3,800  of  4,160.    Elapsed: 1378.0744791030884.\n",
      "  Batch 3,850  of  4,160.    Elapsed: 1395.5483932495117.\n",
      "  Batch 3,900  of  4,160.    Elapsed: 1412.9318914413452.\n",
      "  Batch 3,950  of  4,160.    Elapsed: 1430.3181035518646.\n",
      "  Batch 4,000  of  4,160.    Elapsed: 1447.692236661911.\n",
      "  Batch 4,050  of  4,160.    Elapsed: 1465.055870294571.\n",
      "  Batch 4,100  of  4,160.    Elapsed: 1482.4238018989563.\n",
      "  Batch 4,150  of  4,160.    Elapsed: 1499.8198413848877.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoc h took: 1503.2315645217896\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99327\n",
      "  F1: 0.94412\n",
      "  Validation Loss: 0.05169\n",
      "  Validation took: 153.33362436294556\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 5 -----\n",
      "  Batch    50  of  4,160.    Elapsed: 17.53506851196289.\n",
      "  Batch   100  of  4,160.    Elapsed: 34.919634103775024.\n",
      "  Batch   150  of  4,160.    Elapsed: 52.278512477874756.\n",
      "  Batch   200  of  4,160.    Elapsed: 69.65893530845642.\n",
      "  Batch   250  of  4,160.    Elapsed: 86.97881054878235.\n",
      "  Batch   300  of  4,160.    Elapsed: 104.3911702632904.\n",
      "  Batch   350  of  4,160.    Elapsed: 121.86532139778137.\n",
      "  Batch   400  of  4,160.    Elapsed: 139.20152497291565.\n",
      "  Batch   450  of  4,160.    Elapsed: 156.600355386734.\n",
      "  Batch   500  of  4,160.    Elapsed: 174.05803418159485.\n",
      "  Batch   550  of  4,160.    Elapsed: 191.46745371818542.\n",
      "  Batch   600  of  4,160.    Elapsed: 208.81180262565613.\n",
      "  Batch   650  of  4,160.    Elapsed: 226.15194511413574.\n",
      "  Batch   700  of  4,160.    Elapsed: 243.65865421295166.\n",
      "  Batch   750  of  4,160.    Elapsed: 261.0377504825592.\n",
      "  Batch   800  of  4,160.    Elapsed: 278.39647936820984.\n",
      "  Batch   850  of  4,160.    Elapsed: 295.77442693710327.\n",
      "  Batch   900  of  4,160.    Elapsed: 313.09039878845215.\n",
      "  Batch   950  of  4,160.    Elapsed: 330.4678556919098.\n",
      "  Batch 1,000  of  4,160.    Elapsed: 347.9394385814667.\n",
      "  Batch 1,050  of  4,160.    Elapsed: 365.41579937934875.\n",
      "  Batch 1,100  of  4,160.    Elapsed: 382.7806646823883.\n",
      "  Batch 1,150  of  4,160.    Elapsed: 400.2134475708008.\n",
      "  Batch 1,200  of  4,160.    Elapsed: 417.6096749305725.\n",
      "  Batch 1,250  of  4,160.    Elapsed: 434.94291067123413.\n",
      "  Batch 1,300  of  4,160.    Elapsed: 452.38016152381897.\n",
      "  Batch 1,350  of  4,160.    Elapsed: 469.8151595592499.\n",
      "  Batch 1,400  of  4,160.    Elapsed: 487.1495854854584.\n",
      "  Batch 1,450  of  4,160.    Elapsed: 504.53657841682434.\n",
      "  Batch 1,500  of  4,160.    Elapsed: 521.8764336109161.\n",
      "  Batch 1,550  of  4,160.    Elapsed: 539.1648602485657.\n",
      "  Batch 1,600  of  4,160.    Elapsed: 556.5924689769745.\n",
      "  Batch 1,650  of  4,160.    Elapsed: 573.9765155315399.\n",
      "  Batch 1,700  of  4,160.    Elapsed: 591.4202346801758.\n",
      "  Batch 1,750  of  4,160.    Elapsed: 608.74982213974.\n",
      "  Batch 1,800  of  4,160.    Elapsed: 626.1464726924896.\n",
      "  Batch 1,850  of  4,160.    Elapsed: 643.5163879394531.\n",
      "  Batch 1,900  of  4,160.    Elapsed: 660.8297505378723.\n",
      "  Batch 1,950  of  4,160.    Elapsed: 678.2512247562408.\n",
      "  Batch 2,000  of  4,160.    Elapsed: 695.6848201751709.\n",
      "  Batch 2,050  of  4,160.    Elapsed: 713.0842008590698.\n",
      "  Batch 2,100  of  4,160.    Elapsed: 730.4865081310272.\n",
      "  Batch 2,150  of  4,160.    Elapsed: 747.9227468967438.\n",
      "  Batch 2,200  of  4,160.    Elapsed: 765.3657054901123.\n",
      "  Batch 2,250  of  4,160.    Elapsed: 782.7404136657715.\n",
      "  Batch 2,300  of  4,160.    Elapsed: 800.1146128177643.\n",
      "  Batch 2,350  of  4,160.    Elapsed: 817.4878916740417.\n",
      "  Batch 2,400  of  4,160.    Elapsed: 834.8368690013885.\n",
      "  Batch 2,450  of  4,160.    Elapsed: 852.2581405639648.\n",
      "  Batch 2,500  of  4,160.    Elapsed: 869.6621129512787.\n",
      "  Batch 2,550  of  4,160.    Elapsed: 887.0788300037384.\n",
      "  Batch 2,600  of  4,160.    Elapsed: 904.5661904811859.\n",
      "  Batch 2,650  of  4,160.    Elapsed: 921.8942334651947.\n",
      "  Batch 2,700  of  4,160.    Elapsed: 939.3226008415222.\n",
      "  Batch 2,750  of  4,160.    Elapsed: 956.747368812561.\n",
      "  Batch 2,800  of  4,160.    Elapsed: 974.1999952793121.\n",
      "  Batch 2,850  of  4,160.    Elapsed: 991.6283559799194.\n",
      "  Batch 2,900  of  4,160.    Elapsed: 1009.110940694809.\n",
      "  Batch 2,950  of  4,160.    Elapsed: 1026.5302910804749.\n",
      "  Batch 3,000  of  4,160.    Elapsed: 1044.027423620224.\n",
      "  Batch 3,050  of  4,160.    Elapsed: 1061.3115532398224.\n",
      "  Batch 3,100  of  4,160.    Elapsed: 1078.5904924869537.\n",
      "  Batch 3,150  of  4,160.    Elapsed: 1095.8507978916168.\n",
      "  Batch 3,200  of  4,160.    Elapsed: 1113.0996799468994.\n",
      "  Batch 3,250  of  4,160.    Elapsed: 1130.4036452770233.\n",
      "  Batch 3,300  of  4,160.    Elapsed: 1147.7468497753143.\n",
      "  Batch 3,350  of  4,160.    Elapsed: 1165.0410158634186.\n",
      "  Batch 3,400  of  4,160.    Elapsed: 1182.319665670395.\n",
      "  Batch 3,450  of  4,160.    Elapsed: 1199.5987265110016.\n",
      "  Batch 3,500  of  4,160.    Elapsed: 1216.8556282520294.\n",
      "  Batch 3,550  of  4,160.    Elapsed: 1234.131842136383.\n",
      "  Batch 3,600  of  4,160.    Elapsed: 1251.4214143753052.\n",
      "  Batch 3,650  of  4,160.    Elapsed: 1268.781608581543.\n",
      "  Batch 3,700  of  4,160.    Elapsed: 1286.116971731186.\n",
      "  Batch 3,750  of  4,160.    Elapsed: 1303.4189324378967.\n",
      "  Batch 3,800  of  4,160.    Elapsed: 1320.7739963531494.\n",
      "  Batch 3,850  of  4,160.    Elapsed: 1338.0902786254883.\n",
      "  Batch 3,900  of  4,160.    Elapsed: 1355.423182964325.\n",
      "  Batch 3,950  of  4,160.    Elapsed: 1372.6957409381866.\n",
      "  Batch 4,000  of  4,160.    Elapsed: 1390.034790277481.\n",
      "  Batch 4,050  of  4,160.    Elapsed: 1407.2970838546753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4,100  of  4,160.    Elapsed: 1424.5584454536438.\n",
      "  Batch 4,150  of  4,160.    Elapsed: 1441.8161218166351.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoc h took: 1445.2066023349762\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99375\n",
      "  F1: 0.94318\n",
      "  Validation Loss: 0.04326\n",
      "  Validation took: 153.01307678222656\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0688be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/bert_regexp_stopwords_lemmatization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
