{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['up',\n",
       " 'needn',\n",
       " 'won',\n",
       " 'shan',\n",
       " \"hadn't\",\n",
       " 'here',\n",
       " 'doesn',\n",
       " 'have',\n",
       " 'am',\n",
       " 'having']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "random.sample(stopwords.words('english'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"FakeNews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/train.csv.zip\")\n",
    "test_dataset = pd.read_csv(f\"./data/{CUR_DATASET}/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.fillna(\"null data\")\n",
    "test_dataset = test_dataset.fillna(\"null data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>house dem aide: even see comey's letter jason ...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>house dem aide: even see comey's letter jason ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flynn: hillary clinton, big woman campus - bre...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fired october 29, 2016 tension...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 civilians killed single us airstrike identi...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>videos 15 civilians killed single us airstrike...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  house dem aide: even see comey's letter jason ...       Darrell Lucus   \n",
       "1   1  flynn: hillary clinton, big woman campus - bre...     Daniel J. Flynn   \n",
       "2   2                              truth might get fired  Consortiumnews.com   \n",
       "3   3  15 civilians killed single us airstrike identi...     Jessica Purkiss   \n",
       "4   4  iranian woman jailed fictional unpublished sto...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide: even see comey's letter jason ...      1  \n",
       "1  ever get feeling life circles roundabout rathe...      0  \n",
       "2  truth might get fired october 29, 2016 tension...      1  \n",
       "3  videos 15 civilians killed single us airstrike...      1  \n",
       "4  print iranian woman sentenced six years prison...      1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', '', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"she's\",\n",
       " 'from',\n",
       " \"aren't\",\n",
       " 'after',\n",
       " 'shouldn',\n",
       " 'same',\n",
       " 'it',\n",
       " 'their',\n",
       " 'in',\n",
       " 'until',\n",
       " 'needn',\n",
       " 'them',\n",
       " \"you'd\",\n",
       " 'doing',\n",
       " 'aren',\n",
       " \"doesn't\",\n",
       " 'me',\n",
       " 'any',\n",
       " 'having',\n",
       " 'how']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = [preprocessing_text_fn[\"no_punctuation\"](word) for word in stop_words]\n",
    "random.sample(stop_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_stopwords(text, stop_words=STOP_WORDS):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sequence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "    return filtered_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>house dem aide even see comeys letter jason ch...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>house dem aide even see comeys letter jason ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flynn hillary clinton big woman campus breitbart</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>ever get feeling life circles roundabout rathe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth might get fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>civilians killed single us airstrike identified</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>videos civilians killed single us airstrike id...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>print iranian woman sentenced six years prison...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jackie mason hollywood would love trump bombed...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>trying times jackie mason voice reason in week...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>life life luxury elton johns favorite shark pi...</td>\n",
       "      <td>null data</td>\n",
       "      <td>ever wonder britains iconic pop pianist gets l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>benoît hamon wins french socialist partys pres...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>paris france chose idealistic traditional cand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>excerpts draft script donald trumps qampa blac...</td>\n",
       "      <td>null data</td>\n",
       "      <td>donaldtrump scheduled make highly anticipated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>backchannel plan ukraine russia courtesy trump...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>week michaelflynn resigned national security a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  house dem aide even see comeys letter jason ch...   \n",
       "1   1   flynn hillary clinton big woman campus breitbart   \n",
       "2   2                              truth might get fired   \n",
       "3   3    civilians killed single us airstrike identified   \n",
       "4   4  iranian woman jailed fictional unpublished sto...   \n",
       "5   5  jackie mason hollywood would love trump bombed...   \n",
       "6   6  life life luxury elton johns favorite shark pi...   \n",
       "7   7  benoît hamon wins french socialist partys pres...   \n",
       "8   8  excerpts draft script donald trumps qampa blac...   \n",
       "9   9  backchannel plan ukraine russia courtesy trump...   \n",
       "\n",
       "                         author  \\\n",
       "0                 Darrell Lucus   \n",
       "1               Daniel J. Flynn   \n",
       "2            Consortiumnews.com   \n",
       "3               Jessica Purkiss   \n",
       "4                Howard Portnoy   \n",
       "5               Daniel Nussbaum   \n",
       "6                     null data   \n",
       "7               Alissa J. Rubin   \n",
       "8                     null data   \n",
       "9  Megan Twohey and Scott Shane   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide even see comeys letter jason ch...      1  \n",
       "1  ever get feeling life circles roundabout rathe...      0  \n",
       "2  truth might get fired october tension intellig...      1  \n",
       "3  videos civilians killed single us airstrike id...      1  \n",
       "4  print iranian woman sentenced six years prison...      1  \n",
       "5  trying times jackie mason voice reason in week...      0  \n",
       "6  ever wonder britains iconic pop pianist gets l...      1  \n",
       "7  paris france chose idealistic traditional cand...      0  \n",
       "8  donaldtrump scheduled make highly anticipated ...      0  \n",
       "9  week michaelflynn resigned national security a...      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"title\"] = train_dataset[\"title\"].apply(preprocess_text)\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(preprocess_text)\n",
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues if Not Purse ...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO Calif After years of scorning the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>NoDAPL Native American Leaders Vow to Stay All...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos NoDAPL Native American Leaders Vow to S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback This T...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you dont succeed trydifferent spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report Meme Wars E</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>mins ago Views Comments Likes For the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Trump is USAs antique hero Clinton will be nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>Pelosi Calls for FBI Investigation to Find Out...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Sunday on NBCs Meet the Press House Minority L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>Weekly Featured Profile Randy Shannon</td>\n",
       "      <td>Trevor Loudon</td>\n",
       "      <td>You are here Home Articles of the Bound Weekly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "      <td>null data</td>\n",
       "      <td>Urban Population Booms Will Make Climate Chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>null data</td>\n",
       "      <td>cognitive dissident</td>\n",
       "      <td>dont we have the receipt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues if Not Purse ...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  NoDAPL Native American Leaders Vow to Stay All...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback This T...   \n",
       "4  20804                          Keiser Report Meme Wars E   \n",
       "5  20805  Trump is USAs antique hero Clinton will be nex...   \n",
       "6  20806  Pelosi Calls for FBI Investigation to Find Out...   \n",
       "7  20807              Weekly Featured Profile Randy Shannon   \n",
       "8  20808  Urban Population Booms Will Make Climate Chang...   \n",
       "9  20809                                          null data   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO Calif After years of scorning the po...  \n",
       "1                null data  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos NoDAPL Native American Leaders Vow to S...  \n",
       "3            Daniel Victor  If at first you dont succeed trydifferent spor...  \n",
       "4  Truth Broadcast Network   mins ago Views Comments Likes For the first t...  \n",
       "5                null data  Trump is USAs antique hero Clinton will be nex...  \n",
       "6                  Pam Key  Sunday on NBCs Meet the Press House Minority L...  \n",
       "7            Trevor Loudon  You are here Home Articles of the Bound Weekly...  \n",
       "8                null data  Urban Population Booms Will Make Climate Chang...  \n",
       "9      cognitive dissident                           dont we have the receipt  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"title\"] = test_dataset[\"title\"].apply(preprocess_text)\n",
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(preprocess_text)\n",
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (train_dataset['title'] + \" \" + train_dataset['text']).values\n",
    "test_text = (test_dataset['title'] + \" \" + test_dataset['text']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit length of each article\n",
    "max_length = 3000\n",
    "lengths = np.array([len(x) for x in train_text])\n",
    "train_text = train_text[lengths < max_length]\n",
    "labels = labels[lengths < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check actual max length of an article\n",
    "article_length = max(np.array([len(x) for x in train_text]))\n",
    "article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec model with vector size = 100\n",
    "vec_size = 100\n",
    "\n",
    "# workers - number of CPU threads\n",
    "word_model = gensim.models.Word2Vec(train_text, vector_size = vec_size, window = 5, workers = 12)\n",
    "word_model.train(train_text, epochs = 10, total_examples = len(train_text))\n",
    "wv = word_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "encoded_articles = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_articles = pad_sequences(encoded_articles, maxlen = article_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12347, 2999)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros(shape=(vocabulary_size, vec_size))\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    ind = wv.has_index_for(w)\n",
    "    if ind:\n",
    "        emb_matrix[i] = wv.get_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_articles, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim = vocabulary_size, \n",
    "                    output_dim = vec_size,\n",
    "                    input_length = article_length,\n",
    "                    embeddings_initializer = Constant(emb_matrix))\n",
    "         )\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2999, 100)         10193000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2999, 32)          17024     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 95968)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               12284032  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,496,137\n",
      "Trainable params: 22,496,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def keras_f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(),loss='binary_crossentropy', metrics=['accuracy', keras_f1_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "73/73 [==============================] - 13s 117ms/step - loss: 0.3191 - accuracy: 0.8460 - keras_f1_score: 0.8830 - val_loss: 0.1263 - val_accuracy: 0.9566 - val_keras_f1_score: 0.9636\n",
      "Epoch 2/5\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 0.0372 - accuracy: 0.9902 - keras_f1_score: 0.9917 - val_loss: 0.1205 - val_accuracy: 0.9615 - val_keras_f1_score: 0.9667\n",
      "Epoch 3/5\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 0.7216 - accuracy: 0.9662 - keras_f1_score: 0.9722 - val_loss: 0.2070 - val_accuracy: 0.9394 - val_keras_f1_score: 0.9501\n",
      "Epoch 4/5\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 0.0261 - accuracy: 0.9918 - keras_f1_score: 0.9932 - val_loss: 0.2266 - val_accuracy: 0.9521 - val_keras_f1_score: 0.9595\n",
      "Epoch 5/5\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 0.0037 - accuracy: 0.9985 - keras_f1_score: 0.9988 - val_loss: 0.3559 - val_accuracy: 0.9495 - val_keras_f1_score: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e5bc71288>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def accuracy(predictions, labels):\n",
    "    predictions[predictions >= 0.5] = 1\n",
    "    predictions[predictions < 0.5] = 0\n",
    "    return accuracy_score(labels, predictions)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    predictions[predictions >= 0.5] = 1\n",
    "    predictions[predictions < 0.5] = 0\n",
    "    return f1_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIAR dataset tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset_train = pd.read_csv('./data/LIAR/train.tsv', sep='\\t', header = None)\n",
    "liar_dataset_test = pd.read_csv('./data/LIAR/test.tsv', sep='\\t', header = None)\n",
    "liar_dataset_valid = pd.read_csv('./data/LIAR/valid.tsv', sep='\\t', header = None)\n",
    "liar_dataset = pd.concat([liar_dataset_train, liar_dataset_test, liar_dataset_valid], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset = liar_dataset.iloc[:, [1, 2]]\n",
    "liar_dataset = liar_dataset.rename(columns = {1: 'label', 2: 'statements'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset['label'] = liar_dataset['label'].replace({\n",
    "    'false' : False,\n",
    "    'barely-true' : False,\n",
    "    'pants-fire' : False,\n",
    "    'half-true' : True,\n",
    "    'mostly-true' : True,\n",
    "    'true' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_label = liar_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_liar_text = tokenizer.texts_to_sequences(liar_dataset['statements'])\n",
    "padded_liar_text = pad_sequences(encoded_liar_text, maxlen = article_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on test dataset (without training on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 4s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(padded_liar_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5391290751309514"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(pred, liar_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894262683736369"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_f1_score(pred, liar_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
