{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"CT-FAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = pd.read_csv(f'../data/{CUR_DATASET}/Task3_english_dev.csv')\n",
    "data_train = pd.read_csv(f'../data/{CUR_DATASET}/Task3_english_training.csv')\n",
    "data_test = pd.read_csv(f'../data/{CUR_DATASET}/English_data_test_release_with_rating.csv')\n",
    "\n",
    "data_concat = pd.concat([data_train, data_dev])\n",
    "data_concat.rename(columns={'our rating':'label'}, inplace=True)\n",
    "data_test.rename(columns={'our rating':'label'}, inplace=True)\n",
    "data_concat['label'] = data_concat['label'].apply(lambda x: x.lower())\n",
    "data_test['label'] = data_test['label'].apply(lambda x: x.lower())\n",
    "\n",
    "train_dataset = data_concat\n",
    "test_dataset = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cb85e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>Distracted driving causes more deaths in Canad...</td>\n",
       "      <td>You Can Be Fined $1,500 If Your Passenger Is U...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>Missouri politicians have made statements afte...</td>\n",
       "      <td>Missouri lawmakers condemn Las Vegas shooting</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>Home Alone 2: Lost in New York is full of viol...</td>\n",
       "      <td>CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>But things took a turn for the worse when riot...</td>\n",
       "      <td>Obama’s Daughters Caught on Camera Burning US ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>It’s no secret that Epstein and Schiff share a...</td>\n",
       "      <td>Leaked Visitor Logs Reveal Schiff’s 78 Visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  public_id                                               text  \\\n",
       "0  5a228e0e  Distracted driving causes more deaths in Canad...   \n",
       "1  30c605a1  Missouri politicians have made statements afte...   \n",
       "2  c3dea290  Home Alone 2: Lost in New York is full of viol...   \n",
       "3  f14e8eb6  But things took a turn for the worse when riot...   \n",
       "4  faf024d6  It’s no secret that Epstein and Schiff share a...   \n",
       "\n",
       "                                               title            label  \n",
       "0  You Can Be Fined $1,500 If Your Passenger Is U...            false  \n",
       "1      Missouri lawmakers condemn Las Vegas shooting  partially false  \n",
       "2  CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...  partially false  \n",
       "3  Obama’s Daughters Caught on Camera Burning US ...            false  \n",
       "4  Leaked Visitor Logs Reveal Schiff’s 78 Visits ...            false  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0fec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.fillna(\"null data\")\n",
    "test_dataset = test_dataset.fillna(\"null data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7061d4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['false', 'other', 'partially false', 'true'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9953dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].replace({\n",
    "    'false' : 0,\n",
    "    'partially false' : 1,\n",
    "    'true' : 2,\n",
    "    'other' : 3,\n",
    "})\n",
    "\n",
    "test_dataset['label'] = test_dataset['label'].replace({\n",
    "    'false' : 0,\n",
    "    'partially false' : 1,\n",
    "    'true' : 2,\n",
    "    'other' : 3,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset[\"label\"].values.astype(int)\n",
    "test_labels = test_dataset[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee421",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe069c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4346120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddeb4e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9bea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    # \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset[\"title\"] = train_dataset[\"title\"].apply(preprocess_text)\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(preprocess_text)\n",
    "\n",
    "test_dataset[\"title\"] = test_dataset[\"title\"].apply(preprocess_text)\n",
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25112b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    for dataset in [train_dataset, test_dataset]:\n",
    "        for col in [\"title\", \"text\"]:\n",
    "\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6292eec",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4b4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e159b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for dataset in [train_dataset, test_dataset]:\n",
    "        for col in [\"title\", \"text\"]:\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "849ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (train_dataset[\"title\"] + \" \" + train_dataset[\"text\"]).values\n",
    "test_text = (test_dataset[\"title\"] + \" \" + test_dataset[\"text\"]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7676622f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  fined 1500 passenger using mobile phone starting next week distracted driving cause death canada impaired driving every province territory law driving operating cell phone tell passenger stay phone driving measure necessary distracted driving claimed life impaired driving province like british columbia ontario quebec alberta nova scotia manitoba newfoundland labrador mobile phone even held passenger dangerous distraction driver starting next week distracted screen held passenger attracts penalty 1500 three demerit point driver screen mix doesnt matter holding device using facetime taking selfies driver showing driver funny cat video nono province mobile phone categorised visual display unit meaning considered akin television screen important practice safe driving sake fellow driver canada cracking distracted driving problem rollout stricter law impose harsher penalty heftier fine guilty offender taking effect next week add serious penalty convicted distracted driving\n",
      "Tokenized:  ['fined', '1500', 'passenger', 'using', 'mobile', 'phone', 'starting', 'next', 'week', 'distracted', 'driving', 'cause', 'death', 'canada', 'impaired', 'driving', 'every', 'province', 'territory', 'law', 'driving', 'operating', 'cell', 'phone', 'tell', 'passenger', 'stay', 'phone', 'driving', 'measure', 'necessary', 'distracted', 'driving', 'claimed', 'life', 'impaired', 'driving', 'province', 'like', 'british', 'columbia', 'ontario', 'quebec', 'alberta', 'nova', 'scotia', 'manitoba', 'newfoundland', 'labrador', 'mobile', 'phone', 'even', 'held', 'passenger', 'dangerous', 'distraction', 'driver', 'starting', 'next', 'week', 'distracted', 'screen', 'held', 'passenger', 'attracts', 'penalty', '1500', 'three', 'dem', '##eri', '##t', 'point', 'driver', 'screen', 'mix', 'doesn', '##t', 'matter', 'holding', 'device', 'using', 'face', '##time', 'taking', 'self', '##ies', 'driver', 'showing', 'driver', 'funny', 'cat', 'video', 'non', '##o', 'province', 'mobile', 'phone', 'cat', '##ego', '##rise', '##d', 'visual', 'display', 'unit', 'meaning', 'considered', 'akin', 'television', 'screen', 'important', 'practice', 'safe', 'driving', 'sake', 'fellow', 'driver', 'canada', 'cracking', 'distracted', 'driving', 'problem', 'roll', '##out', 'strict', '##er', 'law', 'impose', 'harsh', '##er', 'penalty', 'he', '##ft', '##ier', 'fine', 'guilty', 'offender', 'taking', 'effect', 'next', 'week', 'add', 'serious', 'penalty', 'convicted', 'distracted', 'driving']\n",
      "Token IDs:  [16981, 10347, 4628, 2478, 4684, 3042, 3225, 2279, 2733, 11116, 4439, 3426, 2331, 2710, 18234, 4439, 2296, 2874, 3700, 2375, 4439, 4082, 3526, 3042, 2425, 4628, 2994, 3042, 4439, 5468, 4072, 11116, 4439, 3555, 2166, 18234, 4439, 2874, 2066, 2329, 3996, 4561, 5447, 7649, 6846, 9676, 10512, 11944, 18604, 4684, 3042, 2130, 2218, 4628, 4795, 14836, 4062, 3225, 2279, 2733, 11116, 3898, 2218, 4628, 17771, 6531, 10347, 2093, 17183, 11124, 2102, 2391, 4062, 3898, 4666, 2987, 2102, 3043, 3173, 5080, 2478, 2227, 7292, 2635, 2969, 3111, 4062, 4760, 4062, 6057, 4937, 2678, 2512, 2080, 2874, 4684, 3042, 4937, 20265, 29346, 2094, 5107, 4653, 3131, 3574, 2641, 17793, 2547, 3898, 2590, 3218, 3647, 4439, 8739, 3507, 4062, 2710, 15729, 11116, 4439, 3291, 4897, 5833, 9384, 2121, 2375, 17607, 8401, 2121, 6531, 2002, 6199, 3771, 2986, 5905, 25042, 2635, 3466, 2279, 2733, 5587, 3809, 6531, 7979, 11116, 4439]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_text[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(train_text[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = len(np.unique(labels)), \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dce90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/1264 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1264/1264 [00:04<00:00, 258.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  4905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae808334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.shape, train_text_filtered.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, labels_filtered = tokenize_map(train_text_filtered, labels_filtered)\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34763ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21142e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca257209",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filtered.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filtered = labels[indices]\n",
    "labels_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30770dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_size, val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return balanced_accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 18.526731967926025.\n",
      "  Batch   100  of  2,539.    Elapsed: 35.77412509918213.\n",
      "  Batch   150  of  2,539.    Elapsed: 53.16911959648132.\n",
      "  Batch   200  of  2,539.    Elapsed: 70.50109362602234.\n",
      "  Batch   250  of  2,539.    Elapsed: 87.71071934700012.\n",
      "  Batch   300  of  2,539.    Elapsed: 104.87576293945312.\n",
      "  Batch   350  of  2,539.    Elapsed: 122.0901551246643.\n",
      "  Batch   400  of  2,539.    Elapsed: 139.23676371574402.\n",
      "  Batch   450  of  2,539.    Elapsed: 156.49644017219543.\n",
      "  Batch   500  of  2,539.    Elapsed: 173.66641092300415.\n",
      "  Batch   550  of  2,539.    Elapsed: 190.8449137210846.\n",
      "  Batch   600  of  2,539.    Elapsed: 208.10811042785645.\n",
      "  Batch   650  of  2,539.    Elapsed: 225.41806554794312.\n",
      "  Batch   700  of  2,539.    Elapsed: 242.5963010787964.\n",
      "  Batch   750  of  2,539.    Elapsed: 259.8150019645691.\n",
      "  Batch   800  of  2,539.    Elapsed: 277.03073954582214.\n",
      "  Batch   850  of  2,539.    Elapsed: 294.2204089164734.\n",
      "  Batch   900  of  2,539.    Elapsed: 311.45267367362976.\n",
      "  Batch   950  of  2,539.    Elapsed: 328.7205259799957.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.9467520713806.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 363.1456880569458.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 380.35636734962463.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 397.5317075252533.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 414.7511467933655.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 431.99958658218384.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 449.2067892551422.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 466.4045855998993.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 483.6358699798584.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 500.8517723083496.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.061012506485.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 535.2557792663574.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 552.5092775821686.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 569.7761850357056.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 586.9945588111877.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 604.2493214607239.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 621.4609987735748.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 638.7677853107452.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 656.0087280273438.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 673.2252025604248.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.48024559021.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 707.6647248268127.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 724.8709154129028.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 742.1078555583954.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 759.4563784599304.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 776.6403601169586.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 793.8211891651154.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 811.1262574195862.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 828.3297731876373.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 845.5780079364777.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 862.7937688827515.\n",
      "\n",
      "  Average training loss: 1.76\n",
      "  Training epoc h took: 875.9932994842529\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.22100\n",
      "  F1: 0.17631\n",
      "  Validation Loss: 1.74472\n",
      "  Validation took: 93.42516279220581\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.188791036605835.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.344515323638916.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.54077911376953.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.7763512134552.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.04314422607422.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.28268504142761.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.50524187088013.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.83048033714294.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.02928471565247.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.24417757987976.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.38124823570251.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.7107059955597.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.97876143455505.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.214040517807.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.49692463874817.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.72161293029785.\n",
      "  Batch   850  of  2,539.    Elapsed: 293.0192549228668.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.28136110305786.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.5428376197815.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.906946182251.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 362.1586103439331.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 379.43166613578796.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.58168292045593.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.7483117580414.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.9238271713257.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 448.1366653442383.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 465.35143184661865.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.5778214931488.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.89976954460144.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.127557516098.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 534.3111469745636.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.4589757919312.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.6655511856079.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.8676643371582.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 603.0360672473907.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 620.2419354915619.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.4149820804596.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.5553059577942.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.739856004715.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.9911489486694.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.2556207180023.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.4606335163116.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.728390455246.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 758.0000972747803.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.1935887336731.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.4143579006195.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.6879301071167.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.9245049953461.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 844.095618724823.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.3040566444397.\n",
      "\n",
      "  Average training loss: 1.71\n",
      "  Training epoc h took: 874.5481903553009\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23504\n",
      "  F1: 0.21685\n",
      "  Validation Loss: 1.73069\n",
      "  Validation took: 93.49365901947021\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.222593069076538.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.5208842754364.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.734877586364746.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.94830989837646.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.20139861106873.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.44753122329712.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.68340706825256.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.90251350402832.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.14713311195374.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.3691520690918.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.69142246246338.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.89913034439087.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.14192271232605.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.3318042755127.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.5602924823761.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.83797311782837.\n",
      "  Batch   850  of  2,539.    Elapsed: 293.01138973236084.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.1692225933075.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.4072325229645.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.62476682662964.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.8434007167816.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 379.07635974884033.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.3148367404938.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.54607129096985.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.8325185775757.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 448.08989787101746.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 465.3095238208771.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.6067910194397.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.84105801582336.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.0673949718475.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 534.2613208293915.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.539300441742.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.7796285152435.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 586.0454759597778.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 603.2834923267365.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,800  of  2,539.    Elapsed: 620.6085796356201.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.896146774292.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 655.0860097408295.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 672.4487993717194.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.6684730052948.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.8717904090881.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 724.1131122112274.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 741.3255364894867.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 758.6409070491791.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.868989944458.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 793.0881237983704.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 810.3698906898499.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 827.5748028755188.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 844.870617389679.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 862.0970323085785.\n",
      "\n",
      "  Average training loss: 1.60\n",
      "  Training epoc h took: 875.2838418483734\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24055\n",
      "  F1: 0.23421\n",
      "  Validation Loss: 1.74911\n",
      "  Validation took: 93.46911263465881\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.218132734298706.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.49049663543701.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.67805862426758.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.90917348861694.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.08667850494385.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.29119086265564.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.55710363388062.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.71794748306274.\n",
      "  Batch   450  of  2,539.    Elapsed: 154.93361616134644.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.11634635925293.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.38102626800537.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.53217148780823.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.6857569217682.\n",
      "  Batch   700  of  2,539.    Elapsed: 240.93396639823914.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.2046504020691.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.4109890460968.\n",
      "  Batch   850  of  2,539.    Elapsed: 292.63205218315125.\n",
      "  Batch   900  of  2,539.    Elapsed: 309.91395711898804.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.1450242996216.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.3105912208557.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.5009996891022.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 378.6823835372925.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 395.86428356170654.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.02560353279114.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.2905945777893.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.46845149993896.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 464.70777583122253.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 481.8627939224243.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.1183178424835.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.349442243576.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 533.6027188301086.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 550.8690893650055.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.0985631942749.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.3635115623474.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 602.642370223999.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 619.9617409706116.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.2675096988678.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.4802119731903.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.7030498981476.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.9485709667206.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.1713283061981.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.3976538181305.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.6138708591461.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 757.7839167118073.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 774.93537068367.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.1266160011292.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.2997117042542.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.4850800037384.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 843.7150557041168.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.9848685264587.\n",
      "\n",
      "  Average training loss: 1.36\n",
      "  Training epoc h took: 874.2468535900116\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24173\n",
      "  F1: 0.23418\n",
      "  Validation Loss: 2.06459\n",
      "  Validation took: 93.53988718986511\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.173540830612183.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.42645311355591.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.63637709617615.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.885413646698.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.12278652191162.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.34333372116089.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.61811518669128.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.91275787353516.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.2187957763672.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.49120450019836.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.78502583503723.\n",
      "  Batch   600  of  2,539.    Elapsed: 207.01762771606445.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.2079975605011.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.48166513442993.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.69412183761597.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.8953354358673.\n",
      "  Batch   850  of  2,539.    Elapsed: 293.1108829975128.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.2894175052643.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.45035123825073.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.61598348617554.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.78532791137695.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 379.0212061405182.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.20831203460693.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.47575545310974.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.68514609336853.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.98023438453674.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 465.1698453426361.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.4287495613098.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.64406061172485.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.8563416004181.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 534.1015543937683.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.3731393814087.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.579530954361.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.7828166484833.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 603.0267672538757.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 620.2369709014893.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.4205510616302.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.7298941612244.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 672.1107366085052.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.3037903308868.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.4633076190948.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.7340190410614.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.919171333313.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 758.1738121509552.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.3963139057159.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.5917282104492.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.8171601295471.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 827.0804283618927.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 844.3644976615906.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.5723385810852.\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epoc h took: 874.7729995250702\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24685\n",
      "  F1: 0.24004\n",
      "  Validation Loss: 2.31283\n",
      "  Validation took: 93.46443247795105\n",
      "\n",
      "Training...\n",
      "----- Epoch 6 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.147011280059814.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.33327531814575.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.576077699661255.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.80544519424438.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.03359889984131.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.27435040473938.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.61184430122375.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.78081965446472.\n",
      "  Batch   450  of  2,539.    Elapsed: 155.00418496131897.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.325599193573.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.53935170173645.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.83805298805237.\n",
      "  Batch   650  of  2,539.    Elapsed: 224.04672360420227.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.29811882972717.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.4398729801178.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.6394639015198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   850  of  2,539.    Elapsed: 292.8903269767761.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.0629482269287.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.2508850097656.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.4425871372223.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.65435814857483.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 378.8284478187561.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.03552770614624.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.23247718811035.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.46280241012573.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.60668778419495.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 464.85051107406616.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.0901689529419.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.2764096260071.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.4534933567047.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 533.6892232894897.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 550.8968191146851.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.084897518158.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.2633333206177.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 602.5811891555786.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 619.8887085914612.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.0805764198303.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.3044457435608.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.4575426578522.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.6249585151672.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 705.8311812877655.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.0361733436584.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.2718849182129.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 757.517094373703.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 774.7467262744904.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 791.9233946800232.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.1162497997284.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.3569061756134.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 843.5444152355194.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.6874296665192.\n",
      "\n",
      "  Average training loss: 0.80\n",
      "  Training epoc h took: 873.969884634018\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24094\n",
      "  F1: 0.23685\n",
      "  Validation Loss: 2.84973\n",
      "  Validation took: 93.48156309127808\n",
      "\n",
      "Training...\n",
      "----- Epoch 7 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.201969385147095.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.47944378852844.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.67059135437012.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.8673448562622.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.07831525802612.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.25691080093384.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.46323823928833.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.69786286354065.\n",
      "  Batch   450  of  2,539.    Elapsed: 154.92127871513367.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.21905255317688.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.41362762451172.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.68001317977905.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.87756204605103.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.1298270225525.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.3184394836426.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.58950304985046.\n",
      "  Batch   850  of  2,539.    Elapsed: 292.8164713382721.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.02940487861633.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.3123550415039.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.55661821365356.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.80522561073303.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 379.0515341758728.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.27131938934326.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.4744589328766.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.66705560684204.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.9181411266327.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 465.13382267951965.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.33391523361206.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.51965522766113.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.7327125072479.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 533.9812772274017.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.261100769043.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.5265214443207.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.6883051395416.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 602.8739564418793.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 620.0465331077576.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.2770304679871.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.4993367195129.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.6823604106903.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.929167509079.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.1242232322693.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.3638458251953.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.6134769916534.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 757.8671133518219.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.085458278656.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.2818360328674.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.5075006484985.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.7546985149384.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 843.906013250351.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.1831426620483.\n",
      "\n",
      "  Average training loss: 0.61\n",
      "  Training epoc h took: 874.3775475025177\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23504\n",
      "  F1: 0.22865\n",
      "  Validation Loss: 3.62678\n",
      "  Validation took: 93.46431636810303\n",
      "\n",
      "Training...\n",
      "----- Epoch 8 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.23170828819275.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.474217891693115.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.663137912750244.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.86879134178162.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.0645170211792.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.3206434249878.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.51405262947083.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.6845941543579.\n",
      "  Batch   450  of  2,539.    Elapsed: 154.92022848129272.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.13565850257874.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.38060522079468.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.6472351551056.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.8216404914856.\n",
      "  Batch   700  of  2,539.    Elapsed: 240.98620176315308.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.17761611938477.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.4127941131592.\n",
      "  Batch   850  of  2,539.    Elapsed: 292.6883330345154.\n",
      "  Batch   900  of  2,539.    Elapsed: 309.86656379699707.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.0960388183594.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.32987546920776.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.52253580093384.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 378.69826006889343.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 395.90980076789856.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.15244340896606.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.345094203949.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.59738397598267.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 464.79916524887085.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.06163811683655.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.25090312957764.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.4670512676239.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 533.7050693035126.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 550.9493608474731.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.1390106678009.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.4252274036407.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 602.577963590622.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 619.8341593742371.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.0732460021973.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.3285267353058.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.4768822193146.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.8070299625397.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.0063207149506.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.2227864265442.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.467809677124.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 757.7566025257111.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 774.9811437129974.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.1301162242889.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.3230106830597.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.6404414176941.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 843.8110501766205.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.0135960578918.\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epoc h took: 874.3041288852692\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.23307\n",
      "  F1: 0.22777\n",
      "  Validation Loss: 4.61566\n",
      "  Validation took: 93.42043662071228\n",
      "\n",
      "Training...\n",
      "----- Epoch 9 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.22549057006836.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.431715965270996.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.620269775390625.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.78500461578369.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.03352928161621.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.21951913833618.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.41086220741272.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.58420300483704.\n",
      "  Batch   450  of  2,539.    Elapsed: 154.7733075618744.\n",
      "  Batch   500  of  2,539.    Elapsed: 171.9531626701355.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.2171356678009.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.51807045936584.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.72701120376587.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.0017695426941.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.16606426239014.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.37993597984314.\n",
      "  Batch   850  of  2,539.    Elapsed: 292.5182101726532.\n",
      "  Batch   900  of  2,539.    Elapsed: 309.71487617492676.\n",
      "  Batch   950  of  2,539.    Elapsed: 326.9290568828583.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.2278757095337.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.5500063896179.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 378.759153842926.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.0257422924042.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.29829359054565.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.50066232681274.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.76097989082336.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 464.9259192943573.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.18823528289795.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.5117304325104.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.7578427791595.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 533.9881839752197.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.2014381885529.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.5267510414124.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.8001697063446.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 603.0369324684143.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 620.2908637523651.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.5315272808075.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.794540643692.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 672.0117037296295.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.2711472511292.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.5707218647003.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.7160649299622.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.9487671852112.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 758.1864967346191.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.3460414409637.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.5869681835175.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.8030178546906.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 827.1353640556335.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 844.3257257938385.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.5348100662231.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoc h took: 874.8214726448059\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.23583\n",
      "  F1: 0.22822\n",
      "  Validation Loss: 5.84098\n",
      "  Validation took: 93.41584181785583\n",
      "\n",
      "Training...\n",
      "----- Epoch 10 / 10 -----\n",
      "  Batch    50  of  2,539.    Elapsed: 17.28123641014099.\n",
      "  Batch   100  of  2,539.    Elapsed: 34.53526043891907.\n",
      "  Batch   150  of  2,539.    Elapsed: 51.757338523864746.\n",
      "  Batch   200  of  2,539.    Elapsed: 68.94752383232117.\n",
      "  Batch   250  of  2,539.    Elapsed: 86.21656274795532.\n",
      "  Batch   300  of  2,539.    Elapsed: 103.44024014472961.\n",
      "  Batch   350  of  2,539.    Elapsed: 120.6424491405487.\n",
      "  Batch   400  of  2,539.    Elapsed: 137.83728003501892.\n",
      "  Batch   450  of  2,539.    Elapsed: 154.99919152259827.\n",
      "  Batch   500  of  2,539.    Elapsed: 172.18993973731995.\n",
      "  Batch   550  of  2,539.    Elapsed: 189.3977279663086.\n",
      "  Batch   600  of  2,539.    Elapsed: 206.568359375.\n",
      "  Batch   650  of  2,539.    Elapsed: 223.82895016670227.\n",
      "  Batch   700  of  2,539.    Elapsed: 241.02302145957947.\n",
      "  Batch   750  of  2,539.    Elapsed: 258.2571232318878.\n",
      "  Batch   800  of  2,539.    Elapsed: 275.4761815071106.\n",
      "  Batch   850  of  2,539.    Elapsed: 292.69597721099854.\n",
      "  Batch   900  of  2,539.    Elapsed: 310.00011825561523.\n",
      "  Batch   950  of  2,539.    Elapsed: 327.26359462738037.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.4587013721466.\n",
      "  Batch 1,050  of  2,539.    Elapsed: 361.72557163238525.\n",
      "  Batch 1,100  of  2,539.    Elapsed: 378.98573064804077.\n",
      "  Batch 1,150  of  2,539.    Elapsed: 396.2118294239044.\n",
      "  Batch 1,200  of  2,539.    Elapsed: 413.5293080806732.\n",
      "  Batch 1,250  of  2,539.    Elapsed: 430.7309019565582.\n",
      "  Batch 1,300  of  2,539.    Elapsed: 447.946240901947.\n",
      "  Batch 1,350  of  2,539.    Elapsed: 465.1476209163666.\n",
      "  Batch 1,400  of  2,539.    Elapsed: 482.38345527648926.\n",
      "  Batch 1,450  of  2,539.    Elapsed: 499.5666732788086.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.8324649333954.\n",
      "  Batch 1,550  of  2,539.    Elapsed: 534.0374839305878.\n",
      "  Batch 1,600  of  2,539.    Elapsed: 551.2711000442505.\n",
      "  Batch 1,650  of  2,539.    Elapsed: 568.5186483860016.\n",
      "  Batch 1,700  of  2,539.    Elapsed: 585.6983299255371.\n",
      "  Batch 1,750  of  2,539.    Elapsed: 602.9752581119537.\n",
      "  Batch 1,800  of  2,539.    Elapsed: 620.2345342636108.\n",
      "  Batch 1,850  of  2,539.    Elapsed: 637.4628012180328.\n",
      "  Batch 1,900  of  2,539.    Elapsed: 654.7012901306152.\n",
      "  Batch 1,950  of  2,539.    Elapsed: 671.9070370197296.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.1178081035614.\n",
      "  Batch 2,050  of  2,539.    Elapsed: 706.2863037586212.\n",
      "  Batch 2,100  of  2,539.    Elapsed: 723.4480333328247.\n",
      "  Batch 2,150  of  2,539.    Elapsed: 740.7740516662598.\n",
      "  Batch 2,200  of  2,539.    Elapsed: 757.9817183017731.\n",
      "  Batch 2,250  of  2,539.    Elapsed: 775.151739358902.\n",
      "  Batch 2,300  of  2,539.    Elapsed: 792.4242498874664.\n",
      "  Batch 2,350  of  2,539.    Elapsed: 809.6832082271576.\n",
      "  Batch 2,400  of  2,539.    Elapsed: 826.925564289093.\n",
      "  Batch 2,450  of  2,539.    Elapsed: 844.1842436790466.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.4888427257538.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoc h took: 874.6947734355927\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.24724\n",
      "  F1: 0.24458\n",
      "  Validation Loss: 6.26673\n",
      "  Validation took: 93.47698283195496\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b180fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./models/bert_{CUR_DATASET}_regexp_stopwords_{if_stopwords}_lemmatization_{if_lemmatize}_multiclass_SJ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
