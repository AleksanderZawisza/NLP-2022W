{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e31007",
   "metadata": {},
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ac1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a41d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1cbe4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5205880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DATASET = \"LIAR-PLUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd79374",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f'../data/{CUR_DATASET}/train2.tsv', sep='\\t', header = None)\n",
    "valid_dataset = pd.read_csv(f'../data/{CUR_DATASET}/val2.tsv', sep='\\t', header = None)\n",
    "test_dataset = pd.read_csv(f'../data/{CUR_DATASET}/test2.tsv', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0fec590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train_dataset.iloc[:, [2, 3, 15]]\n",
    "train = train.rename(columns = {2: 'label', 3: 'statements', 15: 'justification'})\n",
    "\n",
    "val = valid_dataset.iloc[:, [2, 3, 15]]\n",
    "val = val.rename(columns = {2: 'label', 3: 'statements', 15: 'justification'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9953dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset['label'] = dataset['label'].replace({\n",
    "        'false' : 0,\n",
    "        'barely-true' : 0,\n",
    "        'pants-fire' : 0,\n",
    "        'half-true' : 1,\n",
    "        'mostly-true' : 1,\n",
    "        'true' : 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a9f3",
   "metadata": {},
   "source": [
    "# Some More EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d1480d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label             2\n",
       "statements        2\n",
       "justification    88\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c24341",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset = dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9302f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train[\"label\"].values.astype(int)\n",
    "val_labels = val[\"label\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf746345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011876e",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ee421",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe069c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4346120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddeb4e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9bea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31cfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    # \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd2a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7276370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in [train, val]:\n",
    "    dataset[\"statements\"] = dataset[\"statements\"].apply(preprocess_text)\n",
    "    dataset[\"justification\"] = dataset[\"justification\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25112b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    for dataset in [train, val]:\n",
    "        for col in [\"statements\", \"justification\"]:\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6292eec",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b4247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e159b0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marneusz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for dataset in [train, val]:\n",
    "        for col in [\"statements\", \"justification\"]:\n",
    "            dataset[col] = dataset[col].str.lower().str.replace(\"’\", \"'\")\n",
    "            dataset[col] = dataset[col].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "849ade27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (train[\"statements\"] + \" \" + train[\"justification\"]).values\n",
    "val_text = (val[\"statements\"] + \" \" + val[\"justification\"]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e8bb8",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eebdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2a2ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marneusz/anaconda3/envs/nlp-transformers/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d41e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fada532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17dcf749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529662e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6a38f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = len(np.unique(train_labels)), \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d8b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/10154 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2089 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10154/10154 [00:04<00:00, 2066.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  2089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "len_limit = 512\n",
    "LIMIT = 100_000\n",
    "\n",
    "indices = []\n",
    "train_text_filtered = []\n",
    "\n",
    "for i, text in enumerate(tqdm(train_text)):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) <= LIMIT:\n",
    "        train_text_filtered.append(text)\n",
    "        indices.append(i)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94276bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10154])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_filtered = train_labels[indices]\n",
    "# labels_filtered.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46221860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1/notebook\n",
    "\n",
    "def tokenize_map(sentence, labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in tqdm(sentence):\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = len_limit,           # Max length according to our text data.\n",
    "                            padding = 'max_length', # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae808334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_filtered = np.array(train_text)\n",
    "val_text_filtered = np.array(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deaf7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10154,), (10154,), (1280,), (1280,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape, train_text_filtered.shape, val_text.shape, val_text_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd4cf520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10154/10154 [00:05<00:00, 1785.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1861.35it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks = tokenize_map(train_text_filtered)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_input_ids, val_attention_masks = tokenize_map(val_text_filtered)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# test_input_ids, test_attention_masks= tokenize_map(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a58e",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745e5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34763ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f55197e6e90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "transformers.set_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d21142e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca257209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10154]), torch.Size([10154, 512]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c48fb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids, attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a427a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# Validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader\n",
    "\n",
    "# test_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6152d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23503732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edf00b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate\n",
    "                  # eps = 1e-8 # args.adam_epsilon\n",
    "            )\n",
    "\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ee0f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "total_num_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae540af7",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c29ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1_score(predictions, labels):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, pred_flat, zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf381a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f67393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e9f7c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----- Epoch 1 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 175.45674347877502.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 349.02380657196045.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 523.5164775848389.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 697.1212501525879.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 871.1205580234528.\n",
      "\n",
      "  Average training loss: 0.68\n",
      "  Training epoc h took: 884.4071106910706\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61406\n",
      "  F1: 0.61682\n",
      "  Validation Loss: 0.67573\n",
      "  Validation took: 47.516018867492676\n",
      "\n",
      "Training...\n",
      "----- Epoch 2 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 175.65511393547058.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 353.0663757324219.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 529.5510084629059.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 701.7482740879059.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 874.0486047267914.\n",
      "\n",
      "  Average training loss: 0.64\n",
      "  Training epoc h took: 887.2981009483337\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.62187\n",
      "  F1: 0.60906\n",
      "  Validation Loss: 0.67344\n",
      "  Validation took: 47.14480757713318\n",
      "\n",
      "Training...\n",
      "----- Epoch 3 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.16059160232544.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.15039682388306.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.2519862651825.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.3342113494873.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.7773234844208.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoc h took: 874.0555539131165\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60938\n",
      "  F1: 0.61104\n",
      "  Validation Loss: 0.84911\n",
      "  Validation took: 47.11613368988037\n",
      "\n",
      "Training...\n",
      "----- Epoch 4 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 171.87466406822205.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 343.71831583976746.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.0480127334595.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.1762914657593.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.2814128398895.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoc h took: 873.5710837841034\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.62344\n",
      "  F1: 0.58466\n",
      "  Validation Loss: 1.34360\n",
      "  Validation took: 47.09685254096985\n",
      "\n",
      "Training...\n",
      "----- Epoch 5 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.08957624435425.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.22773790359497.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.3372066020966.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.5981004238129.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.572015285492.\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epoc h took: 873.8306398391724\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60703\n",
      "  F1: 0.60382\n",
      "  Validation Loss: 1.77001\n",
      "  Validation took: 47.09807729721069\n",
      "\n",
      "Training...\n",
      "----- Epoch 6 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.23374199867249.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.2237739562988.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.2654075622559.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.1243886947632.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.3332073688507.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoc h took: 873.6070711612701\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61016\n",
      "  F1: 0.60760\n",
      "  Validation Loss: 1.91955\n",
      "  Validation took: 47.09052109718323\n",
      "\n",
      "Training...\n",
      "----- Epoch 7 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 171.9664216041565.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.3292672634125.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.5223743915558.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.680097579956.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.9134531021118.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoc h took: 874.1855711936951\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.59609\n",
      "  F1: 0.60326\n",
      "  Validation Loss: 2.26633\n",
      "  Validation took: 47.102250814437866\n",
      "\n",
      "Training...\n",
      "----- Epoch 8 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 173.74396896362305.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.71182560920715.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.788932800293.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.7599716186523.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.932047367096.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoc h took: 875.2611773014069\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61562\n",
      "  F1: 0.59507\n",
      "  Validation Loss: 2.28170\n",
      "  Validation took: 47.12735843658447\n",
      "\n",
      "Training...\n",
      "----- Epoch 9 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.2224624156952.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.4113414287567.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.2431924343109.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.3556513786316.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.3538792133331.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoc h took: 873.636696100235\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60781\n",
      "  F1: 0.58647\n",
      "  Validation Loss: 2.38738\n",
      "  Validation took: 47.13043522834778\n",
      "\n",
      "Training...\n",
      "----- Epoch 10 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.06367945671082.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.27571272850037.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.3639326095581.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.4203689098358.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.621354341507.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoc h took: 873.884037733078\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61094\n",
      "  F1: 0.58244\n",
      "  Validation Loss: 2.48303\n",
      "  Validation took: 47.13246393203735\n",
      "\n",
      "Training...\n",
      "----- Epoch 11 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.15728402137756.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.29849791526794.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.4090991020203.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.762261390686.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.0416221618652.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoc h took: 874.3374154567719\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60000\n",
      "  F1: 0.59414\n",
      "  Validation Loss: 2.56159\n",
      "  Validation took: 47.16621470451355\n",
      "\n",
      "Training...\n",
      "----- Epoch 12 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.46717977523804.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.62838435173035.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.9343631267548.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.2144198417664.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.4761850833893.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoc h took: 874.7991900444031\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60703\n",
      "  F1: 0.58634\n",
      "  Validation Loss: 2.57558\n",
      "  Validation took: 47.70655536651611\n",
      "\n",
      "Training...\n",
      "----- Epoch 13 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.32593059539795.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.61399936676025.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.8330857753754.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.7756662368774.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.4372141361237.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoc h took: 874.7064890861511\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61484\n",
      "  F1: 0.58783\n",
      "  Validation Loss: 2.57485\n",
      "  Validation took: 47.14174842834473\n",
      "\n",
      "Training...\n",
      "----- Epoch 14 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.3009648323059.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.50411200523376.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.69655418396.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.8524761199951.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.1143090724945.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoc h took: 874.5270340442657\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60469\n",
      "  F1: 0.58975\n",
      "  Validation Loss: 2.62701\n",
      "  Validation took: 47.171974420547485\n",
      "\n",
      "Training...\n",
      "----- Epoch 15 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.38294291496277.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.8707399368286.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 517.2358329296112.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 689.8718070983887.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 862.502114534378.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoc h took: 875.8472137451172\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60781\n",
      "  F1: 0.58790\n",
      "  Validation Loss: 2.63453\n",
      "  Validation took: 47.226134061813354\n",
      "\n",
      "Training...\n",
      "----- Epoch 16 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.66000366210938.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 345.35785460472107.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 518.0960268974304.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 690.633297920227.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 863.0776824951172.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoc h took: 876.3368775844574\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.60703\n",
      "  F1: 0.58728\n",
      "  Validation Loss: 2.64544\n",
      "  Validation took: 47.14756917953491\n",
      "\n",
      "Training...\n",
      "----- Epoch 17 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.1030035018921.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.20143008232117.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.3800897598267.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.7685272693634.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 861.0287554264069.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoc h took: 874.3484251499176\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60781\n",
      "  F1: 0.58790\n",
      "  Validation Loss: 2.65395\n",
      "  Validation took: 47.11502647399902\n",
      "\n",
      "Training...\n",
      "----- Epoch 18 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.2247109413147.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.41288709640503.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.6125838756561.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.7442991733551.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.8153083324432.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoc h took: 874.1051504611969\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60938\n",
      "  F1: 0.58863\n",
      "  Validation Loss: 2.66066\n",
      "  Validation took: 47.10652565956116\n",
      "\n",
      "Training...\n",
      "----- Epoch 19 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.0677409172058.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.3586103916168.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.2838385105133.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.4825754165649.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.835132598877.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoc h took: 874.1211934089661\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.60938\n",
      "  F1: 0.58863\n",
      "  Validation Loss: 2.66666\n",
      "  Validation took: 47.101133823394775\n",
      "\n",
      "Training...\n",
      "----- Epoch 20 / 20 -----\n",
      "  Batch   500  of  2,539.    Elapsed: 172.23602437973022.\n",
      "  Batch 1,000  of  2,539.    Elapsed: 344.20594096183777.\n",
      "  Batch 1,500  of  2,539.    Elapsed: 516.2585444450378.\n",
      "  Batch 2,000  of  2,539.    Elapsed: 688.3464233875275.\n",
      "  Batch 2,500  of  2,539.    Elapsed: 860.2389883995056.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoc h took: 873.4939610958099\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.61016\n",
      "  F1: 0.58853\n",
      "  Validation Loss: 2.66862\n",
      "  Validation took: 47.09238123893738\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "validations_labels_ep = []\n",
    "actual_labels_ep = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "for i in range(0, num_epochs):\n",
    "    print('')\n",
    "    print('Training...')\n",
    "    print('----- Epoch {:} / {:} -----'.format(i + 1, num_epochs))\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels)[0]\n",
    "        logits = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)[1]\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epoc h took: {:}'.format(training_time))\n",
    "    \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[0]\n",
    "\n",
    "            logits = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1_score(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.5f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.5f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = time.time() - t0\n",
    "    \n",
    "    print('  Validation Loss: {0:.5f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b180fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./models/bert_{CUR_DATASET}_regexp_stopwords_{if_stopwords}_lemmatization_{if_lemmatize}_binary_SJ_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-transformers] *",
   "language": "python",
   "name": "conda-env-nlp-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
