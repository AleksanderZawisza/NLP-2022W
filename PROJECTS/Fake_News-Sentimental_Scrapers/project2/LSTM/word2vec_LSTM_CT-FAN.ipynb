{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences, to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")  # Punkt Sentence Tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # Part of Speech Tagger\n",
    "nltk.download(\"wordnet\")  # a lexical database of English; useful for synonyms, hyponyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after', 'himself', \"you'll\", 'of', 't', 'haven', 'won', 'the', 'to', 'is']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "random.sample(stopwords.words('english'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = pd.read_csv('../data/CT-FAN/Task3_english_dev.csv')\n",
    "data_train = pd.read_csv('../data/CT-FAN/Task3_english_training.csv')\n",
    "data_test = pd.read_csv('../data/CT-FAN/English_data_test_release_with_rating.csv')\n",
    "data_concat = pd.concat([data_train, data_dev])\n",
    "data_concat.rename(columns={'our rating':'label'}, inplace=True)\n",
    "data_test.rename(columns={'our rating':'label'}, inplace=True)\n",
    "data_concat['label'] = data_concat['label'].apply(lambda x: x.lower())\n",
    "data_test['label'] = data_test['label'].apply(lambda x: x.lower())\n",
    "\n",
    "train_dataset = data_concat\n",
    "test_dataset = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>Distracted driving causes more deaths in Canad...</td>\n",
       "      <td>You Can Be Fined $1,500 If Your Passenger Is U...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>Missouri politicians have made statements afte...</td>\n",
       "      <td>Missouri lawmakers condemn Las Vegas shooting</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>Home Alone 2: Lost in New York is full of viol...</td>\n",
       "      <td>CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>But things took a turn for the worse when riot...</td>\n",
       "      <td>Obama’s Daughters Caught on Camera Burning US ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>It’s no secret that Epstein and Schiff share a...</td>\n",
       "      <td>Leaked Visitor Logs Reveal Schiff’s 78 Visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  public_id                                               text  \\\n",
       "0  5a228e0e  Distracted driving causes more deaths in Canad...   \n",
       "1  30c605a1  Missouri politicians have made statements afte...   \n",
       "2  c3dea290  Home Alone 2: Lost in New York is full of viol...   \n",
       "3  f14e8eb6  But things took a turn for the worse when riot...   \n",
       "4  faf024d6  It’s no secret that Epstein and Schiff share a...   \n",
       "\n",
       "                                               title            label  \n",
       "0  You Can Be Fined $1,500 If Your Passenger Is U...            false  \n",
       "1      Missouri lawmakers condemn Las Vegas shooting  partially false  \n",
       "2  CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...  partially false  \n",
       "3  Obama’s Daughters Caught on Camera Burning US ...            false  \n",
       "4  Leaked Visitor Logs Reveal Schiff’s 78 Visits ...            false  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.fillna(\"null data\")\n",
    "test_dataset = test_dataset.fillna(\"null data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_stopwords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_stopwords:\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    test_dataset[\"title\"] = test_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    test_dataset[\"title\"] = test_dataset[\"title\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    test_dataset[\"text\"] = test_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    test_dataset[\"text\"] = test_dataset[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>distracted driving causes deaths canada impair...</td>\n",
       "      <td>fined $1,500 passenger using mobile phone, sta...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>missouri politicians made statements mass shoo...</td>\n",
       "      <td>missouri lawmakers condemn las vegas shooting</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>home alone 2: lost new york full violence that...</td>\n",
       "      <td>cbc cuts donald trump's 'home alone 2' cameo b...</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>things took turn worse riot police fired tear ...</td>\n",
       "      <td>obama's daughters caught camera burning us fla...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>secret epstein schiff share long history perve...</td>\n",
       "      <td>leaked visitor logs reveal schiff's 78 visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  public_id                                               text  \\\n",
       "0  5a228e0e  distracted driving causes deaths canada impair...   \n",
       "1  30c605a1  missouri politicians made statements mass shoo...   \n",
       "2  c3dea290  home alone 2: lost new york full violence that...   \n",
       "3  f14e8eb6  things took turn worse riot police fired tear ...   \n",
       "4  faf024d6  secret epstein schiff share long history perve...   \n",
       "\n",
       "                                               title            label  \n",
       "0  fined $1,500 passenger using mobile phone, sta...            false  \n",
       "1      missouri lawmakers condemn las vegas shooting  partially false  \n",
       "2  cbc cuts donald trump's 'home alone 2' cameo b...  partially false  \n",
       "3  obama's daughters caught camera burning us fla...            false  \n",
       "4  leaked visitor logs reveal schiff's 78 visits ...            false  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_text_fn = {\n",
    "    \"no_punctuation\": lambda txt: re.sub(r'[^\\w\\s]','', txt),\n",
    "    \"no_special_symbols\": lambda txt: re.sub('[$,#,&]', '', txt),\n",
    "    \"no_digits\": lambda txt: re.sub('\\d*', '', txt),\n",
    "    \"no_www\": lambda txt: re.sub('w{3}', '', txt),\n",
    "    \"no_urls\": lambda txt: re.sub('http\\S+', '', txt),\n",
    "    \"no_spaces\": lambda txt: re.sub('\\s+', ' ', txt),\n",
    "    \"no_single_chars\": lambda txt: re.sub(r'\\s+[a-zA-Z]\\s+', '', txt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, pipeline = preprocessing_text_fn):\n",
    "    text = str(text)\n",
    "    for fn in pipeline.keys():\n",
    "        text = pipeline[fn](text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hers',\n",
       " 'with',\n",
       " 'then',\n",
       " 'further',\n",
       " 'or',\n",
       " 'they',\n",
       " 'those',\n",
       " 'why',\n",
       " 'if',\n",
       " 'shan',\n",
       " 'of',\n",
       " 'yours',\n",
       " 'her',\n",
       " 'haven',\n",
       " 's',\n",
       " \"it's\",\n",
       " \"mustn't\",\n",
       " 'shouldn',\n",
       " 'up',\n",
       " 'on']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = [preprocessing_text_fn[\"no_punctuation\"](word) for word in stop_words]\n",
    "random.sample(stop_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_stopwords(text, stop_words=STOP_WORDS):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sequence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "    return filtered_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>distracted driving causes deaths canada impair...</td>\n",
       "      <td>fined passenger using mobile phone starting ne...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>missouri politicians made statements mass shoo...</td>\n",
       "      <td>missouri lawmakers condemn las vegas shooting</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>home alone lost new york full violence that op...</td>\n",
       "      <td>cbc cuts donald trumps home alone cameo broadcast</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>things took turn worse riot police fired tear ...</td>\n",
       "      <td>obamas daughters caught camera burning us flag...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>secret epstein schiff share long history perve...</td>\n",
       "      <td>leaked visitor logs reveal schiffs visits epst...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c03ed5db</td>\n",
       "      <td>nation updated pmbgovernor secretary state geo...</td>\n",
       "      <td>km governor secretary state georgia took money...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61bd9a69</td>\n",
       "      <td>november us food drug administration fda publi...</td>\n",
       "      <td>fda shocking study cells used vaccines contami...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bb1999cc</td>\n",
       "      <td>trump confirms bombing accident immediately sc...</td>\n",
       "      <td>israel hits beirut nuclear missile trump leban...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c1dc1ac6</td>\n",
       "      <td>show antiamerican sentiment surprise precisely...</td>\n",
       "      <td>obamas daughters caught camera burning us flag...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f2182a54</td>\n",
       "      <td>fema camps portable human cages realall it scr...</td>\n",
       "      <td>fields human cages discovered caruthers califo...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  public_id                                               text  \\\n",
       "0  5a228e0e  distracted driving causes deaths canada impair...   \n",
       "1  30c605a1  missouri politicians made statements mass shoo...   \n",
       "2  c3dea290  home alone lost new york full violence that op...   \n",
       "3  f14e8eb6  things took turn worse riot police fired tear ...   \n",
       "4  faf024d6  secret epstein schiff share long history perve...   \n",
       "5  c03ed5db  nation updated pmbgovernor secretary state geo...   \n",
       "6  61bd9a69  november us food drug administration fda publi...   \n",
       "7  bb1999cc  trump confirms bombing accident immediately sc...   \n",
       "8  c1dc1ac6  show antiamerican sentiment surprise precisely...   \n",
       "9  f2182a54  fema camps portable human cages realall it scr...   \n",
       "\n",
       "                                               title            label  \n",
       "0  fined passenger using mobile phone starting ne...            false  \n",
       "1      missouri lawmakers condemn las vegas shooting  partially false  \n",
       "2  cbc cuts donald trumps home alone cameo broadcast  partially false  \n",
       "3  obamas daughters caught camera burning us flag...            false  \n",
       "4  leaked visitor logs reveal schiffs visits epst...            false  \n",
       "5  km governor secretary state georgia took money...            other  \n",
       "6  fda shocking study cells used vaccines contami...            false  \n",
       "7  israel hits beirut nuclear missile trump leban...            false  \n",
       "8  obamas daughters caught camera burning us flag...            false  \n",
       "9  fields human cages discovered caruthers califo...            false  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"title\"] = train_dataset[\"title\"].apply(preprocess_text)\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(preprocess_text)\n",
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122653045997905671927713471889615536378</td>\n",
       "      <td>deputy secretary us treasury said way end plag...</td>\n",
       "      <td>us treasury deputy sec warns shortages likely ...</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275389285957305997321446227088442471741</td>\n",
       "      <td>kabulapproximately twelve minutes us troops wi...</td>\n",
       "      <td>cnn praises taliban wearing masks attack</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>333248764296609831067233855420575814716</td>\n",
       "      <td>vast majority oblivious americans dismissed co...</td>\n",
       "      <td>tennessee legalized government covid kidnappin...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264019763253447756851916399533799891538</td>\n",
       "      <td>natural news theres secret layer information c...</td>\n",
       "      <td>medical shocker scientists sloan kettering dis...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158073737187690682830899773280916034317</td>\n",
       "      <td>recent study reported ncbi national institutes...</td>\n",
       "      <td>study results facemasks ineffective block tran...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>104668593793347563618573389721561849271</td>\n",
       "      <td>ttav experiencing heavy censorship many social...</td>\n",
       "      <td>neutralize potential damage mrna vaccines</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>285745438442578180936789861954628585938</td>\n",
       "      <td>one people hesitant receive covid vaccine know...</td>\n",
       "      <td>cdc quietly admits death toll covid vaccines g...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>142500044244587305036676237978793639344</td>\n",
       "      <td>us deaths related vaccines less months entire ...</td>\n",
       "      <td>exclusive per cdc nearly twice many vaccine re...</td>\n",
       "      <td>partially false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240988184127263059916419185466606679317</td>\n",
       "      <td>delta covid variant currently rampant united k...</td>\n",
       "      <td>fully vaccinated people higher chance death du...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>212759982338406281914383909445151155412</td>\n",
       "      <td>natural news via words cdcs director dr rochel...</td>\n",
       "      <td>cdc confesses vaccines failing vaxxed superspr...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID  \\\n",
       "0  122653045997905671927713471889615536378   \n",
       "1  275389285957305997321446227088442471741   \n",
       "2  333248764296609831067233855420575814716   \n",
       "3  264019763253447756851916399533799891538   \n",
       "4  158073737187690682830899773280916034317   \n",
       "5  104668593793347563618573389721561849271   \n",
       "6  285745438442578180936789861954628585938   \n",
       "7  142500044244587305036676237978793639344   \n",
       "8  240988184127263059916419185466606679317   \n",
       "9  212759982338406281914383909445151155412   \n",
       "\n",
       "                                                text  \\\n",
       "0  deputy secretary us treasury said way end plag...   \n",
       "1  kabulapproximately twelve minutes us troops wi...   \n",
       "2  vast majority oblivious americans dismissed co...   \n",
       "3  natural news theres secret layer information c...   \n",
       "4  recent study reported ncbi national institutes...   \n",
       "5  ttav experiencing heavy censorship many social...   \n",
       "6  one people hesitant receive covid vaccine know...   \n",
       "7  us deaths related vaccines less months entire ...   \n",
       "8  delta covid variant currently rampant united k...   \n",
       "9  natural news via words cdcs director dr rochel...   \n",
       "\n",
       "                                               title            label  \n",
       "0  us treasury deputy sec warns shortages likely ...  partially false  \n",
       "1           cnn praises taliban wearing masks attack            other  \n",
       "2  tennessee legalized government covid kidnappin...            false  \n",
       "3  medical shocker scientists sloan kettering dis...            false  \n",
       "4  study results facemasks ineffective block tran...            false  \n",
       "5          neutralize potential damage mrna vaccines            false  \n",
       "6  cdc quietly admits death toll covid vaccines g...            false  \n",
       "7  exclusive per cdc nearly twice many vaccine re...  partially false  \n",
       "8  fully vaccinated people higher chance death du...            false  \n",
       "9  cdc confesses vaccines failing vaxxed superspr...            false  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"title\"] = test_dataset[\"title\"].apply(preprocess_text)\n",
    "test_dataset[\"text\"] = test_dataset[\"text\"].apply(preprocess_text)\n",
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MondayPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if if_lemmatize:\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"title\"] = train_dataset[\"title\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    test_dataset[\"title\"] = test_dataset[\"title\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    test_dataset[\"title\"] = test_dataset[\"title\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n",
    "    test_dataset[\"text\"] = test_dataset[\"text\"].str.lower().str.replace(\"’\", \"'\")\n",
    "    test_dataset[\"text\"] = test_dataset[\"text\"].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in word_tokenize(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = (train_dataset['title'] + \" \" + train_dataset['text']).values\n",
    "test_text = (test_dataset['title'] + \" \" + test_dataset['text']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset['label'].values\n",
    "test_labels = test_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "train_labels = to_categorical(train_labels, 4)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "test_labels = to_categorical(test_labels, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit length of each article\n",
    "max_length = 3000\n",
    "lengths = np.array([len(x) for x in train_text])\n",
    "train_text = train_text[lengths < max_length]\n",
    "train_labels = train_labels[lengths < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check actual max length of an article\n",
    "article_length = max(np.array([len(x) for x in train_text]))\n",
    "article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec model with vector size = 100\n",
    "vec_size = 100\n",
    "\n",
    "# workers - number of CPU threads\n",
    "word_model = gensim.models.Word2Vec(train_text, vector_size = vec_size, window = 5, workers = 12)\n",
    "word_model.train(train_text, epochs = 10, total_examples = len(train_text))\n",
    "wv = word_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "encoded_articles = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_articles = pad_sequences(encoded_articles, maxlen = article_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2997)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros(shape=(vocabulary_size, vec_size))\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    ind = wv.has_index_for(w)\n",
    "    if ind:\n",
    "        emb_matrix[i] = wv.get_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_articles, train_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim = vocabulary_size, \n",
    "                    output_dim = vec_size,\n",
    "                    input_length = article_length,\n",
    "                    embeddings_initializer = Constant(emb_matrix))\n",
    "         )\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2997, 100)         1713300   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2997, 16)          7488      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 47952)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1534496   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,255,584\n",
      "Trainable params: 3,255,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def keras_f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(),loss=CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 5s 102ms/step - loss: 1.3223 - accuracy: 0.4000 - val_loss: 1.3292 - val_accuracy: 0.5150\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.2416 - accuracy: 0.4800 - val_loss: 1.1190 - val_accuracy: 0.5350\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.9627 - accuracy: 0.6000 - val_loss: 1.0434 - val_accuracy: 0.5900\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.8085 - accuracy: 0.6950 - val_loss: 1.0594 - val_accuracy: 0.5900\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.7699 - accuracy: 0.6850 - val_loss: 1.0708 - val_accuracy: 0.6100\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.7007 - accuracy: 0.7333 - val_loss: 1.0317 - val_accuracy: 0.6150\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.5858 - accuracy: 0.7850 - val_loss: 1.0229 - val_accuracy: 0.6150\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.5090 - accuracy: 0.8217 - val_loss: 1.1533 - val_accuracy: 0.6250\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.4242 - accuracy: 0.8150 - val_loss: 1.1439 - val_accuracy: 0.5800\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.3911 - accuracy: 0.8567 - val_loss: 1.2906 - val_accuracy: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec9f32ec48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "train_pred = np.argmax(model.predict(x_train), axis=1)\n",
    "train_truth = np.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866666666666667"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(train_truth, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166415923113651"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balanced accuracy\n",
    "balanced_accuracy_score(train_pred, train_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit length of each test article\n",
    "lengths = np.array([len(x) for x in test_text])\n",
    "test_text = test_text[lengths <= article_length]\n",
    "test_labels = test_labels[lengths <= article_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_articles = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_articles = pad_sequences(encoded_test_articles, maxlen = article_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = np.argmax(model.predict(padded_test_articles), axis=1)\n",
    "test_truth = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4581005586592179"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(test_pred, test_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31610367984775567"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balanced accuracy\n",
    "balanced_accuracy_score(test_truth, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
