{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"./data/tvtropes_books\"\n",
    "\n",
    "for fname in (f\"tvtropes_books-{suffix}.json\" for suffix in [\"train\", \"test\", \"val\"]):\n",
    "    assert (Path(DATA_PATH) / fname).is_file(), f\"File {fname} not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, val_list, test_list = [], [], []\n",
    "for path, which in ((Path(DATA_PATH) / f\"tvtropes_books-{suffix}.json\", suffix) for suffix in [\"train\", \"test\", \"val\"]):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            globals()[f\"{which}_list\"].append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_with_position(str_):\n",
    "    word_pos_list = []\n",
    "    for m in re.finditer(r'\\S+', str_):\n",
    "        pos, word = m.span(), m.group()\n",
    "        word_pos_list.append((word, pos))\n",
    "    return word_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    return_pos=True\n",
    "):\n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    if return_pos:\n",
    "        return tuple(zip(*[(word, pos) for word, pos in split_with_position(input_text) if word]))\n",
    "    else:\n",
    "        return tuple(word for word, pos in split_with_position(input_text) if word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(interval_1, interval_2):\n",
    "    assert interval_1[0] <= interval_1[1] and interval_2[0] <= interval_2[1]\n",
    "    return interval_1[0] >= interval_2[0] and interval_1[1] <= interval_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_list):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for data in data_list:\n",
    "        if data[\"has_spoiler\"]:\n",
    "            sentence_data = data[\"sentences\"]\n",
    "            i = 0\n",
    "            while i < len(sentence_data):\n",
    "                # i points to the sentence to process\n",
    "                input_words_list = []\n",
    "                input_labels_list = []\n",
    "                cur_words_count = 0\n",
    "\n",
    "                while i < len(sentence_data):\n",
    "                    next_sentence_words, next_sentence_word_positions = text_to_word_sequence(sentence_data[i][1])\n",
    "                    if cur_words_count + len(next_sentence_words) > DIM:\n",
    "                        if len(next_sentence_words) > DIM:\n",
    "                            i += 1\n",
    "                        break\n",
    "                    cur_words_count += len(next_sentence_words)\n",
    "                    input_words_list.extend(next_sentence_words)\n",
    "                    input_labels_list.extend(any(in_range(pos, spoiler_boundary) for spoiler_boundary in sentence_data[i][2]) for pos in next_sentence_word_positions)\n",
    "                    i += 1\n",
    "\n",
    "                if input_words_list:\n",
    "                    X_list.append(input_words_list)\n",
    "                    y_list.append(input_labels_list)\n",
    "    X_list = [\" \".join(s) for s in X_list]\n",
    "    return X_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list, y_train_list = prepare_dataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list, y_val_list = prepare_dataset(val_list)\n",
    "X_test_list, y_test_list = prepare_dataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 19:01:01.909444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 19:01:02.694525: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 19:01:02.694601: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-19 19:01:02.694608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "# Preprocessed_reviews contains all the cleaned reviews.\n",
    "t.fit_on_texts(X_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "X_train = sequence.pad_sequences(t.texts_to_sequences(X_train_list), maxlen=DIM, padding='post')\n",
    "y_train = np.expand_dims(sequence.pad_sequences(y_train_list, maxlen=DIM, padding='post'), axis=-1)\n",
    "\n",
    "X_val = sequence.pad_sequences(t.texts_to_sequences(X_val_list), maxlen=DIM, padding='post')\n",
    "y_val = np.expand_dims(sequence.pad_sequences(y_val_list, maxlen=DIM, padding='post'), axis=-1)\n",
    "\n",
    "X_test = sequence.pad_sequences(t.texts_to_sequences(X_test_list), maxlen=DIM, padding='post')\n",
    "y_test = np.expand_dims(sequence.pad_sequences(y_test_list, maxlen=DIM, padding='post'), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    EMBEDDING_DIM = coefs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((X_train.max()+1, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, Bidirectional, Attention, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary accuracy dziala dobrze, bo testowałem - PW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardSimilarity(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.metric: tf.keras.metrics.Metric = tf.keras.metrics.IoU(**kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.greater(y_pred, 0.5)\n",
    "        self.metric.update_state(y_true, y_pred, sample_weight=sample_weight) # Dzięki temu, że tu mamy sample_weight, to metryka wspiera masking\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobienstwo Jaccarda gdy ground truth dla danego labela jest zbiorem pustym będzie 0. Jednak średnie podobieństwo poprawnie ignoruje taki wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 19:01:13.414095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.420170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.420365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.420889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 19:01:13.421568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.421699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.421808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.802228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.802389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.802505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 19:01:13.802593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4118 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 512, 100)     8765500     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 512, 512)     731136      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 512, 512)    1574912     ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 512, 512)     0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 512, 512)    1574912     ['attention[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512, 512)     0           ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512, 1)       513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,646,973\n",
      "Trainable params: 3,881,473\n",
      "Non-trainable params: 8,765,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##LSTM\n",
    "##fixing numpy RS\n",
    "np.random.seed(0)\n",
    "##fixing tensorflow RS\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "HIDDEN_DIM= 256\n",
    "DIM = 512\n",
    "\n",
    "inputs = Input(shape=(DIM, ))\n",
    "x = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable = False, mask_zero=True)(inputs)\n",
    "\n",
    "x = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
    "\n",
    "x = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
    "\n",
    "x = Attention()([x, x, x])\n",
    "\n",
    "x = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "outputs = x\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[\n",
    "                  'binary_accuracy',\n",
    "                  JaccardSimilarity('jaccard_nonspoilers', num_classes=2, target_class_ids=[0]),\n",
    "                  JaccardSimilarity('jaccard_spoilers', num_classes=2, target_class_ids=[1]),\n",
    "                  JaccardSimilarity('mean_jaccard', num_classes=2, target_class_ids=[0, 1])\n",
    "              ],\n",
    "              optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sprawdzenie, czy poprawnie dziala masking\n",
    "# test = np.array([[1, 2, 3] + [0] * 509, [200, 0, 1, 1] + [0] * 508])\n",
    "# data_out = np.array([[0, 0, 0] + [1] * 509, [1, 0, 0, 1] + [1] * 508])\n",
    "# data_out = np.expand_dims(data_out, axis = -1)\n",
    "# model.evaluate(test, data_out, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 19:01:28.480035: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3060 Laptop GPU\" frequency: 1425 num_cores: 30 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 3145728 shared_memory_size_per_multiprocessor: 102400 memory_size: 4318887936 bandwidth: 336048000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-01-19 19:01:29.643140: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_42/output/_24'\n",
      "2023-01-19 19:01:30.511955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2023-01-19 19:01:32.509865: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fb3c589f8c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-19 19:01:32.509893: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-01-19 19:01:32.515811: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-19 19:01:32.611234: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-01-19 19:01:32.611850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1856/1857 [============================>.] - ETA: 0s - loss: 0.6267 - binary_accuracy: 0.6461 - jaccard_nonspoilers: 0.5112 - jaccard_spoilers: 0.4381 - mean_jaccard: 0.4747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 19:04:05.556555: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3060 Laptop GPU\" frequency: 1425 num_cores: 30 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 3145728 shared_memory_size_per_multiprocessor: 102400 memory_size: 4318887936 bandwidth: 336048000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1857/1857 [==============================] - 177s 86ms/step - loss: 0.6267 - binary_accuracy: 0.6461 - jaccard_nonspoilers: 0.5112 - jaccard_spoilers: 0.4382 - mean_jaccard: 0.4747 - val_loss: 0.6010 - val_binary_accuracy: 0.6721 - val_jaccard_nonspoilers: 0.5574 - val_jaccard_spoilers: 0.4415 - val_mean_jaccard: 0.4994\n",
      "Epoch 2/5\n",
      "1857/1857 [==============================] - 157s 85ms/step - loss: 0.5926 - binary_accuracy: 0.6829 - jaccard_nonspoilers: 0.5402 - jaccard_spoilers: 0.4946 - mean_jaccard: 0.5174 - val_loss: 0.5952 - val_binary_accuracy: 0.6764 - val_jaccard_nonspoilers: 0.5837 - val_jaccard_spoilers: 0.4079 - val_mean_jaccard: 0.4958\n",
      "Epoch 3/5\n",
      "1857/1857 [==============================] - 156s 84ms/step - loss: 0.5765 - binary_accuracy: 0.6974 - jaccard_nonspoilers: 0.5564 - jaccard_spoilers: 0.5123 - mean_jaccard: 0.5343 - val_loss: 0.5769 - val_binary_accuracy: 0.6970 - val_jaccard_nonspoilers: 0.5407 - val_jaccard_spoilers: 0.5290 - val_mean_jaccard: 0.5348\n",
      "Epoch 4/5\n",
      "1857/1857 [==============================] - 159s 86ms/step - loss: 0.5590 - binary_accuracy: 0.7118 - jaccard_nonspoilers: 0.5734 - jaccard_spoilers: 0.5294 - mean_jaccard: 0.5514 - val_loss: 0.5762 - val_binary_accuracy: 0.6949 - val_jaccard_nonspoilers: 0.5536 - val_jaccard_spoilers: 0.5094 - val_mean_jaccard: 0.5315\n",
      "Epoch 5/5\n",
      "1857/1857 [==============================] - 157s 85ms/step - loss: 0.5404 - binary_accuracy: 0.7266 - jaccard_nonspoilers: 0.5893 - jaccard_spoilers: 0.5502 - mean_jaccard: 0.5697 - val_loss: 0.5755 - val_binary_accuracy: 0.7011 - val_jaccard_nonspoilers: 0.5658 - val_jaccard_spoilers: 0.5103 - val_mean_jaccard: 0.5381\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fb708165b40>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name = f\"./checkpoints/lstm-with-attention-best-val-512\"\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_name,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 8s 36ms/step - loss: 0.5748 - binary_accuracy: 0.7059 - jaccard_nonspoilers: 0.5763 - jaccard_spoilers: 0.5099 - mean_jaccard: 0.5431\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.5748364925384521,\n 0.7059265375137329,\n 0.5763131380081177,\n 0.5098705291748047,\n 0.5430918335914612]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_name)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
