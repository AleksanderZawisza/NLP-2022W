{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedc87c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwesolowski/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from datasets.utils import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cd96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pwesolowski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from transformers import create_optimizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "stop_words_list = nltk.corpus.stopwords.words('english')\n",
    "[stop_words_list.append(arg) for arg in [\".\", \"*\", \"!\", \"'\", \":\", '\"', \"?!\", \",\", \"(\", \")\", \"?\"]]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f98e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970d6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44b2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273262it [00:02, 93616.80it/s] \n",
      "34158it [00:00, 209026.16it/s]\n",
      "34158it [00:00, 52728.06it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_reviews_from_tropes(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        reviews = []\n",
    "        for line in tqdm(f):\n",
    "            book = json.loads(line)\n",
    "            reviews.append(\n",
    "                {\n",
    "                    \"label\": book[\"has_spoiler\"],\n",
    "                    \"sentences\": book[\"sentences\"]\n",
    "                }\n",
    "            )\n",
    "        return reviews\n",
    "\n",
    "\n",
    "tropes_train = get_reviews_from_tropes(\n",
    "    f\"{DATA_PATH}/tropes/tvtropes_books-train.json\"\n",
    ")\n",
    "tropes_test = get_reviews_from_tropes(\n",
    "    f\"{DATA_PATH}/tropes/tvtropes_books-test.json\"\n",
    ")\n",
    "tropes_val = get_reviews_from_tropes(\n",
    "    f\"{DATA_PATH}/tropes/tvtropes_books-val.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f829c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "if model_name == \"distilbert-base-uncased\":\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, dropout=0.2\n",
    "    )\n",
    "elif model_name == \"bert-base-uncased\":\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, classifier_dropout=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec48f059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7425eea860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name = f\"./checkpoints/best_val_tropes_{model_name}\"\n",
    "model.load_weights(checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0554886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(review_list):\n",
    "    tokenized_reviews = Dataset.from_list([{\"text\": x} for x in review_list]).map(preprocess_function, batched=True)\n",
    "    return model.prepare_tf_dataset(\n",
    "        tokenized_reviews, shuffle=False, batch_size=32, collate_fn=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b8cb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "    tf_data = create_tf_dataset(texts)\n",
    "#     outputs = model(**tokenizer(texts, return_tensors=\"tf\", truncation=True))\n",
    "    outputs = model.predict(tf_data, verbose=0)\n",
    "    probas = tf.nn.softmax(outputs.logits).numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430b1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"nonspoiler\", \"spoiler\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138d5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(data_item):\n",
    "    return \" \".join(sentence[1] for sentence in data_item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e580349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_(word: str) -> str:\n",
    "    for i in [\".\", \"*\", \"!\", \"'\", \":\", '\"', \"?!\", \",\", \"(\", \")\", \"?\", \";\"]:\n",
    "        word = word.replace(i, \"\")\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29306d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_try(word):\n",
    "    try:\n",
    "        return exp_dct[word]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ebd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sentece(sentence):\n",
    "    review_to_predict = merge_sentences(sentence)\n",
    "    \n",
    "    review = [word_(word) for word in review_to_predict.split(\" \") if word_(word) not in stop_words_list]\n",
    "    review = \" \".join(review)\n",
    "    \n",
    "    exp = explainer.explain_instance(review, predictor, num_features=10, num_samples=1000)\n",
    "    \n",
    "    exp_dct = dict(exp.as_list())\n",
    "    exp_dct = {key: value for key, value in exp_dct.items() if value > 0}\n",
    "\n",
    "    lst = []\n",
    "    for sen in tropes_train[0]['sentences']:\n",
    "        sen_ = [word_(word) for word in sen[1].split(\" \")]        \n",
    "        lst.append(sum([dict_try(word) for word in sen_]))\n",
    "        \n",
    "    arr_troops = np.array([false_true[0] for false_true in sentence['sentences']])\n",
    "    max_int = arr_troops.sum()\n",
    "    choice_troop = arr_troops.argsort()[-max_int:][::-1]    \n",
    "    \n",
    "    arr_exp = np.array(lst)\n",
    "    chooice_exp = arr_exp.argsort()[-max_int:][::-1]\n",
    "    \n",
    "    if list(set(chooice_exp).intersection(set(choice_troop))):\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    \n",
    "    return val, sum([1 for chooice in chooice_exp.tolist() if chooice in choice_troop.tolist()])/len(choice_troop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe131b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t = [tropes_train_ for tropes_train_ in tropes_train if tropes_train_['label'] == True and len(tropes_train_['sentences']) > 4 and len(tropes_train_['sentences']) < 12]\n",
    "len(t_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765cd9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_t = t_t[:200]\n",
    "len(t_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95decf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 200/200 [11:44<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "lst = [explain_sentece(tropes_train_) for tropes_train_ in tqdm(t_t, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a554f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_lst(lst):\n",
    "    if not lst:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c32999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_of_explenation(review_):\n",
    "    review_to_predict = merge_sentences(review_)\n",
    "\n",
    "    review = [word_(word) for word in review_to_predict.split(\" \") if word_(word) not in stop_words_list]\n",
    "    review = \" \".join(review)\n",
    "\n",
    "    exp = explainer.explain_instance(review, predictor, num_features=10, num_samples=1000)\n",
    "\n",
    "    exp_dct = dict(exp.as_list())\n",
    "    post_val = {key: value for key, value in exp_dct.items() if value > 0}\n",
    "\n",
    "    sens = [sen[1][sen[2][0][0]: sen[2][0][1]] for sen in review_['sentences'] if sen[0] == True]\n",
    "\n",
    "    spoilers = word_tokenize(\" \".join(sens))\n",
    "    spoilers = [spoiler.lower() for spoiler in spoilers]\n",
    "    \n",
    "    post_val_5 = list(post_val.keys())[0:5]\n",
    "    post_val_1 = list(post_val.keys())[0]\n",
    "    \n",
    "    find_words_10 = list(set(post_val).intersection(set(spoilers)))\n",
    "    find_words_5 = list(set(post_val_5).intersection(set(spoilers)))\n",
    "    find_words_1 = list(set(post_val_1).intersection(set(spoilers)))\n",
    "    \n",
    "    return (empty_lst(find_words_10), empty_lst(find_words_5), empty_lst(find_words_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9209b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [11:41<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "results = [intersection_of_explenation(sen) for sen in tqdm(t_t, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b793a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (1, 0.42857142857142855), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.25), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (1, 0.25), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.3333333333333333), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.3333333333333333), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.3333333333333333), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.5), (0, 0.0), (1, 0.5), (0, 0.0), (0, 0.0), (1, 0.5714285714285714), (0, 0.0), (1, 0.2), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.42857142857142855), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.625), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.3333333333333333), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.2), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.625), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.16666666666666666), (0, 0.0), (1, 0.75), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.6), (1, 0.2), (1, 0.8333333333333334), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.9), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.3333333333333333), (0, 0.0), (1, 0.4), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (0, 0.0), (1, 0.5), (0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f142af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (0, 0, 0), (1, 1, 0), (0, 0, 0), (1, 1, 1), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (0, 0, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (0, 0, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (0, 0, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 1), (0, 0, 0), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (0, 0, 1), (1, 1, 1), (0, 0, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 1), (0, 0, 0), (1, 1, 0), (0, 0, 0), (0, 0, 0), (1, 1, 1), (0, 0, 0), (1, 1, 1), (1, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (0, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (0, 0, 0), (1, 1, 1), (1, 1, 0), (1, 0, 0), (1, 1, 0), (0, 0, 0), (1, 1, 0), (0, 0, 0), (1, 1, 1), (1, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa881f8c",
   "metadata": {},
   "source": [
    "## Średnia po pierwszym i drugim elemencie w lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10cc3a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16, 0.0638095238095238)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([x[0] for x in lst]), np.mean([x[1] for x in lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b65cb8",
   "metadata": {},
   "source": [
    "## Średnie po trzech elementach w results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4014fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.825, 0.815, 0.335)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([x[0] for x in results]), np.mean([x[1] for x in results]), np.mean([x[2] for x in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532db945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
