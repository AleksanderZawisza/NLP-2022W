\section{Discussion}\label{discussion}
There are several contributions to this work. First of all, we were not able to find a detailed comparison of statistical and deep learning models. Often works focus on a selected group of models like transformers in case of \cite{sig_bert}, 1D CNNs in \cite{audio_1dcnn} or Support Vector Machines explored in \cite{oldAudio}.

What is more, often different datasets are being used, which overall makes the results difficult to compare. Our work compares older and newer models on different datasets. This creates a common ground and context for other works and researchers. When it comes to the comparison of our results to the current state-of-the-art, again the situation is a bit complex. State-of-the-art solution for different, in our opinion simpler, task of genre classification based on audio stands at 80.93\% achieved in \cite{audio_1dcnn}. The best lyrics-based prediction accuracy that we were able to find was 77.63\%, achieved by the BERT model in \cite{sig_bert}.

This cast a strong shadow on our best result of 57.17\% but again, those results are difficult to compare. Why? This is due to the fact that the authors of mentioned SOTA preprocessed the data in an undocumented way, significantly simplifying the original problem present in the dataset. Naturally, as mentioned numerously in this document we also made changes to the formulation of the original problem, but due to the missing information from other works we are not able to reproduce this process in a similar way, hence making the results non-conclusive. What is worse, the mentioned article makes use of a different dataset, which makes the accuracy even harder to compare. 

